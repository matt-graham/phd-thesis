\chapter{Approximate inference}\label{ch:approximate-inference}

\enlargethispage{\baselineskip}
In the previous chapter we argued that the key computational challenge in performing inference in probabilistic models is being able to evaluate integrals with respect to probability distributions defined on high-dimensional spaces. Generally these integrals will not have analytic solutions and for models with even moderate numbers of unobserved variables, numerical quadrature approaches to evaluating integrals are computationally infeasible. % due to the exponential scaling of computation cost with dimension.

%\marginpar{Although this may seem a paradox, all exact science is dominated by the idea of approximation. When a man tells you that he knows the exact truth about anything, you are safe in inferring that he is an inexact man.\\---Bertrand Russell}
%\marginpar{Be approximately right rather than exactly wrong.\\--John W. Tukey}
\marginpar{Truth is much too complicated to allow anything but approximations.\\---John von Neumann}
In this chapter we will review some of the key algorithms proposed for computing \emph{approximate} solutions to inference problems. A unifying aspect to all of these methods is trading off some loss of the accuracy of the answers provided to inferential queries, for a potentially significant increase in computational tractability. The literature on \emph{approximate inference} methods is vast and so necessarily this chapter will only be a partial review of the methods directly relevant to this thesis.

Approximate inference methods can be roughly partitioned into two groups: methods in which integrals with respect to the target distribution are estimated by averaging over samples from a distribution over the target space and those in which a more tractable approximation to the target distribution is found by optimising the approximation to be `close' to the target distribution. In this chapter we will concentrate on the sampling-based approaches to approximate inference. 

In particular we will focus on \ac{MCMC} methods, as these form the key basis for the contributions discussed in later chapters. We will review the key theory underlying Monte Carlo integration and \ac{MCMC} methods and some of the standard algorithms for implementing these approaches. We will conclude with a discussion of auxiliary variable \ac{MCMC} methods which are central to the methods discussed in the remainder of this thesis.   

Although they are not the main focus of this thesis we will make use of several optimisation-based approximate inference methods within the algorithms discussed in the following chapters. We review the ideas underlying these methods in Appendix \ref{app:optimisation-based-approximate-inference}. 

\section{Monte Carlo methods}

%\footnote{We will concentrate for the moment on models with explicit density functions.}
Inference at both the level of computing conditional expectations of unobserved variables in a model and in evaluating evidence terms to allow model comparison involves integrating functions against a probability distribution. Typically the distribution of interest will be defined by a probability density with respect to a base measure. Therefore we wish to be able to compute integrals of the form
\begin{equation}\label{eq:integral-against-density}
  \int_{\set{X}} f(\vct{x}) \, \tgtprob(\dr\vct{x}) =
  \int_{\set{X}} f(\vct{x}) \, \tgtdens(\vct{x}) \,\mu(\dr\vct{x})
\end{equation}
where $\tgtdens$ is the density of a target distribution $\tgtprob$ on a space $\set{X}$ with respect to a base measure $\mu$ and $f$ is a measurable function. For instance in the case of computing the \emph{posterior mean} in a Bayesian inference problem with observed variables $\rvct{y}$ and latent variables $\rvct{x}$ where the posterior density $\pden{\rvct{x}|\rvct{y}}$ is defined with respect to the $D$-dimensional Lebesgue measure, we would have  $\tgtdens(\vct{x}) = \pden{\rvct{x}|\rvct{y}}(\vct{x}\gvn\vct{y})$ for an observed $\vct{y}$, $\mu(\vct{x}) = \lebm{D}(\vct{x})$ and $f(\vct{x}) = \vct{x}$. Often we will only be able to evaluate $\tgtdens$ up to an unknown normalising constant i.e. $\tgtdens(\vct{x}) = \frac{1}{Z}\utgtdens(\vct{x})$ with we able to evaluate $\utgtdens$ pointwise but $Z$ intractable to compute. For example in a Bayesian inference setting $\utgtdens(\vct{x})$ would be the joint density $\pden{\rvct{x},\rvct{y}}(\vct{x},\vct{y})$ and $Z$ the model evidence $\pden{\rvct{y}}(\vct{y})$. When performing inference in undirected models, we would instead have that $\utgtdens$ is the product of unnormalised factors and $Z$ the corresponding normaliser.

%The key idea of the methods we will discuss in this section is that we can estimate \eqref{eq:integral-against-density} by generating a set of random samples from a probability distribution defined on $\set{X}$ and then computing a (potentially weighted) average of the value of the function $f$ evaluated at these sample points. The most direct approach is to sample independently from the probability distribution defined by the target density $\tgtdens$. As we will see this is not necessarily feasible to do for complex target densities defined on high dimensional spaces, however a  of related methods for generating and using random samples to approximate integrals with respect to target densities arising from complex probabilistic models have been developed.

\subsection{Monte Carlo integration}\label{subsec:monte-carlo-method}

\marginpar{The eponym of the Monte Carlo method is a Monocan casino, favoured haunt of the uncle of Stanis{\l}aw Ulam, one of the method's inventors.}
The framework that unifies all of the methods we will discuss in this section is the \emph{Monte Carlo} method for integration \citep{ulam1949monte}. Let $\rvct{x}$ be a random vector distributed according to the target distribution i.e. $\prob{\rvct{x}} = \tgtprob$. Given an arbitrary measurable function $f : \set{X} \to \reals$ we define a random variable $\rvar{f} = f(\rvct{x})$. Our task is to compute expectations $\expc{\rvar{f}} = \bar{f}$ corresponding to the integral \eqref{eq:integral-against-density}. We assume that $\expc{\rvar{f}}$ exists and both $\expc{\rvar{f}}$ and $\var{\rvar{f}}$ are finite. For now we assume we have a way of generating values of $N$ random variables $\lbrace \rvct{x}_n\rbrace_{n=1}^N$, each marginally distributed according to the target distribution i.e. $\prob{\rvct{x}_n} = \tgtprob ~\forall n \in 1 \dots N$ but potentially not independent of each other. We define random variables 
\begin{equation}
  \rvar{f}_n = f\lpa\rvct{x}_n\rpa \quad\forall n \in \fset{1\dots N}
  \quad\textrm{and}\quad
  \hatf_N = \frac{1}{N} \sum_{n=1}^N \rvar{f}_n.
\end{equation}
Due to linearity of the expectation operator, we have that
\begin{equation}
  \expc{\hatf_N} = 
  \frac{1}{N} \sum_{n=1}^N \expc{\rvar{f}_n} = 
  \frac{1}{N} \sum_{n=1}^N \bar{f} = 
  \bar{f}
\end{equation}
%\marginpar{The variance of a random variable $\rvar{x}$ is defined as $\var{\rvar{x}} =\expc{(\rvar{x} - \expc{\rvar{x}})^2}$.}
and so that in expectation $\hatf_N$ is equal to $\bar{f}$, i.e. realisations of $\hatf_N$ are \emph{unbiased estimators} of $\bar{f}$. Note that this result does not require any independence assumptions about the generated random variables. Now considering the variance of $\hatf_N$ we can show that
%\begin{align}
%  \var{\hatf_N} 
%  =\,& 
%  \expc{\lpa \frac{1}{N} \sum_{n=1}^N \lpa f(\rvct{x}_n) \rpa - \bar{f}\rpa^2}\\
%  =\,&
%  \frac{1}{N^2} \expc{\lpa \sum_{n=1}^N \lpa f(\rvct{x}_n) - \bar{f} \rpa\rpa^2}\\
%  =\,&
%  \frac{1}{N^2}\sum_{n=1}^N \expc{\lpa f(\rvct{x}_n) - \bar{f} \rpa^2} +\,\\
%  &
%  \frac{2}{N^2}\sum_{n=1}^N \sum_{m=1}^{n-1} 
%  \expc{\lpa f(\rvct{x}_n) - \bar{f} \rpa\lpa f(\rvct{x}_m) - \bar{f} \rpa}
%  \\
%  =\,&
%  \frac{\var{f(\rvct{x})}}{N}
%  \lpa 
%    1 +
%    \frac{2}{N}\sum_{n=1}^N \sum_{m=1}^{n-1} 
%      \frac{\expc{\lpa f(\rvct{x}_n) - \bar{f} \rpa\lpa f(\rvct{x}_m) - \bar{f} \rpa}}
%      {\var{f(\rvct{x})}}
%  \rpa
%\end{align}
\begin{align}\label{eq:monte-carlo-variance-general}
  \var{\hatf_N}
  =
  \frac{\var{\rvar{f}}}{N}
  \lpa 
    1 +\frac{2}{N}\sum_{n=1}^{N-1} \sum_{m=1}^{n-1} \frac{\cov{\rvar{f}_n, \rvar{f}_m}}{\var{\rvar{f}}}
  \rpa.
\end{align}
If the generated random variables $\lbrace \rvct{x}_n\rbrace_{n=1}^N$ and so $\lbrace \rvar{f}_n\rbrace_{n=1}^N$ are independent, then $\cov{\rvar{f}_n, \rvar{f}_m} = 0 ~\forall m\neq n$. In this case \eqref{eq:monte-carlo-variance-general} reduces to $ \var{\hatf_N} = \var{\rvar{f}} / {N}$, 
%\begin{equation}\label{eq:monte-carlo-variance-independent}
%  \var{\hatf_N}
%  =
%  \frac{\var{\rvar{f}}}{N},
%\end{equation}
i.e. the variance of the \emph{Monte Carlo estimate} $\hatf_N$ for $\bar{f}$ is inversely proportional to the number of samples $N$. Importantly this scaling does not depend on the dimension of $\rvct{x}$. 

Therefore if we can generate a set of independent random variables from the target distribution, we can estimate expectations that asymptotically tend to the true value as $N$ increases, with a typical deviation from the true value (as measured by the standard deviation, i.e. the square root of variance) that is $\mathcal{O}\lpa N^{-\frac{1}{2}}\rpa$. In comparison a fourth-order quadrature method such as \emph{Simpson's rule} has an error that is $\mathcal{O}\lpa N^{-\frac{4}{D}}\rpa$ for a grid of $N$ points uniformly spaced across a $D$ dimensional space. Asymptotically for $D > 8$, Monte Carlo integration will therefore give better convergence than Simpson's rule, and even for smaller dimensions large constant factors in the Simpson's rule dependence can mean Monte Carlo performs better for practical $N$.

Note that computing Monte Carlo estimates from independent random variables is not optimal in terms of minimising $\var{\hatf_N}$ for a given $f$; the covariance terms in \eqref{eq:monte-carlo-variance-general} can be negative which can reduce the overall variance. A wide range of \emph{variance reduction} methods have been proposed to exploit this and produce lower variance of Monte Carlo estimates for a given $f$ \citep{kroese2011variance}. Although these methods can be important in practice for achieving an estimator with a practical variance for a specific $f$ of interest, we will generally concentrate on the case where we do not necessarily know $f$ in advance. % and so cannot easily exploit these approaches. %More generally applicable are \emph{quasi-Monte Carlo} methods \citep{niederreiter1992random,morokoff1995quasi} which use specially constructed \emph{low-discrepancy sequences} to more evenly tile the sample space. The error of quasi-Monte Carlo is upper bounded by $\mathcal{O}(\log(N)^D N^{-1})$ errors which can improve efficiency in some cases. The methods a

\subsection{Pseudo-random number generation}

\begin{figure}
\centering
\tikzsetnextfilename{linear-congruential-generator-sequence}
\pgfplotstableread{data/lcg-37-61-128-37.cvs}{\lcgfile}
\drawgrid[zero color=black!80, one color=Maroon, cell ht=0.22em, cell wd=0.22em]{\lcgfile}
\caption[Example linear congruential generator sequence.]{Binary representation of linear congruential generator sequence $s_{n+1} = 37s_n + 61 \kern-4pt\mod 128$. Columns left to right represents successive integer states in sequence. From least significant (bottom) to most significant (top), the bits in each column have patterns repeating with periods 2, 4, 8, 16, 32, 64, 128.}
\label{fig:example-lcg-sequence}
\end{figure}

%\footnote{The sequences are determnistic in the sense that if the generator internal state is known all values in the sequence can be reconstructed exactly.}
\marginpar{The generation of random numbers is too important to be left to chance.\\ ---Robert R. Coveyou}
Virtually all statistical computations involving random numbers in practice make use of \acp{PRNG}. Rather than generating samples via a truly random process\footnote{We consider a true random process as one in which it is impossible to precisely predict the next value in the sequence given the previous values.}, \acp{PRNG} produce deterministic sequences of integers in a fixed range that nonetheless maintain many of the properties of a random sequence. In particular through careful choice of the updates, sequences with a very long period (number of iterations before the sequence begins repeating), a uniform distribution across the numbers in the sequence range and low correlation between successive states can be constructed. 

A very simple example of a class of \acp{PRNG} is the \emph{linear congruential generator} \citep{lehmer1951mathematical} which obeys the recurrent update
\begin{equation}\label{eq:lcg-update}
  s_{n+1} = (a s_n + c) \kern-4pt\mod m
  \quad \textrm{with} \quad
   0 < a < m,~ 0 \leq c < m,
\end{equation}
with $a$, $c$ and $m$ integer parameters. If $a$, $c$ and $m$ are chosen appropriately, iterating the update \eqref{eq:lcg-update} from an initial seed $0 \leq s_0 < m$, will produce a sequence of states which visits all the integers in $[0, m)$ before repeating. An example state sequence with $m=128$ is shown in Figure \ref{fig:example-lcg-sequence}. In practice, linear congruential generators produce sequences with poor statistical properties, particularly when used to generate random points in high dimensional spaces \citep{marsaglia1968random}, hence most modern numerical computing libraries use more robust \acp{PRNG} such as the \emph{Mersenne-Twister} \citep{matsumoto1998mersenne}, which is used in all experiments in this thesis.

The raw output of a \ac{PRNG} is an integer sequence, with typically the sequence elements uniformly distributed over all integers in a range $[0, 2^n)$ for some $n \in \naturals$. All real values are represented at a finite precision on computers, typically using a floating point representation \citep{ieee2008standard} of \emph{single} (24-bit mantissa) or \emph{double} (53-bit mantissa) precision. Through an appropriate linear transformation, the integer outputs of a \ac{PRNG} can be converted to floating-point values uniformly distributed across a finite interval. \ac{PRNG} implementations typically provide a primitive to generate floating-point values uniformly distributed on $[0, 1)$. Given the ability to generate sequences of (effectively) independent samples from a uniform distribution $\mathcal{U}(0,1)$, the question is then how to use these to produce random samples from arbitrary densities. %This will be the subject of the following sub-sections.

\subsection{Transform sampling}

\begin{figure}
\def\numgrid{11}
\resizebox{\textwidth}{!}{
\includetikz{box-muller-transform}
}
\caption[Visualisation of Box--Muller transform.]{Visualisation of Box--Muller transform. Left axis shows uniform grid on $\set{U} = [0,1]^2$ and right-axis shows grid points after mapping through $\vct{g}$ in transformed space $\set{X} = \reals^2$.}
\label{fig:box-muller-transform}
\end{figure}
\enlargethispage{1\baselineskip}
Samples from many standard distributions can be generated by exploiting the transformation of random variables relationships discussed in \ref{subsec:change-of-variables}. Let $\rvct{u}$ be a $D$-dimensional vector of independent random variables marginally distributed according to $\mathcal{U}(0,1)$ and $\vct{g} :  [0,1)^D \to \set{X}$ be a diffeomorphism with $\set{X} \subseteq \reals^D$. If we define $\rvct{x} = \vct{g}(\rvct{u})$, then by the change of variables formula \eqref{eq:change-of-variables-vector-bijective} we have that
\begin{equation}\label{eq:transform-sampling-uniform}
  \pden{\rvct{x}}(\vct{x}) = \left|\pd{\vct{g}^{-1}(\vct{x})}{\vct{x}}\right|.
\end{equation}
For example for $D=2$, $\set{X} = \reals^2$ and a bijective map $\vct{g}$ defined by
\begin{equation*}\label{eq:box-muller-transform}
\begin{split}
  \vct{g}\kern0.5pt\begin{pmatrix} u_1\\u_2\end{pmatrix} =
  \begin{bmatrix} \sqrt{-2\log u_1}\cos(2\uppi u_1) \\ \sqrt{-2\log u_1}\sin(2\uppi u_2)\end{bmatrix}
  ,~
  \vct{g}^{-1}\kern0.5pt\begin{pmatrix} x_1\\x_2\end{pmatrix} =
  \begin{bmatrix} \exp\lpa-\frac{1}{2}(x_1^2+x_2^2)\rpa \\ \frac{1}{2\uppi}\arctan\lpa\frac{x_1}{x_2}\rpa\end{bmatrix},
\end{split}
\end{equation*}
then we have that the density of the transformed $\rvct{x} = \vct{g}(\rvct{u})$ is
\begin{equation}\label{eq:box-muller-transform-density}
  \pden{\rvct{x}}(\vct{x}) = 
  \frac{1}{\sqrt{2\uppi}}\exp\lpa-\frac{x_1^2}{2}\rpa
  \frac{1}{\sqrt{2\uppi}}\exp\lpa-\frac{x_2^2}{2}\rpa ,
\end{equation}
i.e. $\rvar{x}_1$ and $\rvar{x}_2$ are independent random variables with standard normal distributions $\nrm{0,1}$. This is the \emph{Box--Muller transform} \citep{box1958note}, and allows generation of independent standard normal variables given a \ac{PRNG} primitive for sampling from $\mathcal{U}(0,1)$. A visualisation of the transformation of space applied by the method is shown in Figure \ref{fig:box-muller-transform}. 

%Due to the relatively high cost of the trigonometric function evaluations, more efficient alternatives to Box--Muller are usually used in practice to generate normal random variables such as a rejection sampling (see next sub-section) based variant \citep{marsaglia1968random} or the \emph{Ziggurat algorithm} \citep{marsaglia2000ziggurat}. % (which also generalises to other symmetric univariate distributions).

A general method for sampling from univariate distributions is to use an inverse \ac{CDF} transform. For a probability density $\tgtdens$ on a scalar random variable, the corresponding \ac{CDF} $r : \reals \to [0,1]$ is defined as
\begin{equation}
  r(x) = \int_{-\infty}^x \tgtdens(v) \,\dr v
  \implies
  \pd{r(x)}{x} = \tgtdens(x).
\end{equation}
If $\rvar{u}$ is a standard uniform random variable and $\rvar{x} = r^{-1}(\rvar{u})$ then
\begin{equation}
  \pden{\rvar{x}}(x) = \left|\pd{r(x)}{x}\right| = \tgtdens(x).
\end{equation}
To be able to use the inverse \ac{CDF} transform method we need to be able to evaluate $r^{-1}$, sometimes termed the \emph{quantile function}. Often neither the \ac{CDF} or quantile function of a univariate distribution will have closed form solutions however we can use polynomial approximation methods and iterative solvers to evaluate both to arbitrary precision \citep{olver2013fast}. For some distributions such as the standard normal $\nrm{0,1}$ even though the \ac{CDF} and quantile function do not have analytic forms in terms of elementary functions it is common for numerical computing libraries to provide approximations to both which are accurate to within small multiples of machine precision. Although the inverse \ac{CDF} transform method gives a general recipe for sampling from univariate densities, it is not easy to generalise to multivariate densities and alternatives can be simpler to implement and more numerically stable.

\subsection{Rejection sampling}

\begin{figure}[t]
\centering
\includetikz{rejection-sampling}
\caption[Visualisation of rejection sampling.]{Visualisation of rejection sampling. The green curve shows the (unnormalised) target density $\utgtdens$, the green region underneath representing the area we wish to sample points uniformly from. The dashed orange curve shows the scaled proposal density $M q$, with the orange (plus green) region representing the area we uniformly propose values from. Two example proposals are shown: $\diamond$ is under the target density and so accepted; $*$ is outside of the green region and so would be rejected.}
\label{fig:rejection-sampling}
\end{figure}

An important class of generic sampling methods, particularly due their use as a building block in other algorithms, is rejection sampling \citep{vonneumann1951various}. Rejection sampling uses the observation that to sample from a distribution with density $\tgtdens : \set{X} \to [0, \infty)$ it is sufficient to uniformly sample from the volume under the graph of $\lpa\vct{x}, \tgtdens(\vct{x})\rpa$.

The key requirement in rejection sampling is to identify a \emph{proposal distribution} $Q$ which we can generate independent samples from and has a density $q=\td{Q}{\mu}$ that upper bounds the potentially unnormalised target density $\utgtdens$ across its full support $\set{X}$ when multiplied by a known constant $M$, i.e. $\utgtdens(x) \leq M q(x) ~\forall x \in \set{X}$. The requirement to be able to generate independent samples from $Q$ can be met for example by distributions amenable to transform sampling, e.g. the standard normal. The second requirement is generally more challenging and as we will see the efficiency of rejection sampling methods is very dependent on how tight the bound can be made.

\begin{algorithm}[!t]
\caption{Rejection sampling.}
\label{alg:rejection-sampling}
\input{algorithms/rejection-sampling}
\end{algorithm}

Algorithm \ref{alg:rejection-sampling} describes the rejection sampling method to produce a single independent sample from a target distribution. A visualisation of how the algorithm works for a univariate target distribution is shown in Figure \ref{fig:rejection-sampling}. The overall aim is to generate points uniformly distributed across the green area under the (unnormalised) target density curve. This is achieved by generating points uniformly under the dashed orange curve corresponding to the scaled proposal density and then accepting only those which are below the green curve. To generate a point under the dashed orange curve we first generate an $x$ from the proposal distribution and then generate an auxiliary `height' variable by sampling uniformly from $[0, Mq(x)]$. If the sampled height is below the green curve we accept (as in the $\diamond$ example in Figure \ref{fig:rejection-sampling}) else we reject the sample (as in the $*$ example in Figure \ref{fig:rejection-sampling}).

\begin{figure}[t]
\centering
\includetikz{rejection-sampling-factor-graph}
\caption{Factor graph of rejection sampling process.}
\label{fig:rejection-sampling-factor-graph}
\end{figure}

Figure \ref{fig:rejection-sampling-factor-graph} shows the rejection sampling generative process as a directed factor graph, with $\rvct{x}$ be a random variable representing the proposal, $\rvar{u}$ the uniform auxiliary variable drawn to sample the `height' and $\rvar{a}$ a binary variable that indicates whether the proposal is accepted ($\rvar{a} = 1$) or not ($\rvar{a} = 0$). By marginalising out $\rvar{u}$ we have that that
\begin{equation}\label{eq:rejection-sampling-joint-prob}
  \pden{\rvct{x},\rvar{a}}(\vct{x},a) = 
  q(\vct{x}) \lpa\frac{\utgtdens(\vct{x})}{Mq(\vct{x})}\rpa^a
  \lpa1 - \frac{\utgtdens(\vct{x})}{Mq(\vct{x})}\rpa^{1-a},
\end{equation}
and further marginalising over the proposal $\rvct{x}$
\begin{equation}\label{eq:rejection-sampling-accept-prob}
  \pden{\rvar{a}}(a) = \lpa\frac{Z}{M}\rpa^a \lpa 1 - \frac{Z}{M}\rpa^{1-a}.
\end{equation}
Conditioning on the proposal being accepted we therefore have that
\begin{equation}\label{eq:rejection-sampling-cond-dens-given-accept}
  \pden{\rvct{x}|\rvar{a}}(\vct{x}\gvn 1) =
  \frac{q(\vct{x})\frac{\utgtdens(\vct{x})}{Mq(\vct{x})}}{\frac{Z}{M}} = \frac{\utgtdens(\vct{x})}{Z} = \tgtdens(\vct{x}).
\end{equation}
Therefore the accepted proposals are distributed according to the target density as required. Further from \eqref{eq:rejection-sampling-accept-prob} we have that the $\pden{\rvar{a}}(1) = \frac{Z}{M}$. This suggests we can use the accept rate to estimate $Z$ but also hints at the difficulty in finding a $M$ which guarantees the upper bound requirement as for $\frac{Z}{M}$ to be a valid probability $M \geq Z$ i.e. $M$ needs to be an upper bound on the unknown normalising constant $Z$. This relationship also suggests it is desirable to set $M$ as small as possible to maximise the acceptance rate. %; for a fixed proposal density $q$ this will involve setting $M$ to a value such that $M q(\vct{x}) = \utgtdens(\vct{x})$ for at least one $\vct{x}$ (as for example in Figure \ref{fig:rejection-sampling}).

%For univariate target densities which are log-concave, \emph{adaptive rejection sampling} \citep{gilks1992adaptive} offers an efficient adaptive method 
Although rejection sampling can be an efficient method of sampling from univariate target distributions (particularly for distributions with log-concave densities where adaptive variants are available \citep{gilks1992adaptive}), it generally scales very poorly with the dimensionality of the target distribution. This is as the ratio of the volume under the target density to the volume under the scaled proposal density (in terms of Figure \ref{fig:rejection-sampling} the ratio of the green area to the green plus orange regions), and so the probability of accepting a proposal, will tend become exponentially smaller as the dimensionality increases. This is an example of the so-called \emph{curse of dimensionality}. Therefore although rejection sampling can be a useful subroutine for generating random variables from low-dimensional distributions, in general it is not a viable option for generating samples directly for high-dimensional Monte Carlo integration.

\subsection{Importance sampling}

So far we have considered methods for generating samples directly from a target distribution. Although samples can be of value in themselves for giving a representative set of plausible values from the target distribution (e.g. for visualisation purposes), usually the end goal is to estimate integrals of the form in \eqref{eq:integral-against-density}. 

\emph{Importance sampling} \citep{kahn1951estimation} is a Monte Carlo method which allows arbitrary integrals to be estimated. If $Q$ is a distribution, with density $q = \td{Q}{\mu}$, which is absolutely continuous with respect to the target distribution (which requires that $\tgtdens(\vct{x}) > 0 \Rightarrow q(\vct{x}) > 0$), then importance sampling is based on the identity
\begin{equation}\label{eq:importance-sampling-integral}
  \bar{f} =
  \frac
  {\int_{\set{X}} f(\vct{x}) \,\utgtdens(\vct{x}) \,\mu(\dr\vct{x})}
  {\int_{\set{X}} \utgtdens(\vct{x}) \,\mu(\dr\vct{x})}
  =
  \frac
  {\int_{\set{X}} f(\vct{x}) \,\frac{\utgtdens(\vct{x})}{q(\vct{x})} \, q(\vct{x}) \,\mu(\dr\vct{x})}
  {\int_{\set{X}} \frac{\utgtdens(\vct{x})}{q(\vct{x})} \, q(\vct{x}) \,\mu(\dr\vct{x})}.
\end{equation}
Each of the numerator and denominator in \eqref{eq:importance-sampling-integral} take the form of an expectation of a measurable function of a random variable $\rvct{x}$ with distribution $Q$. Further the denominator is exactly equal to $Z = \int_{\set{X}}\utgtdens(\vct{x})\,\mu(\dr\vct{x})$. We therefore have that
\begin{equation}\label{eq:importance-sampling-expectations}
  Z \bar{f} = \expc{w(\rvct{x}) f(\rvct{x})}
  ~~\textrm{and}~~
  Z = \expc{w(\rvct{x})}
  ~~\textrm{with}~~
  w(\vct{x}) = \frac{\utgtdens(\vct{x})}{q(\vct{x})}.
\end{equation}
If we can generate random variables $\lbrace \rvct{x}_n \rbrace_{n=1}^N$ each marginally distributed according to $Q$ we can therefore form Monte Carlo estimates of both the numerator and denominator. We define $\hat{\rvar{Z}}_N$ and $\hat{\rvar{g}}_N$ as
\begin{equation}\label{eq:importance-sampling-mc-estimates}
  \hat{\rvar{Z}}_N = \frac{1}{N} \sum_{n=1}^N w\lpa\rvct{x}_n\rpa
  ~~\textrm{and}~~
  \hat{\rvar{g}}_N = \frac{1}{\hat{\rvar{Z}}} \sum_{n=1}^N w\lpa\rvct{x}_n\rpa f\lpa\rvct{x}_n\rpa.
\end{equation}
By the same argument as Section \ref{subsec:monte-carlo-method}, $\expc{\hat{\rvar{Z}}_N} = Z$ and $\expc{\hat{\rvar{g}}_N} = Z\bar{f}$. We can therefore use importance sampling to form an unbiased estimate of the unknown normalising constant $Z$. 

If we define $\hatf_N = \hat{\rvar{g}}_N / \hat{\rvar{Z}}_N$, then this is a biased\footnote{In cases where the normalising constant $Z$ is known, we can instead use $w(\vct{x}) = \frac{\tgtdens(\vct{x})}{q(\vct{x})}$ in which case the ratio estimator is not required and an unbiased estimates can be calculated. As the problems we are interested in will generally have unknown $Z$ we do not consider this case further} estimator for $\bar{f}$ as in general the expectation of the ratio of two random variables is not equal to the ratios of their expectations. However if both the numerator and denominator have finite variance, i.e. $\var{\hat{\rvar{Z}}_N} < \infty$ and $\var{\hat{\rvar{g}}_N} < \infty$, then $\hatf_N$ is a \emph{consistent} estimator for $\bar{f}$ i.e. $\lim_{N\to\infty} \expc{\hatf_N} = \bar{f}$.

\begin{figure}[t]
\centering
\pgfplotsset{cycle list/Dark2-3}
\begin{subfigure}[b]{.48\linewidth}
\centering
\includetikz{importance-sampling-1}
\end{subfigure}
\begin{subfigure}[b]{.48\linewidth}
\centering
\includetikz{importance-sampling-2}
\end{subfigure}
\caption[Visualisation of importance sampling.]{Visualisation of importance sampling. On both axes the green curve shows the unnormalised target density $\utgtdens$, the dashed orange curve the density $q$ values are sampled from and the dotted violet curve the importance weighting function $w(x) = \frac{\utgtdens(x)}{q(x)}$ to estimate expectations with respect to the target density using samples from $q$. In the left axis the $q$ chosen is under-dispersed compared to $\utgtdens$ leading to very large $w$ values in the right tail. In contrast in the right axis, the broader $q$ leads to less extreme variation in $w$.}
\label{fig:importance-sampling}
\end{figure}

The $w(\rvar{x}_n)$ values are typically termed the \emph{importance weights}. If a few of the weights are very large, the weighted sums in \eqref{eq:importance-sampling-mc-estimates} will be dominated by those few values, reducing the effective number of samples in the Monte Carlo estimates. This can particularly be a problem if the are regions of $\set{X}$ with low probability under $q$ where $\tgtdens(\vct{x}) \gg q(\vct{x})$. As sampling points in these regions will be a rare event, a large number of samples may be needed to diagnose the issue adding further difficulty. A general recommendation is to use densities $q$ with tails as least as heavy of those of $\tgtdens$, and in general the closer the match between $q$ and $\tgtdens$ the better \citep{mackay2003information,owen2013importance}. Figure \ref{fig:importance-sampling} shows a visualisation of the effect of the choice of $q$ on the importance weights.

%A heuristic that can be used to assess the quality of importance sampling estimates is the (importance sampling) \emph{effective sample size} \citep{kong1992note,owen2013importance}. It approximately quantifies how many independent samples from the target $\tgtdens$ would be required to get a Monte Carlo estimate with a similar variance to that achieved using an importance sampling estimator with weights $\lbrace w(\rvct{x}_n) \rbrace_{n=1}^N$ given \ac{IID} $\lbrace \rvct{x}_n\rbrace_{n=1}^N$ generated from $q$. It can be calculated as
%\begin{equation}\label{eq:is-effective-sample-size}
%  \rvar{N}_{\textrm{eff}} = 
%  \lpa \sum_{n=1}^N \bar{\rvar{w}}_n^2 \rpa^{-1}
%  \quad\textrm{where}\quad \bar{\rvar{w}}_n  = \frac{w\lpa\rvct{x}_n\rpa}{\sum_{m=1}^N w\lpa\rvct{x}_m\rpa},
%\end{equation}
%i.e. as the reciprocal of the sum of squares of the normalised importance weights. If $\rvar{N}_{\textrm{eff}} \ll N$ this can suggest an issue with the choice of sampling density $q$. The diagnostic is not fool proof however as it is based on a finite sample size, and it may be that rare extreme importance weights due to e.g. a mode of the target $\tgtdens$ in the tails of $q$, are not encountered in a particular run giving a misleadingly high $\rvar{N}_{\textrm{eff}}$.

When previously discussing rejection sampling, we introduced an auxiliary binary accept indicator variable, $\rvar{a}$, associated with each proposed sample $\rvct{x}$ (see Figure \ref{fig:rejection-sampling-factor-graph}). If we generate $N$ independent proposal -- indicator pairs $\lbrace \rvct{x}_n, \rvar{a}_n \rbrace_{n=1}^N$ then the number of accepted proposals is $\rvar{N}_{\textrm{acc}} = \sum_{n=1}^N \rvar{a}_n$. Conditioned on $\rvar{N}_{\textrm{acc}}$ being a value more than one, the generated rejection sampling variables $\lbrace \rvct{x}_n, \rvar{a}_n \rbrace_{n=1}^N$ can be used to form an \emph{unbiased} Monte Carlo estimate of $\bar{f}$ using the estimator
\begin{equation}\label{eq:rejection-sampler-mc-estimator}
  \hatf_N^{\,\textsc{rs}} = \frac{\sum_{n=1}^N\rvar{a}_n f\lpa\rvct{x}_n\rpa}{\sum_{m=1}^N \rvar{a}_m},
\end{equation}
which just corresponds to computing the empirical mean of the accepted proposals i.e. the standard Monte Carlo estimator. In comparison importance sampling forms a biased but consistent estimator for $\bar{f}$ from $N$ samples $\lbrace \rvct{x}_n \rbrace_{n=1}^N$ from a distribution $Q$ using the estimator
\begin{equation}\label{eq:importance-sampler-mc-estimator}
  \hatf_N^{\,\textsc{is}} = \frac{\sum_{n=1}^N w\lpa\rvct{x}_n\rpa f\lpa\rvct{x}_n\rpa}{\sum_{m=1}^N w\lpa\rvct{x}_m\rpa}.
\end{equation}
From this perspective the accept indicators $\rvar{a}_n$ in rejection sampling can be seen to act like binary importance weights, in contrast importance sampling using `soft' weights which mean all sampled $\rvct{x}_n$ make a contribution to the estimator (assuming $w(\vct{x}) \neq 0 ~\forall \vct{x} \in \set{X}$). However this correspondence is only loose. The rejection sampling estimator $\hatf_N^{\,\textsc{rs}}$ is unbiased unlike $\hatf_N^{\,\textsc{is}}$, but this unbiasedness relies on conditioning on a non-zero value for $\rvar{N}_{\textrm{acc}}$ (i.e. the number of accepted samples to generate) and continuing to propose points until this condition is met. In contrast importance sampling generates a fixed number of samples from $Q$ and does not use any auxiliary random variables.

Unlike rejection sampling, there is no need in importance sampling for $q$ to upper-bound the target density. This allows more freedom in the choice of $q$ however it is still important to choose $q$ to be as close as possible to the target while remaining tractable to generate samples from. In general for target densities defined on high-dimensional spaces, it can be difficult to find an appropriate $q$ such that the variation in importance weights is not too extreme \citep{mackay2003information}. % As we will see later however the importance sampling framework can be combined with other methods we will discuss in the following sections to allow it to be scaled to high dimensional problems.

%The estimator $\hat{f}$ is however the ratio of two Monte Carlo estimates. It is therefore not an unbiased estimator for $\bar{f}$, however it is consistent, i.e. in the limit of $N \to \infty$ it converges to $\bar{f}$, providing both $\hat{Z}$ and $\hat{N}$ have finite variance.

\section{Markov chain Monte Carlo}\label{subsec:markov-chain-monte-carlo}

\begin{figure}[t]
\centering
\includetikz{markov-chain-factor-graph}
\caption[Markov chain factor graph.]{Markov chain factor graph. The initial state $\rvct{x}_0$ is sampled according to a density $q$ and each subsequent state $\rvct{x}_n$ is then generated from a transition density $\fwdtrans_n$ conditioned on the previous state $\rvct{x}_{n-1}$.}
\label{fig:markov-chain-factor-graph}
\end{figure}

%The sampling methods considered in the previous section used independent random variables to form Monte Carlo estimates. 
When introducing the Monte Carlo method we saw that is was not necessary for the random variables used in a Monte Carlo estimator to be independent. While it can be impractically computationally expensive to generate independent samples from complex high-dimensional target distributions, simulating a stochastic process which converges in distribution to the target and produces a sequence of \emph{dependent} random variables is often a more tractable task. This is the idea exploited by \acf{MCMC} methods.

A \emph{Markov chain} is an ordered sequence of random variables $\lbrace\rvct{x}_n\rbrace_{n=0}^N$ which have the \emph{Markov property} --- for all $n \in \fset{1 \dots N}$, $\rvct{x}_n$ is conditionally independent of $\lbrace \rvct{x}_n \rbrace_{m < n -1}$ given $\rvct{x}_{n-1}$. This conditional independence structure is visualised as a factor graph in Figure \ref{fig:markov-chain-factor-graph}.

For a Markov chain defined on a general measurable state space $(\set{X}, \sset{F})$, the probability distribution of a state $\rvct{x}_n$ given the state $\rvct{x}_{n-1}$ is specified for each $n \in \fset{1\dots N}$ by a \emph{transition operator}, $\fwdtransop_n : \sset{F} \times \set{X} \to [0, 1]$. In particular the transition operators define a series of regular conditional distributions for each $n \in \fset{1 \dots N}$
\begin{equation}\label{eq:markov-kernel-definition}
  \prob{\rvct{x}_n|\rvct{x}_{n-1}}(\set{A} \gvn \vct{x}) =
  \fwdtransop_n\lpa\set{A} \gvn \vct{x}\rpa
  \quad 
  \forall \set{A} \in \sset{F},~
  \vct{x} \in \set{X}.
\end{equation}
We will often assume that the chain is \emph{homogeneous}, i.e. that the same transition operator is used for all steps $\fwdtransop_n = \fwdtransop ~\forall n\in\fset{1 \dots N}$. 

%A Markov chain is generated using a \emph{Markov kernel} or \emph{transition operator}, which is a regular conditional probability $\prob{\rvct{x}^{(n+1)}}(\set{A} \gvn \rvct{x}_n)$, which defines the probability of the state $\rvct{x}^{(n+1)}$ being in $\set{A}$ conditioned on the state $\rvct{x}_n$. 
%A Markov chain is generated using a \emph{transition density}\footnote{More generally we can define a \emph{Markov kernel} as a regular conditional probability $\prob{\rvct{x}^{(n+1)}}(\set{A} \gvn \rvct{x}_n)$, which defines the probability of the state $\rvct{x}^{(n+1)}$ being in $\set{A}$ conditioned on the state $\rvct{x}_n$. The transition density corresponding to this Markov kernel may not be well defined, for example in the common case where the Markov kernel includes a singular measure term corresponding to remaining at the current state. However as we did previously when discussing deterministic factors, in the interests of readability we will informally treat the Dirac delta as defining the density of a singular measure, and refer to a transition density even when this is not strictly defined.} which defines the probability density of the next state given the current state. We will use the following shorthand notation for transition densities 
%\begin{equation}\label{eq:time-dependent-transition}
%  \fwdtrans_n\lpa \vct{x}' \gvn \vct{x}\rpa = \pden{\rvct{x}_n|\rvct{x}_{n-1}}(\vct{x}'\gvn%\vct{x}).
%\end{equation}
%If the transition density is the same for all steps, which we will assume in most cases, the Markov %chain is said to be \emph{homogeneous} and we will drop the subscript on the transition density.

The key property required of a transition operator for use in \ac{MCMC} methods is that the target distribution $\tgtprob$ is \emph{invariant} under the transition, that is it satisfies
\begin{equation}
  \label{eq:invariant-distribution}
  \tgtprob(\set{A}) = \int_{\set{X}} \fwdtransop(\set{A} \gvn \vct{x}) \,\tgtprob(\dr\vct{x})
  \quad\forall \set{A} \in \sset{F},  
\end{equation}
The invariance property means that if a chain state $\rvct{x}_n$ is distributed according to the target $\tgtprob$, all subsequent chain states $\rvct{x}_{n+1},\,\rvct{x}_{n+2}\dots$ will also be marginally distributed according to the target. Therefore given a single random sample $\rvct{x}_{0}$ from the target distribution, a series of dependent states marginally distributed according to the target could be generated and used to form Monte Carlo estimates of expectations.

Being able to generate even one exact sample from a complex high-dimensional target distribution is generally infeasible. Importantly however the marginal distribution on the chain state $\prob{\rvct{x}_n}$ of a Markov chain with a transition operator which leaves the target distribution invariant will converge to the target distribution irrespective of the distribution of the initial chain state if the target distribution is the \emph{unique} invariant distribution of the chain.%, in which case it is said to be the chain's \emph{stationary distribution}.

To have a unique invariant distribution, a chain must be \emph{irreducible} and \emph{aperiodic} \citep{tierney1994markov}. For a chain on a measurable space $(\set{X},\sset{F})$, irreducibility is defined with respect to a measure $\nu$, which could but does not necessarily need to be the target distribution $\tgtprob$. A chain is $\nu$-irreducible if starting at any point in $\set{X}$ there is a non-zero probability of moving to any set with positive $\nu$-measure in a finite number of steps, i.e.
\begin{equation}\label{eq:irreducibility-criteria}
\begin{split}
  \forall \vct{x} \in \set{X},\, \set{A} \in \sset{F} : \nu(\set{A}) > 0
  ~~\exists\, m \in \integers^+ :
  \prob{\rvct{x}_m|\rvct{x}_0}(\set{A} \gvn \vct{x}) > 0. 
\end{split}
\end{equation}  
A chain is periodic (and aperiodic otherwise) if disjoint regions of $\set{X}$ are visited cyclically, i.e. there exists an integer $r > 1$ and an ordered set of $r$ disjoint $\tgtprob$-positive subsets of $\set{X}$, $\lbrace \set{A}_i \rbrace_{i=1}^r$ such that $\fwdtransop(\set{A}_{j} \gvn \vct{x}) = 1 ~\forall \vct{x} \in \set{A}_i,~ i \in \fset{1 \dots r}, ~j = (i + 1) \kern-2pt\mod r$.

If we can construct a $\nu$-irreducible and aperiodic Markov chain $\lbrace \rvct{x}_n \rbrace_{n=0}^N$ which has the target distribution $\tgtprob$ as its invariant distribution, then a \ac{MCMC} estimator $\hatf_N = \frac{1}{N}\sum_{n=1}^N f(\rvct{x}_n)$ converges almost surely as $N \to \infty$ to $\bar{f} = \int_{\set{X}} f \dr\tgtprob$ for all starting states except for a $\nu$-null set\footnote{The `except for a $\nu$-null set' caveat can be removed by requiring the stronger property of \emph{Harris recurrence} \citep{harris1956existence}.}% It has been argued \cite{chan1994discussion} that the floating-point implementations used in practice for \ac{MCMC} methods naturally avoid such `measure-theoretic pathologies' though see \citep{roberts2006harris} for a counter argument.}% The \ac{MCMC} implementations we will use in practice will generally ensure Harris recurrence.}%, and even if not the pathology of a chain starting in the `bad' $P$-null set should be easy to identify.}
 \citep{meyn1993markov}. This convergence of \emph{time-averages} (i.e. over states at different steps of the Markov chain) to \emph{space-averages} (i.e. with respect to the stationary distribution across the state space), is termed \emph{ergodicity} and is a consequence of the \emph{Birkhoff--Khinchin ergodic theorem} \citep{birkhoff1931proof}.

Although irreducibility and aperiodicity of a Markov chain which leaves the target distribution invariant are sufficient for convergence of \ac{MCMC} estimators, this does not tell us anything about the rate of that convergence and so how to quantify the error introduced by computing estimates with a Markov chain simulated for only a finite number of steps. Stronger notions of ergodicity can be used to help quantify convergence; we will concentrate on \emph{geometric ergodicity} here. We first define a notion of distance between two measures $\mu$ and $\nu$ on a measurable space $(\set{X},\sset{F})$, the \emph{total variation distance}, as
\begin{equation}\label{eq:total-variation-measures}
  \left\Vert \mu - \nu \right\Vert_{\textsc{tv}} = \sup_{\set{A} \in \sset{F}} \left| \mu(\set{A}) - \nu(\set{A}) \right|.
\end{equation}
For a $\nu$-irreducible and aperiodic chain with invariant distribution $P$ our earlier statement that the distribution on the chain state converges to $P$ can now be restated more precisely as that for $\nu$-almost all initial states $\rvct{x}_0 = \vct{x}$, $\lim_{n\to\infty} \left\Vert \prob{\rvct{x}_n|\rvct{x}_0}\lpa \cdot \gvn \vct{x}\rpa - \tgtprob \right\Vert_{\textsc{tv}} = 0$. Geometric ergodicity makes a stronger statement that the convergence in total variation distance is geometric in $n$, i.e. that
\begin{equation}\label{eq:geometric-ergodicity}
  \left\Vert \prob{\rvct{x}_n|\rvct{x}_0}\lpa \cdot \gvn \vct{x}\rpa - \tgtprob \right\Vert_{\textsc{tv}} \leq m(\vct{x}) r^n
\end{equation}
for a positive measurable function $m$ which depends on the initial chain state $\vct{x}$ and rate constant $r \in [0, 1)$. For chains which are geometrically ergodic, we can derive an expression for the \emph{asymptotic variance} of an \ac{MCMC} estimator $\hatf_N$ related to the variance of a simple Monte Carlo estimator previously considered in Section \ref{subsec:monte-carlo-method}.

\marginpar{A stochastic process is stationary if the joint distribution of the states at any set of time points does not change if all those times are shifted by a constant.}
%\marginpar{A Markov process $\lbrace \rvct{x}_t \rbrace_{t \in \set{T}}$ is stationary if $\prob{\rvct{x}_0} = \prob{\rvct{x}_t} ~\forall t \in \set{T}$ and $\prob{\rvct{x}_0,\rvct{x}_t} = \prob{\rvct{x}_{s},\rvct{x}_{s+t}} ~\forall s, t, s+t \in \set{T}$.}
As in Section \ref{subsec:monte-carlo-method} we define $\rvar{f}_n = f(\rvct{x}_n)$ and $\hatf_N = \frac{1}{N} \sum_{n=1}^N \rvar{f}_n$, with here the $\lbrace \rvct{x}_n \rbrace_{n=1}^N$ the states of a Markov chain. For a homogeneous Markov chain with a unique invariant distribution $P$ which is \emph{stationary}, the marginal distribution on the states $\prob{\rvct{x}_n}$ is equal to $P$ for all $n$ and we can use the expression for the variance of a general Monte Carlo estimator (which did not assume independence of the random variables) stated earlier in \eqref{eq:monte-carlo-variance-general}. Further the stationarity of the chain means that the covariance $\cov{\rvar{f}_n, \rvar{f}_m}$ depends only on the difference $n - m$, and so the variance of the estimator simplifies to
\begin{equation}\label{eq:monte-carlo-variance-stationary}
  \var{\hatf_N}
  =
  \frac{\var{\rvar{f}}}{N}
  \lpa 
    1 + 2\sum_{n=1}^{N-1} \lpa \frac{N-n}{N} \frac{\cov{\rvar{f}_0, \rvar{f}_n}}{\var{\rvar{f}}}\rpa
  \rpa.
\end{equation}
If we multiply both sides of \eqref{eq:monte-carlo-variance-stationary} by $N$ and define $\rho_{n} = \frac{\cov{\rvar{f}_0, \rvar{f}_n}}{\var{\rvar{f}}}$ (the lag $n$ autocorrelations of $\lbrace \rvar{f}_n \rbrace$), under the assumption that $\sum_{n=1}^\infty |\rho_n| < \infty$ in the limit of $N \to \infty$ we have that
\begin{equation}\label{eq:asymptotic-variance}
  \lim_{N\to\infty} \lpa N \, \var{\hatf_N} \rpa = \var{\rvar{f}} \lpa 1 + 2\sum_{n=1}^\infty \rho_n\rpa.
\end{equation}
Now considering a chain which is geometrically ergodic from its initial state, if $\expc{|\rvar{f}|^{2+\delta}}$ is finite for some $\delta > 0$ then it can be shown \citep{chan1994discussion,geyer1998markov,roberts2004general} that \eqref{eq:asymptotic-variance} is also the asymptotic variance for a \ac{MCMC} estimator calculated using the chain states.

%\footnote{Note this is unrelated to the previous definition for importance sampling.}
This motivates a definition of the \ac{ESS} for an \ac{MCMC} estimator $\hatf_N$ computed using a geometrically ergodic chain as
\begin{equation}\label{eq:effective-sample-size-mcmc}
  N_{\textrm{eff}} = \frac{N}{1 + 2\sum_{n=1}^\infty \rho_n}.
\end{equation} 
The \ac{ESS} quantifies the number of independent samples that would be required in a Monte Carlo estimator to give an equivalent variance to the \ac{MCMC} estimator $\hatf_N$ in the asymptotic limit $N \to \infty$. In practice we cannot evaluate the exact autocorrelations and so we can only compute an estimated \ac{ESS}, $\hat{N}_{\textrm{eff}}$, from one or more simulated chains with the estimation method needing to be carefully chosen to ensure reasonable values \citep{thompson2010comparison}. Although the assumption of geometric ergodicity can often be hard to verify in practice and \ac{ESS} estimates can give misleading results in chains far from convergence, when used appropriately estimated \acp{ESS} can still be a useful heuristic for evaluating and comparing the efficiency of Markov chain estimators and are often available as a standard diagnostic in \ac{MCMC} software packages \citep{plummer2006coda,carpenter2016stan,salvatier2016probabilistic}.

So far we have not discussed how to construct a transition operator giving a chain with the required invariant distribution. As a notational convenience we will consider the transition operator as being specified by a conditional density we term the \emph{transition density} $\fwdtrans : \set{X} \times \set{X}  \to [0, \infty)$ which is defined with respect to a base measure $\mu$ (which we assume to be the same as that which the target density we wish to integrate against is defined with respect to, hence the reuse of notation). The transition operator is then 
\begin{equation}
  \fwdtransop(\set{A} \gvn \vct{x}) =
  \int_{\set{A}} \fwdtrans(\vct{x}' \gvn \vct{x}) \,\mu(\dr\vct{x}')
  \quad \forall \set{A} \in \sset{F},~ \vct{x} \in \set{X}. 
\end{equation}
In practice the probability measure defined by a transition operator will often have a singular component, for example corresponding to a non-zero probability of the chain remaining in the current state. In this case $\fwdtransop$ is not absolutely continuous with respect to $\mu$ and a transition density is not strictly well defined. As we did in the previous chapter however we will informally use Dirac deltas to represent a `density' of singular measures, and so still consider a transition density as existing. The requirement that the transition operator leaves the target distribution invariant, can then be expressed in terms of the target density $\tgtdens$ and transition density $\fwdtrans$ as
\begin{equation}
  \label{eq:invariant-density}
  \tgtdens(\vct{x}') = 
  \int_{\set{X}} \fwdtrans\lpa \vct{x}' \gvn \vct{x}\rpa \,\tgtdens(\vct{x}) \,\mu(\dr\vct{x}) 
  \quad\forall\vct{x}'\in \set{X}.
\end{equation}
Finding a transition density which leaves the target density invariant by satisfying \eqref{eq:invariant-density} seems difficult in general as it involves evaluating an integral against the target density - precisely the computational task which we have been forced to seek approximate solutions to. We can make progress by considering the joint density of a pair of successive states for a chain with invariant distribution $P$ that has converged to stationarity. Then we have that
%\vspace{-1mm}
\begin{equation}\label{eq:chain-state-pair-joint-density-stationary}
  \pden{\rvct{x}_{n}, \vct{x}_{n-1}}(\vct{x}',\vct{x}) = 
  \pden{\rvct{x}_{n}|\vct{x}_{n-1}}(\vct{x}'\gvn\vct{x}) \,\pden{\rvct{x}_{n-1}}(\vct{x}) =
  \fwdtrans(\vct{x}'\gvn\vct{x}) \,\tgtdens(\vct{x}).
\end{equation}
%\vspace{-1mm}
We can also consider factorising this joint density into the product of the marginal density of the current state $\pden{\rvct{x}_{n}}$ and the conditional density of the previous state given the current state $\pden{\rvct{x}_{n-1}|\vct{x}_{n}}$. Due to stationarity $\pden{\rvct{x}_n}$ is also equal to $\tgtdens$ and so we have that $\pden{\rvct{x}_{n-1}|\vct{x}_{n}}$ must be the density of a transition operator which also leaves $\tgtprob$ invariant, corresponding to a time reversed version of the original (stationary) Markov chain\footnote{The time reversal of a Markov chain is always itself a Markov chain irrespective of stationarity (as the defining conditional independence structure is symmetric with respect to the direction of time), however the reverse of a homogeneous Markov chain which is not stationary will not in general itself be homogeneous.}. If we therefore denote $\bwdtrans=\pden{\rvct{x}_{n-1}|\vct{x}_{n}}$ (and which we will term the \emph{backward transition density} in contrast to $\fwdtrans$ which in this context we will qualify as the \emph{forward transition density}), we have that
\begin{equation}\label{eq:foward-backward-transition-density-balance}
  \fwdtrans(\vct{x}'\gvn\vct{x}) \,\tgtdens(\vct{x}) = 
  \bwdtrans(\vct{x}\gvn\vct{x}') \,\tgtdens(\vct{x}')
  ~~\forall \vct{x} \in \set{X},\, \vct{x}' \in \set{X}.
\end{equation}
Integrating both sides with respect to $\vct{x}$, we have that $\forall \vct{x}' \in \set{X}$
\begin{equation}\label{eq:backward-transition-density-invariance-derivation}
\begin{split}
  \int_{\set{X}} \fwdtrans(\vct{x}'\gvn\vct{x}) \,\tgtdens(\vct{x}) \,\mu(\dr\vct{x}) 
  =
  \int_{\set{X}} \bwdtrans(\vct{x}\gvn\vct{x}')  \,\mu(\dr\vct{x}) \,\tgtdens(\vct{x}')
  =
  \tgtdens(\vct{x}'),
\end{split}
\end{equation}
and so that \eqref{eq:invariant-density} is satisfied, with the last inequality arising due to $\bwdtrans$ being a normalised density on its first argument. Therefore if we can find a pair of transition densities, $\fwdtrans$ and $\bwdtrans$, satisfying \eqref{eq:foward-backward-transition-density-balance}, then the transition operator specified by $\fwdtrans$ will leave the target distribution $\tgtprob$ invariant (and by an equivalent argument so will the transition operator specified by $\bwdtrans$). We can further simplify \eqref{eq:foward-backward-transition-density-balance} by requiring that $\fwdtrans = \bwdtrans = \trans$, i.e. that both forward and backward transition densities (and corresponding operators) take the same form and so that the chain at stationarity is \emph{reversible}, in which case have that
\begin{equation}\label{eq:detailed-balance}
  \trans(\vct{x}'\gvn\vct{x}) \,\tgtdens(\vct{x}) = 
  \trans(\vct{x}\gvn\vct{x}') \,\tgtdens(\vct{x}')
  ~~\forall \vct{x} \in \set{X},\, \vct{x}' \in \set{X}.
\end{equation}
This is often termed the \emph{detailed balance} condition. Importantly both the detailed balance \eqref{eq:detailed-balance} and \emph{generalised balance} \eqref{eq:foward-backward-transition-density-balance} conditions can also be written in terms of the unnormalised density $\utgtdens$ by multiplying both sides by $Z$, and so can be checked even when $Z$ is unknown.

%(a criticism made of some of these results is that choice of `reference' reversible chain to compare to can be somewhat arbitrary \citep{mira2000non})
The restriction to reversible transition operators in detailed balance, while sufficient for \eqref{eq:invariant-density} to hold is not necessary. Markov chains which satisfy the generalised balance condition but not detailed balance are termed \emph{non-reversible}, and there are theoretical results suggesting that non-reversible Markov chains can sometimes achieve significantly improved convergence compared to related reversible chains \citep{diaconis2000analysis,neal2004improving,ichiki2013violation}. While there are several general purpose frameworks for specifying reversible transition operators which leave a target distribution invariant, developing methods for constructing irreversible transition operators with a desired invariant distribution has proven more challenging. The approaches proposed to date are generally limited in practice to special cases such as finite state spaces \citep{suwa2010markov,turitsyn2011irreversible,sun2010improving} or chains with tractable invariant distributions such as the multivariate normal \citep{bierkens2016non}. 

Nonetheless non-reversible Markov chains are still commonly used in \ac{MCMC} applications. Given a set of transition operators which each individually leave a target distribution invariant, the sequential composition of the transition operators will by induction necessarily also leave the target distribution invariant. Even if the individual transition operators are all reversible, the overall sequential composition will generally not be (instead having an adjoint `backward' operator corresponding to applying the individual transitions in the reversed order). Sequentially combining several reversible transition operators is common in \ac{MCMC} implementations, though this is more often the result of each individual operator not meaning the requirements for ergodicity in isolation and so needing to be combined with other operators, rather than due to a specific aim of introducing irreversibility.

Having now introduced the key theory underlying \ac{MCMC} methods, we will now discuss practical implementations of the approach. In the following sub-sections we review two of the most popular frameworks for constructing reversible transition operators which leave a target distribution invariant: the \emph{Metropolis--Hastings} algorithm and \emph{Gibbs sampling}. %, \emph{slice sampling} and \emph{Hamiltonian Monte Carlo}. %These methods are central to the contributions introduced in the subsequent chapters in this thesis.

\subsection{Metropolis--Hastings}

\begin{figure}[t]
\centering
\includetikz{metropolis-hastings}
\caption[Visualisation of Metropolis--Hastings algorithm.]{Visualisation of Metropolis--Hastings algorithm in a univariate target distribution. The green curves shows the unnormalised target density. The arrows indicate the current chain state. The orange curves show the density of proposed moves from this state, with the left axis using a narrower proposal than the right. The violet curves show the proposal density scaled by the acceptance probability of the proposed move, this reducing the probability of transitions to states with lower density than the current state. The orange region between the violet and orange curves represents the probability mass reallocated to rejections by the downscaling by the acceptance function. The broader proposal in the right axis has an increased probability of making a move to the other mode in the target density but at a cost of an increased rejection probability.}
\label{fig:metropolis-hastings}
\end{figure}

\marginpar{Although the algorithm has come to be commonly known by Edward Metropolis' name as first author on the 1953 paper \citep{metropolis1953equation}, it is believed that Arianna and Marshall Rosenbluth, two of the other co-authors, were the main contributors to the development of the algorithm \citep{gubernatis2005marshall}.}
The seminal \emph{Metropolis--Hastings} algorithm provides a general framework for constructing Markov chains with a desired invariant distribution and is ubiquitous in \ac{MCMC} methodology. The original Rosenbluth--Teller--Metropolis variant of the algorithm \citep{metropolis1953equation} dates to the very beginnings of the Monte Carlo method, having being first implemented on Los Alamos' MANIAC\footnote{\emph{Mathematical Analyzer, Numerical Integrator and Computer.}} one of the earliest programmable computers. The method was generalised in a key paper by Hastings \citep{hastings1970monte}, and the optimality among several competing alternatives of the form now used demonstrated by Peskun \cite{peskun1973optimum}. An extension to Markov chains on trans-dimensional spaces was proposed by Green \citep{green1995reversible}.

\begin{algorithm}[!t]
\caption{Metropolis--Hastings.}
\label{alg:metropolis-hastings}
\input{algorithms/metropolis-hastings}
\end{algorithm}

An outline of the method is given in Algorithm \ref{alg:metropolis-hastings} and a visualisation of its application to a univariate target distribution shown in Figure \ref{fig:metropolis-hastings}. The key idea is to propose updates to the state using an arbitrary transition operator and then correct for this transition operator not necessarily leaving the target distribution invariant by stochastically accepting or rejecting the proposal. If a proposal is rejected the chain remains at the current state, otherwise the chain state takes on the proposed value. The transition density corresponding to Algorithm \ref{alg:metropolis-hastings} is
\begin{equation}\label{eq:metropolis-hastings-transition-density}
\begin{split}
  \trans(\vct{x}' \gvn \vct{x}) &=
  \alpha(\vct{x}'\gvn \vct{x})\,q(\vct{x}'\gvn \vct{x}) + \, \\
  &\phantom{=}  
  \lpa 1 - 
  \int_{\set{X}} \alpha(\vct{x}^*\gvn \vct{x})\,q(\vct{x}^*\gvn \vct{x}) \,\mu(\dr\vct{x}^*)
  \rpa
  \delta(\vct{x}' - \vct{x}),
\end{split}
\end{equation}
with the \emph{acceptance probability} $\alpha : \set{X} \times \set{X} \to [0,1]$ defined as
\begin{equation}\label{eq:metropolis-hastings-acceptance-probability}
  \alpha(\vct{x}'\gvn \vct{x}) =
  \min\lbr 1,\, \frac{q(\vct{x}\gvn\vct{x}')\,\tgtdens(\vct{x}')}{q(\vct{x}'\gvn\vct{x})\,\tgtdens(\vct{x})} \rbr =
  \min\lbr 1,\, \frac{q(\vct{x}\gvn\vct{x}')\,\utgtdens(\vct{x}')}{q(\vct{x}'\gvn\vct{x})\,\utgtdens(\vct{x})} \rbr,
\end{equation}
and  $q : \set{X} \times \set{X} \to [0, \infty)$ the  \emph{proposal density}. %The overall algorithm defines a reversible transition operator which leaves the target distribution $\tgtprob$ invariant as we will now show.

The original Rosenbluth--Teller--Metropolis algorithm used a symmetric proposal density $q(\vct{x}' \gvn \vct{x}) = q(\vct{x} \gvn \vct{x}') ~\forall \vct{x} \in \set{X},\,\vct{x}'\in\set{X}$ (with the extension to the non-symmetric case being due to Hastings \citep{hastings1970monte}), in which case the acceptance probability definition simplifies to
\begin{equation}\label{eq:metropolis-acceptance-probability}
  \alpha(\vct{x}'\gvn \vct{x}) =
  \min\lbr 1,\, \frac{\tgtdens(\vct{x}')}{\tgtdens(\vct{x})} \rbr =
  \min\lbr 1,\, \frac{\utgtdens(\vct{x}')}{\utgtdens(\vct{x})} \rbr.
\end{equation}
Note that in both \eqref{eq:metropolis-hastings-acceptance-probability} and \eqref{eq:metropolis-acceptance-probability} the target density only appears as a ratio and so only need be known up to a constant.

For the purposes of verifying the detailed balance condition \eqref{eq:detailed-balance}, the density of \emph{self-transitions}, i.e. a transition to the same state, can be ignored as \eqref{eq:detailed-balance} is trivially satisfied for $\vct{x}' = \vct{x}$. Considering therefore the cases $\vct{x} \neq \vct{x}'$ where the Dirac delta term representing the singular component corresponding to rejected proposals can be neglected, we have $\forall \vct{x} \in \set{X},\,\vct{x}' \in \set{X} : \vct{x} \neq \vct{x}$
\begin{align}\label{eq:metropolis-hastings-detailed-balance-derivation}
  \trans(\vct{x}'\gvn\vct{x})\,\tgtdens(\vct{x}) &=
  \min\lbr 
    1,\, 
    \frac
      {q(\vct{x}\gvn\vct{x}')\,\tgtdens(\vct{x}')}
      {q(\vct{x}'\gvn\vct{x})\,\tgtdens(\vct{x})} 
  \rbr\,
  q(\vct{x}'\gvn \vct{x})\,\tgtdens(\vct{x})
  \\
  &=
  \min\lbr 
    q(\vct{x}'\gvn \vct{x})\,\tgtdens(\vct{x}),\, 
    q(\vct{x}\gvn\vct{x}')\,\tgtdens(\vct{x}')
  \rbr
  \\
  &=
  \min\lbr 
    \frac
      {q(\vct{x}'\gvn\vct{x})\,\tgtdens(\vct{x})}
      {q(\vct{x}\gvn\vct{x}')\,\tgtdens(\vct{x}')} 
    ,\, 1
  \rbr\,
  q(\vct{x}\gvn \vct{x}')\,\tgtdens(\vct{x}')
  \\
  &=
  \trans(\vct{x}\gvn\vct{x}')\,\tgtdens(\vct{x}').
\end{align}
Therefore the detailed balance condition is satisfied, and the Metropolis--Hastings transition operator leaves the target distribution $\tgtprob$ invariant.

A special case for chains on a Euclidean state space $\set{X} = \reals^D$, is when the proposal transition operator is deterministic and corresponds to a differentiable involution of the current state. Let $\vct{\phi} : \set{X} \to \set{X}$ be an involution, i.e. $\vct{\phi} \circ \vct{\phi}(\vct{x}) = \vct{x} ~\forall \set{X}$ with Jacobian determinant $\jacobproddet{\vct{\phi}}{\vct{x}} = \left|\pd{\vct{\phi}(\vct{x})}{\vct{x}}\right|$ which is defined and non-zero $\tgtprob$-almost everywhere. Then if we define a transition operator via the transition density
\begin{equation}\label{eq:metropolis-hastings-transition-deterministic-proposal}
\begin{split}
  \trans(\vct{x}' \gvn \vct{x}) &=
  \delta\lpa\vct{x}' - \vct{\phi}(\vct{x})\rpa \alpha(\vct{x}) +
  \delta(\vct{x}' - \vct{x}) \lpa 1 - \alpha(\vct{x}) \rpa,
  \\
  \alpha(\vct{x}) &=
  \min\lbr 
    1,\,\frac{\tgtdens \circ \vct{\phi}(\vct{x})}{\tgtdens(\vct{x})} \jacobproddet{\vct{\phi}}{\vct{x}}
  \rbr,
\end{split}
\end{equation}
then this transition operator will leave the target distribution $\tgtprob$ invariant. This deterministic transition operator variant is as a special case of the trans-dimensional Metropolis--Hastings extension introduced by Green \citep{green1995reversible,geyer2003metropolis}. To generate from this transition operator from a current state $\vct{x}$ we compute the proposed move $\vct{\phi}(\vct{x})$ and accept the move with probability $\alpha(\vct{x})$. We can demonstrate that this transition operator leaves $\tgtprob$ invariant by directly verifying \eqref{eq:invariant-density}
\begin{align}
  \label{eq:mh-det-proposal-derivation-1}
  &\int_{\set{X}} \trans(\vct{x}' \gvn \vct{x}) \, \tgtdens(\vct{x}) \,\dr\vct{x}
  \\
  \label{eq:mh-det-proposal-derivation-2}
  &=
  \int_{\set{X}} 
    \delta\lpa\vct{x}' - \vct{\phi}(\vct{x})\rpa \,\alpha(\vct{x}) \, \tgtdens(\vct{x}) +
    \delta(\vct{x}' - \vct{x}) \lpa 1 - \alpha(\vct{x}) \rpa \, \tgtdens(\vct{x})
  \,\dr\vct{x}
  \\
  \label{eq:mh-det-proposal-derivation-3}
  &=
  \int_{\set{X}} 
    \delta\lpa\vct{x}' - \vct{y}\rpa \,\alpha\circ\vct{\phi}(\vct{y}) \, 
    \tgtdens\circ\vct{\phi}(\vct{y}) \, \jacobproddet{\vct{\phi}}{\vct{y}} \,\dr\vct{y} +
  \lpa 1 - \alpha(\vct{x}') \rpa \, \tgtdens(\vct{x}')
  \\
  \label{eq:mh-det-proposal-derivation-4}
  &=
  p(\vct{x}') +
  \alpha\circ\vct{\phi}(\vct{x}') \, \tgtdens\circ\vct{\phi}(\vct{x}') \, 
  \jacobproddet{\vct{\phi}}{\vct{x}'} -
  \alpha(\vct{x}') \, \tgtdens(\vct{x}').
\end{align}
In going from \eqref{eq:mh-det-proposal-derivation-3} to \eqref{eq:mh-det-proposal-derivation-4} we use a change of variables $\vct{y} = \vct{\phi}(\vct{x})$ in the integral. As $\vct{\phi}$ is an involution we have that $\vct{\phi}\circ\vct{\phi}(\vct{x}') = \vct{x}'$ and $\jacobianproddet{\vct{\phi}}\circ\vct{\phi}(\vct{x}') = \jacobproddet{\vct{\phi}}{\vct{x}'}^{-1}$ and so
\begin{equation*}\label{eq:mh-det-proposal-derivation-5}
  \alpha\circ\vct{\phi}(\vct{x}')  \tgtdens\circ\vct{\phi}(\vct{x}') 
  \jacobproddet{\vct{\phi}}{\vct{x}'} =
  \min\lbr 
    \tgtdens\circ\vct{\phi}(\vct{x}') \jacobproddet{\vct{\phi}}{\vct{x}'},
    p(\vct{x}')
  \rbr 
  =
  \alpha(\vct{x}')\tgtdens(\vct{x}').
\end{equation*}
The last two terms in \eqref{eq:mh-det-proposal-derivation-4} therefore cancel and so \eqref{eq:invariant-density} is satisfied by the transition operator defined by \eqref{eq:metropolis-hastings-transition-deterministic-proposal}. 

Although this transition operator leaves the target distribution $P$ invariant, it is clear that it will not generate an ergodic Markov chain. Starting from a point $\vct{x}$ the next chain state will be either $\vct{\phi}(\vct{x})$ if the proposed move is accepted or $\vct{x}$ if rejected. In the former case the next proposed move will be to $\vct{\phi} \circ \vct{\phi}(\vct{x}) = \vct{x}$ i.e. back to the original state. Therefore the chain will visit a maximum of two states. However as noted previously we can sequentially compose individual transition operators which all leave a target distribution invariant. Therefore a deterministic proposal Metropolis--Hastings transition can be combined with other transition operators to ensure the chain is irreducible and aperiodic.

In general for a Metropolis--Hastings transition operator to be irreducible, it is necessary that the proposal operator is irreducible \citep{tierney1994markov}, however this is not sufficient. For a target density which is positive everywhere on $\set{X} = \reals^D$, then a sufficient but not necessary condition for irreducibility is that the proposal density is positive everywhere \citep{roberts2004general}. If the set of points with a non-zero probability of rejection has non-zero $\tgtprob$-measure, then the transition operator is aperiodic \citep{tierney1994markov}.

A common choice of proposal density when the target distribution is defined on $\reals^D$ is a multivariate normal density centred at the current state i.e. $q(\vct{x}'\gvn\vct{x}) = \nrm{\vct{x}' \gvn \vct{x}, \mtx{\Sigma}}$ which satisfies the positivity condition for irreducibility. In general we would achieve optimal performance with a proposal density covariance $\mtx{\Sigma}$ which is proportional to the covariance of the target distribution \citep{rosenthal2011optimal}. In practice we do not have access to the true covariance and so typically an isotropic proposal density is used with covariance $\mtx{\Sigma} = \sigma^2\mathbf{I}$ controlled by a single scale parameter $\sigma$, often termed the \emph{step size} or \emph{proposal width}. This proposal density is symmetric so the simplified acceptance rule \eqref{eq:metropolis-acceptance-probability} can be used, further the proposal density depends only on the difference $\vct{x}' - \vct{x}$ with Metropolis--Hastings methods having these properties often termed \emph{random-walk Metropolis}. 

Random walk Metropolis methods have been extensively theoretically studied, with sufficient conditions known in some cases to ensure geometric ergodicity of a chain \citep{mengersen1996rates,roberts1996geometric} though these can be hard to verify in practical problems. There has also been much work on practical guidelines and methods for tuning the free parameters in the algorithm, including approaches for tuning the step-size using acceptance rates \citep{gelman1997weak,roberts2001optimal} and adaptive variants which automatically estimate a non-isotropic proposal covariance \citep{haario2001adaptive,rosenthal2011optimal}.

In general the choice of proposal density will be key in determining the efficiency of Metropolis--Hastings \ac{MCMC} methods. Ideally we want to be able to propose large moves in the state space to reduce the dependencies between successive chain states and so increase the number of effective samples, however this needs to be balanced with maintaining a reasonable acceptance probability with large proposed moves often having a low acceptance probability. Figure \ref{fig:metropolis-hastings} gives an illustration of this trade-off in a one-dimensional example. 

\begin{figure}[!t]
\centering
\includetikz{concentration-of-measure-example}
\caption[Concentration of measure in high dimensions.]{Illustration of concentration of measure in a multivariate normal distribution. The plots shows the probability density of the distance from the origin $\rvar{r} = \left\Vert\rvct{x}\right\Vert_2$ of a $D$-dimensional multivariate normal random vector $\rvct{x} \sim \nrm{\vct{0},\mathbf{I}}$ for different dimensionalities $D$. As the dimension increases most of the mass concentrates away from the origin around a spherical shell of radius $\sqrt{D}$. For a multivariate normal random vector with mean $\vct{\mu}$ and covariance $\mtx{\Sigma}$ this generalises to the mass being mainly in an ellipsoidal shell aligned with the eigenvectors of $\mtx{\Sigma}$ and centred at $\vct{\mu}$.}
\label{fig:concentration-of-measure-gaussian}
\end{figure}

In high-dimensional spaces this issue is much more severe due to the phenomenon of \emph{concentration of measure}: in probability distributions defined on high-dimensional spaces most of the probability mass will tend to be concentrated into a `small' subset of the space \citep{mackay2003information,barp2017geometry}. An illustration of this phenomenon for the multivariate normal distribution is shown in Figure \ref{fig:concentration-of-measure-gaussian}, where the mass in high dimensions is mostly located in a thin ellipsoidal shell. The region where most of the mass concentrates, termed the \emph{typical set} of the distribution, will for the target distributions of interest generally have a significantly more complex geometry. Finding proposals which can make large moves in such settings is challenging: moves in most directions will have a probability of acceptance which exponentially drops to zero as the distance away from the current state is increased and so simple proposal densities which ignore the geometry the typical set such as those used in random-walk Metropolis will need to make very small moves to have a reasonable probability of acceptance \cite{betancourt2017conceptual}.

%To tackle this issue methods have been proposed which exploit more information about the target distribution than just the point evaluations of the density in the Metropolis--Hasting acceptance probability term. \ac{HMC} methods, which make use of the \emph{gradient} of the target density, are a particularly important class of such methods and a central focus of this thesis. We will discuss \ac{HMC} methods in detail in Chapter \ref{ch:hamiltonian-monte-carlo}.

%which can be efficiently automatically calculated using the reverse-mode automatic differentiation approach discussed in the previous chapter (\S\ref{subsec:computation-graphs}),

\subsection{Gibbs sampling}

\begin{figure}[!t]
\centering
\includetikz{gibbs-sampling}
\caption[Visualisation of Gibbs sampling.]{Schematic of Gibbs sampling transition in a bivariate normal target distribution (ellipses indicate constant density contours). Given an initial state $\rvct{x} = (x_1,x_2)$, the $\rvar{x}_1$ (horizontal) co-ordinate is first updated by independently sampling from the normal conditional $\pden{\rvar{x}_1|\rvar{x}_2}(\cdot\gvn x_2)$, represented by the orange curve. The new partially updated state is then $\rvct{x} = (x_1^*, x_2)$. The second $\rvar{x}_2$ (vertical) co-ordinate is then independently resampled from the normal conditional $\pden{\rvar{x}_2|\rvar{x}_1}(\cdot\gvn x_1^*)$, shown by the green curve. The final updated state is then $\rvct{x} = (x_1^*, x_2^*)$.}
\label{fig:gibbs-sampling}
\end{figure}

\begin{algorithm}[!t]
\caption{Sequential-scan Gibbs.}
\label{alg:gibbs-sampling}
\input{algorithms/sequential-scan-gibbs}
\end{algorithm}

\emph{Gibbs sampling} \citep{geman1984stochastic,gelfand1990sampling}, originally proposed by Geman and Geman for image restoration using a Markov random field image model, is based on the observation that a valid transition operator for a joint target distribution across many variables, is one which updates only a subset of the variables and leaves the conditional distribution on that subset given the rest invariant. Although if used in isolation a transition operator which only updates some components of the state will not give an ergodic chain, as discussed previously multiple transition operators can be combined together to achieve ergodicity. 

More specifically the original formulation of Gibbs sampling defines a Markov chain by sequentially independently resampling each individual variable in the model from its conditional distribution given the current values of the remaining variables. If $\set{I}$ is an index set over the individual variables in the vector target state $\rvct{x}$, then for each $i \in \set{I}$ we partition the state $\rvct{x}$ into the $i^{\rm th}$ variable $\rvar{x}_i$ and a vector containing all the remaining variables $\rvct{x}_{\backslash i}$. For each $i \in \set{I}$ the target density can be factorised in to the marginal density $\tgtdens_{\backslash i}$ on $\rvct{x}_{\backslash i}$ and conditional density $\tgtdens_i$ on $\rvar{x}_i$ given $\rvct{x}_{\backslash i}$, i.e.
\begin{equation}\label{eq:gibbs-sampling-complete-conditional-factorisation}
  \tgtdens(\vct{x}) = \tgtdens_{i}(x_i \gvn \vct{x}_{\backslash i}) \, \tgtdens_{\backslash i}(\vct{x}_{\backslash i}),
\end{equation}
with the conditional densities $\lbrace \tgtdens_i \rbrace_{i \in \set{I}}$ termed the \emph{complete conditionals} of the target density. If each of these complete conditionals corresponds to a distribution we can generate samples from (for example using a transform method or rejection sampling) then we can apply the sequential Gibbs sampling transition operator defined in Algorithm \ref{alg:gibbs-sampling} and visualised for a bivariate example in Figure \ref{fig:gibbs-sampling}.

The sequential Gibbs transition is irreducible and aperiodic under mild conditions \citep{roberts1994simple,chan1993asymptotic}. Rather than using a deterministic sequential scan through the variables, an alternative is to randomly sample without replacement the variable to update on each iteration; unlike the sequential scan version this defines a reversible transition operator. The random update variant is more amenable to theoretical analysis, however in practice the ease of implementation of the sequential scan variant and computational benefits in terms of memory access locality mean it seems to be more often used in practice \citep{he2016scan}. A compromise between the completely random updates and a sequential scan is to randomly permute the update order after each complete scan.

A apparent advantage of Gibbs sampling over Metropolis--Hastings is the lack of a proposal density which needs to be tuned. This has helped popularise `black-box' implementations of Gibbs sampling such as the probabilistic modelling packages BUGS \citep{gilks1994language} and JAGS \citep{plummer2003jags}. A well-known issue with Gibbs sampling however is that its performance is highly dependent on the parameterisation used for the target density \citep{raftery1991many}, with strong correlations between variables leading to large dependencies between successive states and slow convergence to stationarity. This can be alleviated in some cases by using a suitable reparameterisation to reduce dependencies between variables, however this restores the difficulty of tuning free parameters.

Gibbs sampling updates do not necessarily need to be performed by sampling from complete conditionals of single variables - in some cases the complete conditional of a vector of variables has a tractable form which can be sampled from as a `block'; this motivates the name \emph{block Gibbs sampling} for such variants. By accounting for the dependencies between the variables in a block this can help alleviate some of the issues with highly correlated targets where applicable.

Compound terms such as \emph{Metropolis-within-Gibbs} are sometimes used to refer to methods which sequentially apply Metropolis transition operators which each update only a subset of variables in the target distribution. We will however consider the defining feature of Gibbs sampling as being exact sampling from one or more conditionals rather than sequentially applying transition operators which update only subsets of variables and so will only refer to `Gibbs sampling' in that context.

\section{Auxiliary variable methods}

%Gibbs sampling requires the complete conditionals of the target distribution to be tractable to sample from which limits the class of models it can be applied to. Although this can be side-stepped by using (for example) a Metropolis--Hastings method to perform updates leaving the individual complete conditionals invariant, this then requires tuning of the proposals and the associated issues of Metropolis--Hastings methods. Further for target distributions with strong dependencies between the variables Gibbs will typically explore the state space very slowly, requiring long chains for adequate convergence. The performance of random-walk Metropolis methods are very dependent on the tuning of the proposal step-size, and even well tuned proposals will generally perform poorly in target distributions in which the appropriate scale of updates varies across the space or there are non-linear dependencies between variables. Concentration of measure makes this issue even more severe in high-dimensional spaces \citep{betancourt2017geometric,barp2017geometry} and the random-walk nature of the dynamics in both Gibbs sampling and random-walk Metropolis \citep{betancourt2015hamiltonian,betancourt2017geometric} will tend to lead to slow diffusive exploration of the space.

%Although both Gibbs sampling and random-walk Metropolis are commonly used in practice, as discussed above both have drawbacks when applied to complex high-dimensional target distributions. This has spurred the development of \ac{MCMC} methods which are able to better adapt to the local geometry of the target distribution. Typically these methods will have a significantly higher computational cost per update but this is often balanced by    

%can therefore bring large gains when performing inference in complex models. 
 
%One particularly succesful approach for constructing more efficient Markov transition operators, has been the introduction of \emph{auxiliary variables} in to the chain state.
 
Although Gibbs sampling and random-walk Metropolis are commonly used in practice, as discussed above both have drawbacks when applied to complex high-dimensional target distributions. One approach which has proven particularly successful for constructing alternative Markov transition operators which can overcome some of these shortcomings is the introduction of \emph{auxiliary variables} in to the chain state. For concreteness of notation in the following discussion we let the variables of interest, which we term the target variables, be represented by the random vector $\rvct{x} \in \set{X}$ and the introduced auxiliary variables by the random vector $\rvct{a} \in \set{A}$. We assume for generality here multiple auxiliary variables are introduced, however methods using  a single scalar auxiliary variable are a common special case. 

One way of defining a joint distribution across the target and auxiliary variables is to specify an arbitrary conditional distribution $\prob{\rvct{a}|\rvct{x}}$ and choose the marginal distribution $\prob{\rvct{x}}$ to be equal to the target distribution $\tgtprob$. Given samples from a joint distribution we can estimate expectations with respect to the marginal distribution on a subset of the variables by simply ignoring the dimensions of the sampled state we wish to marginalise over. Therefore if we can construct a Markov chain with the resulting joint distribution $\prob{\rvct{x},\rvct{a}}$ as its unique invariant distribution, then we can use the target variable components of the sampled states to estimate expectations with respect to the target distribution. We will consider two \ac{MCMC} methods using this approach, \emph{slice sampling} and \emph{Hamiltonian Monte Carlo} in Sections \ref{subsec:slice-sampling} and \ref{subsec:hamiltonian-monte-carlo}.

An alternative approach is to instead construct a joint distribution on the target and auxiliary variables such that the regular conditional distribution $\prob{\rvct{x}|\rvct{a}}$ is equal to the target distribution $\tgtprob$ across some set of values $\set{A}^* \subset \set{A}$ of the auxiliary variables. In terms of the density $\pden{\rvct{x}|\rvct{a}}$ this requires that
\begin{equation}\label{eq:conditional-auxiliary-variable-property}
  \pden{\rvct{x}|\rvct{a}}(\vct{x}\gvn\vct{a}) = \tgtdens(\vct{x}) \qquad \forall \vct{x} \in \set{X}, \vct{a} \in \set{A}^*.
\end{equation}
If the marginal probability $\prob{\rvct{a}}(\set{A}^*)$ is non-zero, then the sampled states $\lbrace \vct{x}^{(n)},\vct{a}^{(n)}\rbrace_{n=1}^N$ of a Markov chain which has $\prob{\rvct{x},\rvct{a}}$ as its unique invariant distribution can be used to estimate expectations with respect to the target distribution by computing averages over only the sampled target variable values $\vct{x}^{(n)}$ for which the corresponding auxiliary variables $\vct{a}^{(n)}$ take values in $\set{A}^*$ (these being at convergence samples from $\prob{\vct{x}|\vct{a}}(\cdot \gvn \vct{a})$ and so the target distribution), i.e.
\begin{equation}\label{eq:conditional-auxiliary-variable-estimator}
  \int_{\set{X}} f(\vct{x}) \,\tgtprob(\dr\vct{x}) =
  \lim_{N\to \infty} 
  \frac
    {\sum_{n=1}^N \ind{\set{A}^*}(\vct{a}^{(n)}) f(\vct{x}^{(n)})}
    {\sum_{n=1}^N \ind{\set{A}^*}(\vct{a}^{(n)})}.
\end{equation}
We will discuss \emph{simulated tempering}, an \ac{MCMC} method which introduces an auxiliary variable in this manner in Section \ref{subsec:simulated-tempering}.

An issue with this approach is that if $\prob{\rvct{a}}(\set{A}^*)$ is small, the number of sampled states with $\vct{a} \in \set{A}^*$ may be very small or even zero. This can require a large number of samples $N$ for the sufficient samples with auxiliary variables in the required set $\set{A}^*$ to be generated to allow the \ac{MCMC} estimates computed using \eqref{eq:conditional-auxiliary-variable-estimator} to be reliable. 

The estimator in \eqref{eq:conditional-auxiliary-variable-estimator} has a close resemblance to the formulation of the Monte Carlo estimator corresponding to rejection sampling given in \eqref{eq:rejection-sampler-mc-estimator}, with averages computed over the subset of samples meeting an `acceptance' criteria. As noted previously rejection sampling can in fact be considered as an auxiliary variable method, with binary accept indicator variables $\rvar{a} \in \fset{0,1}$ introduced such that the conditional density $\pden{\rvct{x}|\rvar{a}}(\vct{x}\gvn 1)$ is equal to the target density $\tgtdens(\vct{x})$ as shown in \eqref{eq:rejection-sampling-cond-dens-given-accept}, i.e. exactly corresponding to the property in \eqref{eq:conditional-auxiliary-variable-property}. Rejection sampling can therefore be seen to be an example of this construct, though in this case each pair of target -- auxiliary variable samples are generated independently rather than by constructing a Markov chain.

When discussing the rejection sampling estimator \eqref{eq:rejection-sampler-mc-estimator} we saw there was a close link to importance sampling estimator \eqref{eq:importance-sampler-mc-estimator}, with the importance sampling estimator having the potential advantage however of using all of the generated samples in computing estimates. In Chapter \ref{ch:continuous-tempering} we will discuss a related alternative approach to constructing estimators for auxiliary variable methods based on conditioning like simulated tempering, which unlike the estimator in \eqref{eq:conditional-auxiliary-variable-estimator} allows using all of the samples in a Markov chain to compute estimates.

\subsection{Slice sampling}\label{subsec:slice-sampling}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{.46\linewidth}
\centering
\includetikz{slice-sampling-1}
\vspace{3mm}
\end{subfigure}
\begin{subfigure}[b]{.46\linewidth}
\centering
\includetikz{slice-sampling-2}
\end{subfigure}
\caption[Visualisation of linear slice sampling.]{Schematic of linear slice sampling, showing `plan' (left) and `cross-sectional' (right) views of a bivariate target density. Orange curve (left) and line (right) indicates a constant density slice $\set{S}_h$. The black square indicates current target state value $\vct{x}$ and the dashed line is \emph{slice line}, the one-dimensional linear sub-space aligned with the vector $\vct{v}$ which a new value from the state will be sampled on. The extents of the dashed line segment represent the initial bracket new proposed states will be drawn from. Points are proposed on the slice line by drawing a value uniformly from the current bracket. The red circle represents an initial proposed point which is not in the slice and so the right bracket edge is shrunk to this point. The violet circle shows a second sampled point from the new reduced bracket, this point within the slice and so returned as the updated target state.}
\label{fig:slice-sampling}
\end{figure}

\begin{algorithm}[t]
\caption{Linear slice sampling.}
\label{alg:linear-slice-sampling}
\input{algorithms/linear-slice-sampling}
\end{algorithm}

Slice sampling is a family of auxiliary variable \ac{MCMC} methods which exploit the same observation as used to motivate rejection sampling - to sample from a target distribution it is sufficient to uniformly sample from the volume beneath a graph of the target density function. Rather than generate independent points from this volume as in rejection samp\-ling, slice sampling instead constructs a transition operator which leaves the uniform distribution on this volume invariant.

The method we will concentrate on here was proposed by Neal \citep{neal1997markov,neal2003slice}. A related algorithm which uses per data-point auxiliary variables in Bayesian inference problems was developed by Damien, Wakefield and Walker \citep{damien1999gibbs}. Murray, Adams and Mackay later proposed \emph{elliptical slice sampling} \citep{murray2010elliptical}, an extension of Neal's slice sampling method which is particularly effective for target distributions which are well approximated by a multivariate normal distribution. % and which we will discuss at the end of this subsection.
% which leverages the ellipsoidal geometry of the typical set of multivariate normal distributions to construct a slice sampling transition operator
% with a related algorithm which uses per data-point auxiliary variables in Bayesian inference problems developed by Damien, Wakefield and Walker \citep{damien1999gibbs}

Slice sampling defines a Markov chain on an augmented state space by introducing an auxiliary \emph{height} variable $\rvar{h} \in [0, \infty)$ in addition to the target variables $\rvct{x} \in \set{X}$. The conditional density on $\rvar{h}$ is 
\begin{equation}
  \pden{\rvar{h}|\rvct{x}}(h\gvn\rvct{x}) = 
  \mathcal{U}\lpa h\gvn 0, \utgtdens(\vct{x})\rpa = 
  \frac{1}{\utgtdens(\vct{x})} \ind{[0,\utgtdens(\vct{x}))}(h),
\end{equation}
i.e. uniform over the interval between zero and the unnormalised target density value. The joint density on the augmented space is then
\begin{equation}\label{eq:slice-sampling-augmented-target}
  \pden{\rvct{x},\rvar{h}}(\vct{x},h) = 
  \frac{1}{\utgtdens(\vct{x})} \ind{[0,\utgtdens(\vct{x}))}(h)\, \frac{\utgtdens(\vct{x})}{Z} =
  \frac{1}{Z} \ind{[0,\utgtdens(\vct{x}))}(h).
\end{equation}
Marginalising \eqref{eq:slice-sampling-augmented-target} over $\rvar{h}$ recovers the target density i.e. $\pden{\rvct{x}} = \tgtdens$. %Therefore if we can construct a Markov chain with \eqref{eq:slice-sampling-augmented-target} as its unique invariant distribution, the $\rvct{x}$ components of the chain state will be dependent samples from the target distribution for a chain at stationarity and so can be used to compute \ac{MCMC} estimates. Several of the methods we will introduce later in this thesis will also exploit this idea of defining a Markov chain on an augmented state space by introducing auxiliary variables.

The overall slice sampling transition is formed of the sequential composition of a transition operator which updates $\rvar{h}$ given $\rvct{x}$ and a second operator which updates $\rvct{x}$ given $\rvar{h}$, each leaving the distributions corresponding to the conditional densities $\pden{\rvar{h}|\rvct{x}}$ and $\pden{\rvct{x}|\rvar{h}}$ respectively invariant, and so by the same argument as for Gibbs sampling the overall transition leaving the target distribution invariant. By construction the conditional density $\pden{\rvar{h}|\rvct{x}}$ is a simple uniform density and so the first transition operator is a Gibbs sampling update in which the height variable is independently resampled from $\mathcal{U}\lpa 0, \utgtdens(\vct{x})\rpa$, where $\vct{x}$ is the current value of the target state $\rvct{x}$.

The conditional density $\pden{\rvct{x}|\rvar{h}}(\vct{x}\gvn h)$ is also locally uniform, equal to a positive constant whenever $\utgtdens(\vct{x}) > h$ and zero elsewhere. However we can usually only evaluate the density up to an unknown constant as we cannot compute the measure of the set $\set{S}_h = \fset{\vct{x} \in \set{X} : \utgtdens(\vct{x}) > h}$ that the density is non-zero over. In general $\set{S}_h$, which is the eponymous \emph{slice} of slice sampling (so called as it represents a slice through the volume under the density curve at a fixed height $h$), will have a complex geometry including potentially consisting of several disconnected components in the case of multimodal densities. The complexity of the slices generally prevents us therefore from being able to independently sample a new value for $\rvct{x}$ uniformly from $\set{S}_h$ and so we cannot use a full Gibbs sampling scheme corresponding to sequentially independently sampling from $\prob{\rvar{h}|\rvct{x}}$ and $\prob{\rvct{x}|\rvar{h}}$.

A key contribution of \citep{neal2003slice} was to introduce an elegant method for constructing a transition operator which leaves $\prob{\rvct{x}|\rvar{h}}$ invariant. In particular the algorithm has few free parameters to tune, has an efficiency which is relatively robust to the choices of the free choices that are introduced, and will for smooth target densities always move the target state by some amount (in contrast to the potential for rejections in Metropolis--Hastings methods). This method is summarised in Algorithm \ref{alg:linear-slice-sampling} and a visualisation of the process shown in Figure \ref{fig:slice-sampling}.

An important first step in the algorithm is reducing the problem of generating a point uniformly on the multidimensional slice $\set{S}_h$ to making a move on a one-dimensional linear subspace of this slice  (motivating our naming of this algorithm \emph{linear slice sampling}) which includes the current $\rvct{x}$ state. In the original description of the algorithm in \citep{neal2003slice} the one-dimensional subspace is chosen to be axis-aligned, corresponding to updating a single component of the target state. 

In the case of an axis-aligned subspace the restriction of the slice to the one-dimensional subspace is entirely specified by the conditional density on the chosen variable component given the current values of the remaining components in the state. Slice sampling transitions for each variable in the target state can then be applied sequentially akin to Gibbs sampling, but with the advantage over Gibbs of not requiring the complete conditionals to be of a tractable form which we can generate exact samples from. If conditional independency structure in the target density means the complete conditionals depend only on local subsets of variables in the target state using updates of this form has the advantage of exploiting this locality. As with Gibbs sampling however applying slice sampling in this manner makes performance strongly dependent on the parameterisation of the target density, with large magnitude correlations likely to lead to slow exploration of the space. 

\begin{figure}
\centering
\begin{subfigure}[b]{.46\linewidth}
\centering
  \includetikz{axis-aligned-slice-sampler-bivariate-normal}
  \vspace{-6mm}
  \caption{Axis-aligned updates}
  \label{sfig:axis-aligned-linear-slice-sampler}
\end{subfigure}
~~
\begin{subfigure}[b]{.46\linewidth}
\centering
  \includetikz{random-direction-slice-sampler-bivariate-normal}
  \vspace{-6mm}
  \caption{Random direction updates}
  \label{sfig:random-direction-linear-slice-sampler}
\end{subfigure}
  \caption[Linear slice sampler comparison.]{Samples generated using \subref{sfig:axis-aligned-linear-slice-sampler} axis-aligned versus \subref{sfig:random-direction-linear-slice-sampler} random-direction  linear slice sampling in a correlated bivariate normal distribution. In both cases 1000 transitions where performed (with random selection of axis to update on each iteration in \subref{sfig:axis-aligned-linear-slice-sampler}) with every second sampled state shown.  the maximum number of step out iterations is $M=4$ and the initial bracket width is fixed at $w=1$. The dotted ellipse shows the contour of the target density which contains 0.99 of the mass. The random direction chain is able to explore the typical set of the target distribution more effectively in this case with the axis-aligned updates leading to slower diffusion along the major axis of the elliptical contour.}
  \label{fig:linear-slice-sampler-comparison}
\end{figure}

In \citep{neal2003slice} various multivariate extensions of the algorithm are suggested which could help counter this issue, however they add significant implementation complexity compared to the basic algorithm. A simpler alternative is to define the one-dimensional subspace as being the line defined by a randomly chosen vector and passing through the current value of $\rvct{x}$. If this vector is generated independently of the current state this is sufficient to ensure the overall transition retains the correct invariant distribution. 

If little is known about the target distribution a reasonable default is to sample a unit vector of the required dimensionality by generating a random zero-mean isotropic covariance multivariate normal vector and then scaling it to unit norm; if an approximate covariance matrix $\hat{\mtx{\Sigma}}$ is known for the target density then instead generating the vector from $\nrm{\vct{0},\mtx{\Sigma}}$ prior to normalising might be a better choice (as it favours moves aligned with the principle eigenvectors of $\mtx{\Sigma}$) however in this case elliptical slice sampling, which we will discuss shortly, will often be a better choice. 

This random-direction slice sampling variant is discussed in comparison to elliptical slice sampling in \citep{murray2010elliptical}. It is also bears resemblance to the scheme proposed in \citep{chen1998toward} which uses the same auxiliary variable formulation as slice sampling, but there the random direction is chosen in $\set{X}\times[0,\infty)$ i.e. to update both $\rvct{x}$ and $\rvar{h}$ and not used with the remainder of Neal's slice sampling algorithm. An example comparison of applying axis-aligned and random-direction linear slice sampling updates to a strongly positively correlated bivariate normal target distribution is shown in Figure \ref{fig:linear-slice-sampler-comparison}. In this toy example the isotropic random-direction updates are able to more effectively explore the target density.

The generation of the vector $\vct{v}$ determining the one-dimensional sub-space of the slice the update is performed on is represented in Algorithm \ref{alg:linear-slice-sampling} by Line \ref{algline:slice-sample-direction} by $\vct{v}$ being generated from a distribution with density $q$. As well as specifying the \emph{direction} of the slice line, the vector $\vct{v}$ also specifies a scale along this line. In Neal's description of the algorithm this is represented by the explicit \emph{bracket width} parameter $w$. Here instead we assume this parameter is implicitly defined by the Euclidean norm of the vector $\vct{v}$, through suitable choice of $q$ this allowing for direction dependent scales and also the possibility of randomisation of the scale; as we will see shortly however compared to for example random-walk Metropolis updates with a normal proposal, linear slice sampling is much less sensitive to the choice of scale parameters, therefore a single fixed scale will often be sufficient.

Once the slice line direction and scale has been chosen, the remainder of the algorithm can be split into two stages: selection of an initial bracket on the slice line and including the point corresponding to the current state; iteratively uniformly sampling points within the current bracket, accepting the point if it is within the slice $\set{S}_h$ otherwise shrinking the bracket and repeating. The algorithm proposed by Neal ensures both these stages are performed reversibly such that the detailed balance condition \eqref{eq:detailed-balance} is maintained.

The \emph{slice bracket} defines a contiguous interval $\lambda \in [b_l,b_u]$ on the slice line $\vct{x}^*(\lambda) = \vct{x}_n + \lambda \vct{v}$ and always includes the point $\lambda = 0$ corresponding to the current state. The initial bracket is chosen by sampling a upper bound $b_u$ uniformly from $[0, 1]$ and then setting $b_l \gets b_u - 1$; in the $\lambda$ slice line coordinate system this corresponds to a bracket width of one, however in general the slice line vector $\vct{v}$ can have non-unit length and so defines the initial bracket width in the target variable space. Randomising the positioning of the current state within the bracket ensures reversibility as the resulting bracket would have an equal probability (density) of being selected from any other point in the bracket (which the final accepted point will be within).

In general only a subset of the points in the current slice bracket will be within the slice $\set{S}_h$. As new states are proposed by sampling a point uniformly from the current bracket, the probability of such a proposal being in the slice will be equal to the proportion of the bracket that intersects with the slice $\set{S}_h$. In general therefore it is desirable for the bracket to include as much of the slice as possible while not making the proportion of the bracket intersecting with the slice too small such that many points need to be proposed before a point on the slice is found. The magnitude of $\vct{v}$ determines the initial bracket extents and so should ideally chosen based on any knowledge of the `typical scale' of the target density. Often we will have little prior knowledge about such scaling however and the scale will often vary significantly across the target space, and so we may choose an initial bracket which includes only a small proportion of the slice.

The stepping out routine proposed by \citep{neal2003slice} and detailed in Lines \ref{algline:slice-sample-step-out-start} to \ref{algline:slice-sample-step-out-end} in Algorithm \ref{alg:linear-slice-sampling} is designed to counter this issue. The initial slice bracket $[b_l, b_u]$ is iteratively `stepped-out' by incrementing / decrementing the upper / lower bracket bounds until the corresponding endpoint of the bracket lies outside the slice or a pre-determined maximum number of steps out have been performed. Ideally the step out routine will return a bracket which contains all of the intersection of the slice with the slice line while not also including too great a proportion of off slice points; in general the slice may be non-convex or consist of multiple disconnected components and so the intersection of the slice line with the slice may consist of multiple disconnected intervals in which case the stepping out routine will likely only expand the slice to include a subset of these intervals. The adaptivity provided by the stepping out routine will still however generally help to make the performance of the sampler much less sensitive to the choice of the bracket scale in contrast to for example random-walk Metropolis algorithms. % which typically use a single fixed scale. 

Analogously to the randomisation of the initial bracket positioning, in the stepping out routine if a maximum number of step out iterations $M$ is set, the resulting step `budget' is randomly allocated between increments of the upper bound $b_u$ and decrements of the lower bound $b_l$ such that final extended bracket generated by the step out routine would have an equal probability of being generated from any point within the generated bracket interval. If $M$ is set to zero this corresponds to not performing any stepping out and simply using the initial sampled bracket; although reducing the robustness of the algorithm to the choice of the initial bracket width this option has the advantage of minimising the number of target density evaluations by not requiring additional density evaluations at the bracket endpoints during the step-out routine. An alternative `doubling' step-out routine was also proposed in \citep{neal2003slice}. This has the advantage of exponentially expanding the slice bracket compared to the linear growth of the step-out routine described in Algorithm \ref{alg:linear-slice-sampling} and so can be more efficient in target distributions where the typical scales of the density varies across several orders of magnitude. The doubling procedure requires a more complex subsequent procedure for sampling points in the resulting bracket however to ensure reversibility. 

Once the initial bracket has been generated and potentially stepped out, the remainder of the algorithm consists of finding a point on the slice line bracket which is within the slice $\set{S}_h$. This is done in an iterative manner by first sampling a point uniformly from the current bracket and checking if it is in the slice or not. If the proposed point is in the slice, the corresponding value for the target variables is returned at the new state. Otherwise the proposed point is set as the new upper or lower bound of the bracket such that the point corresponding to the current state remains in the bracket. This shrinks the bracket by removing an interval where it is known at least some points are not in the slice. A new point is then sampled uniformly from the smaller bracket and the procedure repeats until a point in the slice is found.

The iterative shrinking of the slice bracket implemented by this procedure introduces a further level of adaptivity in to the slice sampling algorithm, meaning that even if only a small proportion of the initial bracket lies within the slice only relatively few iterations will be needed still till the bracket is shrunk sufficiently for there to be a high probability of proposing a point within the bracket. By ensuring the point corresponding to the current state always remains within the current bracket, reversibility is maintained.

\begin{algorithm}[!t]
\caption{Elliptical slice sampling.}
\label{alg:elliptical-slice-sampling}
\input{algorithms/elliptical-slice-sampling}
\end{algorithm}

An alternative to the linear slice sampling procedure just described, is the \emph{elliptical slice sampling} method proposed in \citep{murray2010elliptical} and described in Algorithm \ref{alg:elliptical-slice-sampling}. As suggested by the name, in elliptical slice sampling rather than proposing points on a line instead an elliptical path in the target space is defined and new points proposed on this ellipse. 

Elliptical slice sampling is intended for use in target distributions which can be approximated by a known normal distribution $\nrm{\vct{\mu},\mtx{\Sigma}}$. This distribution might correspond to a normal prior distribution on model latent variables where the dependence between the latent and observed variables is only weak and so the posterior remains well approximated by the prior or a normal approximation fitted directly to the target distribution using an optimisation based approximate inference scheme such as those discussed in Appendix \ref{app:optimisation-based-approximate-inference} \citep{nishihara2014parallel}. 

In each elliptical slice sampling transition an auxiliary vector $\vct{v}$ is independently sampled from the normal distribution $\nrm{\vct{\mu},\mtx{\Sigma}}$. If the target distribution was exactly described by the normal distribution we could use this independent draw directly as the new chain state (though obviously in this case there would be no advantage in formulating as an \ac{MCMC} method). In reality the target distribution will only approximately described by $\nrm{\vct{\mu},\mtx{\Sigma}}$ and so we wish to instead use this independent draw to define a Markov transition operator that will potentially move the state to a point nearly independent of the current state, but is also able to back off to more conservative proposals closer to the current chain state. This is achieved by defining an elliptical path in target space centred at $\vct{\mu}$, passing through the current state $\vct{x}_n$ and the auxiliary vector $\vct{v}$ and parameterised by an angular variable $\theta$
\begin{equation}\label{eq:elliptical-path}
  \vct{x}^*(\theta) = (\vct{x}_n - \vct{\mu}) \cos\theta + (\vct{v} - \vct{\mu}) \sin\theta + \vct{\mu}.
\end{equation}
If we generated $\theta$ uniformly from $\mathcal{U}(0,2\pi)$ then the corresponding proposed transition $\vct{x}^*(\theta)$ would exactly leave the distribution $\nrm{\vct{\mu},\mtx{\Sigma}}$ invariant. As we instead wish to leave the target distribution invariant, a slice sampling algorithm is used to find a $\theta$  which accounts for the difference between the target distribution and normal approximation. An auxiliary slice height variable $h$ is sampled uniformly from $\mathcal{U}\lpa 0,\utgtdens(\vct{x}_n) / \nrm{\vct{x}_n \gvn \vct{\mu}, \mtx{\Sigma}}\rpa$ and used to define a slice
\begin{equation}
  S_h = 
  \lbr \vct{x} \in \set{X} : \frac{\utgtdens(\vct{x}_n)}{\nrm{\vct{x}_n \gvn \vct{\mu}, \mtx{\Sigma}}} < h \rbr. 
\end{equation}
Similar to the linear slice sampling algorithm, a bracket $[\theta_l, \theta_u]$ on the elliptical path is randomly placed around $\theta = 0$ corresponding to the current state $\vct{x}_n$. Unlike the requirement to choose a suitable initial bracket width in linear slice sampling however, we can define the initial bracket in elliptical slice sampling to include the entire elliptical path i.e. $\theta_l = \theta_u - 2\pi$; we only need to randomise the `cut-point' defining the initial end-points of the bracket to ensure reversibility. This removes the need to choose an initial bracket width (defined by $|\vct{v}|$ in our description of the linear slice algorithm) and for any step out procedure, and so beyond choosing the multivariate normal approximation elliptical slice sampling does not have any free settings which need to be tuned.

Once the initial bracket is defined, a directly analogous iterative procedure to that used in the linear slice sampling algorithm is used to find a $\theta$ value corresponding to a point in the slice while using rejected proposed points to shrink the bracket. As with linear slice sampling, providing the target density is a smooth function and so the intersection of the elliptical path with the slice is a non-zero measure set, then the state moved to by the elliptical slice sampling transition operator will never be equal to the previous state.

\subsection{Hamiltonian Monte Carlo}\label{subsec:hamiltonian-monte-carlo}

The \ac{MCMC} algorithms discussed so far have required only the ability to evaluate a (unnormalised) density function for the target distribution of interest. For distributions defined on real-valued variables the target density function $\utgtdens$ will often be differentiable - the gradient $\pd{\utgtdens}{\vct{x}}$ exists $P$-almost everywhere. In these cases it is natural to consider using the gradient to help guide updates to the state. In particular we might hope to reduce the random-walk behaviour of simpler methods which leads to a slow diffusive exploration of high-dimensional spaces.

% This name arose as it was conceived as a hybrid of existing methods based on deterministic simulation of the dynamics of a system with Monte Carlo methods which ignored the system dynamics but could describe the statistical properties of ensemble systems by computing ergodic averages.
\marginpar{What we refer to as Hamiltonian Monte Carlo here was introduced in \citep{duane1987hybrid} as \emph{Hybrid Monte Carlo}. The alternative \emph{Hamiltonian Monte Carlo} was suggested by MacKay \citep{mackay2003information} to emphasise the role of \mbox{Hamiltonian~dynamics} in the method while maintaining the same acronym \citep{betancourt2017convergence}.}
A particularly powerful auxiliary variable \ac{MCMC} method utilising gradient information is \ac{HMC} \citep{duane1987hybrid,neal2011mcmc}. \ac{HMC} introduces auxiliary \emph{momentum} variables in to the chain state and then uses simulated Hamiltonian dynamics trajectories in the augmented space to generate proposed updates to the momentum--target variables state pair. The simulated Hamiltonian dynamics exhibit key geometric properties that make \ac{HMC} well suited to performing \ac{MCMC} in complex target distributions on high-dimensional spaces \citep{betancourt2017geometric}, with the method able to propose long-range moves with a high probability of acceptance. The reduced random-walk behaviour means that \ac{HMC} often scales better to high-dimensional target distributions than simpler methods such as random-walk Metropolis and Gibbs sampling \citep{betancourt2017conceptual}. Under the assumption of a target distribution consisting of a product of independent factors on $D$ variables, optimally tuned random-walk Metropolis transitions will take $\mathcal{O}(D^2)$ computational effort to achieve near independence between states of the chain while an optimally tuned \ac{HMC} transition can achieve the same with a $\mathcal{O}(D^{\frac{5}{4}})$ cost \citep{neal2011mcmc}.

Most implementations of \ac{HMC} require the target density $p$ is defined with respect to the Lebesgue measure on a Euclidean space $\set{X} = \reals^D$. Target densities with bounded support on $\reals^D$ add complication to the algorithm by requiring checks that proposed updates to the state remain within the support of the target distribution and reflecting at the boundaries of the support \citep{neal2011mcmc}. Often however a change of variables can be performed with a bijective transformation (using Equation \ref{eq:change-of-variables-vector-bijective}) that maps to a density with unbounded support, for example taking a log-transform of a positive variable. %We will consider extensions of the standard \ac{HMC} approach to distributions defined on implicitly-defined manifolds in Chapter \ref{ch:differentiable-generative-models}. %embedded in a Euclidean space

Rather than working directly with the unnormalised target density $\utgtdens$ the \ac{HMC} algorithm is more naturally described in terms of a \emph{potential energy} function $\phi : \reals^D \to \reals$ which is related to $\utgtdens$ by
\begin{equation}\label{eq:boltzmann-gibbs-density}
  \utgtdens(\vct{x}) =  \exp\lpa -\phi(\vct{x}) \rpa \iff \phi(\vct{x}) = -\log \utgtdens(\vct{x}).
\end{equation}
The original \emph{target variables} $\rvct{x} \in \reals^D$ are augmented with a vector of \emph{momentum variables} $\rvct{p} \in \reals^D$. The conditional density on the momenta given the target variables $\pden{\rvct{p}|\rvct{x}}$ is defined in terms of a \emph{kinetic energy} function $\tau : \reals^D \times \reals^D \to \reals$ which is even in its first argument
\begin{equation}
  \pden{\rvct{p}|\rvct{x}}(\vct{p}|\vct{x}) \propto \exp\lpa-\tau(\vct{p}\gvn\vct{x}) \rpa.
\end{equation}
The joint density on the momentum and target variables is then
\begin{equation}\label{eq:hmc-joint-density}
  \pden{\rvct{x},\rvct{p}}(\vct{x},\vct{p}) \propto
  \exp\lpa -\phi(\vct{x}) - \tau(\vct{p}\gvn\vct{x})\rpa = \exp\lpa -\hamiltonian(\vct{x},\vct{p})\rpa.
\end{equation}
The function $\hamiltonian(\vct{x},\vct{p}) = \phi(\vct{x}) + \tau(\vct{p}\gvn\vct{x})$ is termed the \emph{Hamiltonian} for the system. A common simplification is for the momenta to be chosen to be independent of the target variables with a marginal density defined by a kinetic energy $\tau : \reals^D \to \reals$  
\begin{equation}
  \pden{\rvct{p}}(\vct{p}) \propto\exp\lpa-\tau(\vct{p})\rpa.
\end{equation}
In this case the Hamiltonian $\hamiltonian(\vct{x},\vct{p}) = \phi(\vct{x}) + \tau(\vct{p})$ is \emph{separable} - there are no terms jointly dependent on both $\vct{x}$ and $\vct{p}$. 

Commonly a quadratic form 
\(
\tau(\vct{p}) = \frac{1}{2}\vct{p}\tr\mtx{M}^{-1}\vct{p}
\)
is used for the kinetic energy where $\mtx{M}$ is a positive definite matrix typically termed the \emph{mass matrix}. A quadratic kinetic energy corresponds to assuming normally distributed momenta with zero-mean and covariance $\mtx{M}$.

In classical mechanics, the Hamiltonian describes the total energy of a mechanical system, and can be used to define a \emph{canonical Hamiltonian dynamic} via the set of \acp{ODE} 
\begin{equation}\label{eq:canonical-hamiltonian-dynamic}
  \td{\vct{x}}{t} = \pd{\hamiltonian}{\vct{p}}\tr,
  \qquad
  \td{\vct{p}}{t} = -\pd{\hamiltonian}{\vct{x}}\tr.
\end{equation}
We define the \emph{flow map} corresponding to this dynamic as a family of mappings $\flowmap_t : \reals^D \times \reals^D \to \reals^D \times \reals^D$ parameterised by a time $t \in \reals$ such that if $\lpa \vct{x}(t),\vct{p}(t) \rpa$ is the solution to the set of \acp{ODE} \eqref{eq:canonical-hamiltonian-dynamic} at a time $t$ given an initial condition $\vct{x}(0) = \vct{x}_0$, $\vct{p}(0) = \vct{p}_0$ then
\begin{equation}\label{eq:flow-map-definition}
  \flowmap_t(\vct{x}_0,\vct{p}_0) = \lpa \vct{x}(t),\vct{p}(t) \rpa.
\end{equation}
The Hamiltonian flow map has several desirable properties as a proposal generating mechanism for a \ac{MCMC} method. The Hamiltonian is exactly conserved along the trajectories generated by the flow map, i.e. $\hamiltonian(\vct{x},\vct{p}) = \hamiltonian\circ\flowmap_t(\vct{x},\vct{p})$ for all $t \in \reals$ and for any initial $\vct{x},\vct{p}$ pair. As $\pden{\rvct{x},\rvct{p}}(\vct{x},\vct{p}) \propto \exp\lpa-\hamiltonian(\vct{x},\vct{p})\rpa$ this means Hamiltonian trajectories remain confined to constant density surfaces in the augmented state space. 

The Hamiltonian flow map is also \emph{volume preserving} - the Jacobian of the flow map, $\jacobian{\flowmap_t}$ has determinant one for all $t$ and starting from any initial $(\vct{x},\vct{p})$. This volume preservation is a consequence of a stronger geometric property of the dynamic - that the flow map is \emph{symplectic} \citep{leimkuhler2004simulating}. Symplecticity of the flow map is implied by the condition
\begin{equation}
\jacobian{\flowmap_t}\tr\, \begin{bmatrix} \mathbf{0} & \idmtx \\ -\idmtx & \mathbf{0} \end{bmatrix} \,\jacobian{\flowmap_t} = \begin{bmatrix} \mathbf{0} & \idmtx \\ -\idmtx & \mathbf{0} \end{bmatrix}
\end{equation} 
being satisfied. The symplectic nature of Hamiltonian dynamics is central to efficient scaling of \ac{HMC} to high-dimensional spaces \citep{neal2011mcmc,betancourt2017geometric}.

A final crucial property of the Hamiltonian flow map is that it exhibits a time-reversal symmetry under negation of the momenta
\begin{equation}\label{eq:hamiltonian-dynamic-time-reversal-symmetry}
(\vct{x}',\vct{p}') = \flowmap_t(\vct{x},\vct{p})
\iff
(\vct{x},-\vct{p}) = \flowmap_t(\vct{x}',-\vct{p}').
\end{equation}
If we define $\vct{r} : \reals^D\times\reals^D \to \reals^D \times \reals^D$ as a `momentum-reversal' operator such that $\vct{r}(\vct{x},\vct{p}) = (\vct{x},-\vct{p})$ then this time-reversal symmetry means that the composition $\vct{r} \circ \flowmap_t$ is an involution. Further as $\vct{r}$ also has Jacobian determinant one, then the composition $\vct{r} \circ \flowmap_t$ also itself has a unit Jacobian determinant.

Using the previous result for the Metropolis--Hastings accept ratio for the special case of a deterministic proposal formed by an involution \eqref{eq:metropolis-hastings-transition-deterministic-proposal}, we have that a proposal generated by applying $\vct{r} \circ \flowmap_t$ to the current state pair $(\vct{x},\vct{p})$ for any $t$ has an accept probability of one
\begin{align}\label{eq:exact-hmc-accept-probability}
  \alpha(\vct{x},\vct{p}) &=
  \min\lbr 
    1,\,
    \frac
    {
    \exp\lpa
      -\hamiltonian \circ \vct{r} \circ \flowmap_t(\vct{x}, \vct{p})
    \rpa
    }
    {
    \exp\lpa
      -\hamiltonian(\vct{x},\vct{p})
    \rpa
    }
    \left|\,\jacob{\vct{r} \circ \flowmap_t}{\vct{x},\vct{p}}\right|
  \rbr\\
  &=
  \min\lbr 
    1,\,
    \exp\lpa
      \hamiltonian(\vct{x},\vct{p}) -\hamiltonian \circ \vct{r} \circ \flowmap_t(\vct{x}, \vct{p})
    \rpa
  \rbr = 1.
\end{align}
This is a result of the conservation of the Hamiltonian under the flow map (and momentum-reversal operator as $\tau$ is even in the momenta) and the composed map having unit Jacobian determinant. Therefore proposals formed by integrating the \acp{ODE} forward by some length of time from the current state and then reversing the momentum would always be accepted. 

On its own the momentum reversal operator $\vct{r}$ is also an involution with unit Jacobian determinant which exactly conserves the Hamiltonian, and so can also be applied as a `proposal'  with probability of acceptance of one. If we sequentially alternate updates using $\vct{r} \circ \flowmap_t$ and $\vct{r}$, each defines a valid Markov transition operator which leaves the (extended) target invariant, and in sequential composition the momentum reversals cancel. We can therefore construct a Markov chain which leaves the target distribution with density \eqref{eq:hmc-joint-density} invariant by repeatedly generating new states by integrating the Hamiltonian dynamic forward by arbitrary lengths of time. Note that though each of $\vct{r} \circ \flowmap_t$ and $\vct{r}$ are individually reversible and so respect detailed balance, the sequential composition is no longer reversible.

\begin{figure}[t]
\centering
\includetikz{hmc-visualisation}
\vspace{-6mm}
\caption[Visualisation of Hamiltonian Monte Carlo.]{Visualisation of Hamiltonian Monte Carlo applied to a univariate target density $\tgtdens(x) = \alpha\exp(-\alpha x) \lpa 1 + \exp(-x)\rpa^{-1-\alpha}$ with $\alpha = 0.4$. The left axis shows contours (green curves) of the Hamiltonian function on the augmented $(x,p)$ state space. The orange markers shows a Hamiltonian trajectory simulated using the leapfrog method, starting at the square marker and finishing at the triangular marker. The trajectory nearly exactly traces a Hamiltonian contour due to the approximate energy conservation of the simulated dynamic, with the proposed update (from square to triangular markers) therefore accepted with high probability. At the end of the orange trajectory the momentum is randomly resampled, giving a new initial state (purple square marker) for a second simulated trajectory shown by the purple markers. The right axis shows the variation in the Hamiltonian $\hamiltonian$, potential energy $\phi$ and kinetic energy $\tau$ over the two trajectories. In each trajectory the Hamiltonian is close to constant, with shifts in the potential energy matched by opposing shifts in the kinetic energy. There is a step change in the kinetic energy and Hamiltonian when then momentum is resampled at the end of first trajectory.}
\label{fig:hamiltonian-monte-carlo}
\end{figure}

There are two major problems with this scheme. Firstly for most $\phi$ and $\tau$ it is not possible to integrate the \acp{ODE} \eqref{eq:canonical-hamiltonian-dynamic} exactly and so we cannot evaluate the exact flow map $\flowmap_t$. Secondly the scheme as proposed would not be ergodic as the Hamiltonian is conserved by each application of the flow map, and so all states generated in this way would be confined to a constant Hamiltonian manifold in the joint space.

The first issue can be resolved by approximately integrating the \acp{ODE}. Importantly by using a \emph{symplectic integrator} we are able to form approximate Hamiltonian flow maps which maintain the key volume-preservation and time-reversibility properties of the exact flow map dynamic and define, as the name suggests, symplectic maps. There is a large class of such symplectic integrators \citep{leimkuhler2004simulating} however for separable Hamiltonians an appealingly simple scheme is the \emph{St\"ormer--Verlet} or \emph{leapfrog} integrator. If we first define the following component maps
\begin{align}\label{eq:leapgfrog-component-steps}
  \hat{\flowmap}_{\delta t}^{\textsc{a}}(\vct{x},\vct{p}) = 
  \lpa\vct{x}+\delta t \nabla\tau(\vct{p})\tr,\vct{p}\rpa,
  ~
  \hat{\flowmap}_{\delta t}^{\textsc{b}}(\vct{x},\vct{p}) = 
  \lpa\vct{x},\vct{p}-\delta t \nabla\phi(\vct{x})\tr\rpa,
\end{align}
then a leapfrog step is defined by the symmetric composition
\begin{equation}\label{eq:leapfrog-step-composition}
  \hat{\flowmap}^{\textsc{lf}}_{\delta t} = 
  \hat{\flowmap}_{\frac{\delta t}{2}}^{\textsc{b}} \circ 
  \hat{\flowmap}_{\delta t}^{\textsc{a}} \circ 
  \hat{\flowmap}_{\frac{\delta t}{2}}^{\textsc{b}}.
\end{equation}
Each leapfrog step is time-reversible and volume conserving. The composition of $L$ leapfrog steps $\lpa\hat{\flowmap}^{\textsc{lf}}_{\delta t}\rpa^L$ maintains these properties. The alternative symmetric composition $\hat{\flowmap}_{\frac{\delta t}{2}}^{\textsc{a}} \circ \hat{\flowmap}_{\delta t}^{\textsc{b}} \circ \hat{\flowmap}_{\frac{\delta t}{2}}^{\textsc{a}}$ also defines a symplectic integrator and is also sometimes termed the leapfrog method. In practice when multiple leapfrog steps are composed together the intermediate half time-steps can be combined, for example using \eqref{eq:leapfrog-step-composition}
\begin{equation*}
  \lpa
  \hat{\flowmap}_{\frac{\delta t}{2}}^{\textsc{b}} \circ 
  \hat{\flowmap}_{\delta t}^{\textsc{a}} \circ 
  \hat{\flowmap}_{\frac{\delta t}{2}}^{\textsc{b}}
  \rpa
  \circ
  \lpa
  \hat{\flowmap}_{\frac{\delta t}{2}}^{\textsc{b}} \circ 
  \hat{\flowmap}_{\delta t}^{\textsc{a}} \circ 
  \hat{\flowmap}_{\frac{\delta t}{2}}^{\textsc{b}}
  \rpa
  =
  \hat{\flowmap}_{\frac{\delta t}{2}}^{\textsc{b}} \circ 
  \hat{\flowmap}_{\delta t}^{\textsc{a}} \circ 
  \hat{\flowmap}_{\delta t}^{\textsc{b}} \circ
  \hat{\flowmap}_{\delta t}^{\textsc{a}} \circ 
  \hat{\flowmap}_{\frac{\delta t}{2}}^{\textsc{b}}
\end{equation*}
and so the two different symmetric compositions only differ by whether initial and final half momentum or position time steps are taken. 

Although an approximate flow map will no longer exactly conserve the Hamiltonian, a key property of symplectic integrators, including the leapfrog method, is that they correspond to the exact flow map of a modified Hamiltonian system. Providing the integrator step-size $\delta t$ is below a stability threshold this modified Hamiltonian $\tilde{\hamiltonian}_{\delta t}$ will be close to the original target Hamiltonian: 
\(
  \left|\hamiltonian(\vct{x},\vct{p}) - \tilde{\hamiltonian}_{\delta t}(\vct{x},\vct{p})\right|  
  \leq \mathcal{O}(\delta t^k)
\) 
where $k$ is the order of the integrator ($k=2$ for the leapfrog method) \citep{leimkuhler2004simulating}. As the approximate flow map exactly conserves this modified Hamiltonian, this means that the change in the Hamiltonian over long simulated trajectories will remain bounded. If we replace the exact flow map for the approximate flow map corresponding to $L$ steps of the leapfrog integrator with step size $\delta t$ in \eqref{eq:exact-hmc-accept-probability} then we have that the probability of accepting a proposal generated by approximately integrating the \acp{ODE} and then negating the momentum is
\begin{equation}\label{eq:hmc-accept-probability}
  \alpha(\vct{x},\vct{p}) =
  \min\lbr 
    1,\,
    \exp\lpa
      \hamiltonian(\vct{x},\vct{p}) -\hamiltonian \circ \vct{r} \circ \lpa \hat{\flowmap}^{\textsc{lf}}_{\delta t}\rpa^L(\vct{x}, \vct{p})
    \rpa
  \rbr.
\end{equation}
As the change in Hamiltonian over the trajectory remains bounded, if the step-size is small enough the probability of acceptance will remain close to one even for a large number of integrator steps $L$. This means simulated Hamiltonian dynamics can be used to form long-range proposed moves which maintain a high probability of acceptance. An example of this approximate conservation of the Hamiltonian is shown in Figure \ref{fig:hamiltonian-monte-carlo} which shows a visualisation of trajectories simulated using the leapfrog method in a system with a one-dimensional target variable $\rvar{x}$ and the variation in the Hamiltonian, potential and kinetic energies along these trajectories.

\begin{algorithm}[!t]
\caption{Hamiltonian Monte Carlo.}
\label{alg:hamiltonian-monte-carlo}
\input{algorithms/hamiltonian-monte-carlo}
\end{algorithm}

When using an approximate flow map as there is now a non-zero probability of rejection, upon rejecting the momentum will remain at its current state, before then being negated. The next simulated trajectory will therefore backtrack along a previous trajectory after a rejection. To prevent this backtracking behaviour and resolve the issue that Markov transitions consisting solely of simulated dynamics proposals and momentum-reversals would remain confined to a constant modified Hamiltonian manifold, a further update is introduced in to the overall \ac{HMC} transition which changes the momenta while leaving the target variables fixed. 

For the common case of a quadratic kinetic energy $\tau(\vct{p}) = \frac{1}{2}\vct{p}\tr\mtx{M}^{-1}\vct{p}$ and so normal marginal distribution on the momenta, the simplest way to update the momenta is to independently sample a new vector from $\nrm{\vct{0},\mtx{M}}$ at the beginning of each \ac{HMC} transition. This will both perturb the initial Hamiltonian of the system and also mean the initial direction of any simulated trajectory is randomised so that the negation of the previous momentum upon a rejection does not lead to backtracking. In this case as the momentum is independently resampled in each transition there is no need to store the momentum state between successive transitions and the overall \ac{HMC} transition will be reversible.  

Algorithm \ref{alg:hamiltonian-monte-carlo} describes the overall \ac{HMC} transition corresponding to using a quadratic kinetic energy $\tau(\vct{p}) = \frac{1}{2}\vct{p}\tr\mtx{M}^{-1}\vct{p}$, independent moment\-um resampling and a leapfrog integrator. The integrator step size $\delta t$ and number of steps $L$ together determine the total approximate integration time $T = L \delta t$. Intuitively we want to choose $T$ to minimise the dependence of the generated proposals on the current point. In general however we will not know what this optimal integration time is and in most problems it will depend on the starting state. Choosing an appropriate integration time can therefore be challenging, and will often involve some level of trial and error with pilot runs \citep{neal2011mcmc}. Too small values lead to dynamics proposals which remain close to the current state which combined with the independent resampling of the momenta on each transition lead to random-walk like behaviour for the overall transition, with limited gain from using the \ac{HMC} transition over simpler methods such as random-walk Metropolis. 

The computational cost of each \ac{HMC} transition will however scale linearly with $L$ and so the integration time, therefore it is desirable to not increase the integration time beyond the point where there is any gain in decreased dependence between successive points; further as typically the simulated trajectories will be quasi-periodic increasing the integration time can in some cases lead to proposals moving closer to the original state. The integration time does not need to be the same for each transition and randomising it by for example uniformly sampling from an interval can be helpful in some problems to reduce pathological behaviour due to near periodicity of trajectories \citep{neal2011mcmc}.

In combination with the integration time an appropriate value must also be chosen for the integrator step size $\delta t$. As for a fixed integration time $T$ the step size determines the number integrator steps needed and so computational cost per transition, we ideally want to use as large a step size as possible. The step size however also controls how large the typical change in the Hamiltonian is across a simulated trajectory and so the accept rate for the proposed updates. As the step size increases the average accept rate will decrease and beyond some limit typically the dynamic will become unstable and the Hamiltonian error no longer remain bounded, leading to very low accept rates for large $L$. Typically this stability limit will vary depending on the starting state, so we may occasionally encounter unstable diverging trajectories even when the step size is small enough for most trajectories to remain stable. 

Analogously to results for tuning the proposal width for random-walk Metropolis methods, guidelines have been derived for choosing the integrator step-size in \ac{HMC} based on an optimal (in the sense of maximising some measure of computational efficiency) average accept probability for the Metropolis step. A target accept rate of 0.65 has been suggested \citep{neal2011mcmc,beskos2013optimal} under idealised assumptions of a high-dimensional target distribution in which the individual dimensions are independent and when using the leapfrog integrator. Under more general assumptions in \citep{betancourt2014optimizing} an accept rate range of 0.6 to 0.9 was instead recommended as giving close to optimal performance for symplectic integrators of order 2 including the leapfrog method.

To help address the challenges of tuning the free integrator step-size and integration time parameters of the standard \ac{HMC} algorithm, adaptive variants which automatically tune these parameters have been proposed. Of particular note is the \ac{NUTS} algorithm \citep{hoffman2014no}. Rather than using a single fixed integration time, \ac{NUTS} dynamically varies the length of the simulated trajectories, expanding the trajectories until a termination criterion corresponding intuitively to the trajectory `turning back on itself' (hence the name) is met and then using a slice sampling scheme to select a new state from this dynamically generated trajectory. Maintaining reversibility in such a scheme is non-trivial and \ac{NUTS} uses an elegant recursive method to do so: full details are beyond the scope of this review but both \citep{hoffman2014no} and a subsequent review article \citep{betancourt2017conceptual} provide excellent visual explanations of the algorithm. The dynamic integration time scheme is combined in \ac{NUTS} with a stochastic optimisation method for tuning the integrator step-size to achieve a target acceptance rate, with a vanishing adaptation rate ensuring convergence of the chains to stationarity \citep{andrieu2008tutorial}.

Refinements to \ac{NUTS} have been suggested including generalised termination criteria \citep{betancourt2016identifying,betancourt2013generalizing} and an extension to use multinomial sampling of the final state from the generated trajectory instead of slice sampling \citep{betancourt2016identifying,betancourt2017conceptual}. The original \ac{NUTS} algorithm and these refinements have seen widespread empirical success through their implementation in the probabilistic programming framework \emph{Stan} \citep{carpenter2016stan} which combines a general purpose probabilistic model specification language \citep{stan2017stan} and automatic differentiation library \citep{carpenter2015stan} with efficient implementations of approximate inference methods including \ac{NUTS}.

We have so far neglected to mention how the mass matrix $\mtx{M}$ is chosen. The simplest choice is to use $\mtx{M} = \idmtx$; this is a reasonable choice when the target density has a close to isotropic geometry with approximately equal scaling of each dimension and no strong correlations between variables. Using a non-identity mass matrix is equivalent to running \ac{HMC} with a identity mass matrix in a linearly transformed reparameterisation of the target density \citep{neal2011mcmc} and so can be used to account for non-isotropic target densities by rescaling and decorrelating the variables of the target distribution. Ideally the mass matrix would be chosen based on the covariance of the target distribution \citep{neal2011mcmc,betancourt2017conceptual} however in practice this will typically not be available. One option is to estimate the target covariance during an initial adaptive phase in the chain which is used within the \ac{NUTS} implementation in Stan \cite{carpenter2016stan}.

In reality for complex target distributions the geometry of the density will vary across the state space with position-dependent curvature. In these cases a single constant mass matrix will be ineffective at locally decorrelating and normalising the scale of the variables corresponding to the different dimensions of the target distribution. This can degrade the performance of \ac{HMC} methods, with no single step size appropriate in all regions of the state space and typically a trade-off needing to be made between choosing a small step size based on the scale of the most constrained directions and choosing a larger step size for efficiency with the possible result of the simulated dynamic being unable to enter tightly constrained regions of the target distribution \citep{betancourt2013general}. 

For a quadratic kinetic energy function $\tau(\vct{p}) = \frac{1}{2}\vct{p}\tr\mtx{M}^{-1}\vct{p}$, considering the kinetic energy as a random variable $\uptau = \tau(\rvct{p})$, as $\rvct{p}$ has a multivariate normal marginal distribution, $\uptau$ will have mean $\nicefrac{D}{2}$ and standard deviation $\sqrt{D}$ \citep{neal2011mcmc}. As the kinetic energy is bounded below by zero and the Hamiltonian and so sum of kinetic and potential energies approximately conserved along simulated trajectories, the maximal increase in the potential energy along a trajectory is approximately upper bounded by the initial kinetic energy. The potential energy will therefore typically vary by an amount of order $D$ over simulated trajectories. For complex target distributions with varying curvature and scales, the potential energy will often vary by much more than $D$ across the typical set of the distribution and so any single trajectory will typically be only able to cover a small region in the typical set, with exploration of the full typical set of the distribution then degrading to a random-walk like behaviour as only the momentum resampling steps allows movement up and down the full potential energy range \citep{betancourt2013general,betancourt2017conceptual}. %The limitation in the change in potential energy over a trajectory to $\sim \nicefrac{D}{2}$ will also mean if any potential energy barriers between separated modes in the target density involve a potential energy change $\gg D$ then rarely will the simulated trajectories traverse between the modes.

\marginpar{The naming of \acs{RMHMC} arises from a connection to Riemannian geometry, with the metric defining a Riemannian manifold, with the length of shortest path between two points on the manifold, i.e. a \emph{geodesic}, locally defined by the metric.}
A suggested resolution to the issue of varying curvature across the target density is to use a mass matrix which depends on the target state $\rvct{x}$. \acf{RMHMC} \citep{girolami2011riemann} defines a non-separable Hamiltonian using a kinetic energy function
\begin{equation}
  \tau(\vct{p}\gvn\vct{x}) = 
  \frac{1}{2}\vct{p}\tr\lpa\mtx{G}(\vct{x})\rpa^{-1}\vct{p} + \frac{1}{2}\log\left|\mtx{G}(\vct{x})\right|
\end{equation} 
corresponding to $\pden{\rvct{p}|\rvct{x}}(\vct{p}\gvn\vct{x}) = \nrm{\vct{p}\gvn\vct{0}, \mtx{G}(\vct{x})}$ where $\mtx{G} : \reals^D \to \reals^{D\times D}$ is a positive-definite matrix function termed the \emph{metric}. In analogy to the earlier mentioned equivalence between using a non-identity constant mass matrix and running \ac{HMC} with an identity mass matrix in a reparameterised target distribution in terms of a linear transformation of the original target variables \citep{neal2011mcmc}, \ac{RMHMC} can be shown to be equivalent to running \ac{HMC} with an identity mass matrix in a reparameterisation of the target distribution in terms of a \emph{non-linear} transformation of the target variables \citep{nishimura2016geometrically}. This non-linear reparameterisation can locally transform the target distribution so that the resulting density has a geometry more amenable to exploration by the \ac{HMC} dynamic. 

Various schemes have been proposed for choosing a metric for a particular target distribution. In \citep{girolami2011riemann} the \emph{Fisher--Rao metric} \citep{amari1982differential} is suggested as it provides a natural description of the Riemannian geometry of parametric probability distributions and so is particularly relevant for target distributions corresponding to the posterior of Bayesian inference problems for models of \acs{IID} datasets. The Fisher--Rao metric only has a closed form solution however for a limited set of distributions. An alternative more generally applicable metric based on a regularisation of the Hessian of the log target density to ensure positive-definiteness was suggested in \citep{betancourt2013general}. A `geometrically tempered' metric designed to help exploration of multimodal distributions was suggested in \citep{nishimura2016geometrically}.

The non-separable nature of the Hamiltonian in \ac{RMHMC} means that the standard leapfrog method cannot be employed to simulate the resulting dynamic, with alternative symplectic integrators such as the \emph{generalised leapfrog method} \citep{leimkuhler2004simulating} required. These integrators involve implicit steps which requires solving a set of non-linear equations on each iteration. Further evaluation of the inverse of the metric and its log determinant in general have a cost which scales cubically with $D$, therefore the overall computational cost of simulating the \ac{RMHMC} dynamic is much higher per transition than for standard \ac{HMC}. In some target distributions the gain in sampling efficiency over standard \ac{HMC} from using \ac{RMHMC} updates can significantly outweigh the increased computational cost per sample however \citep{girolami2011riemann,betancourt2013general}. 

%n particular if $\vctfunc{g}: \reals^D \to \reals^D$ is a smooth map, then a \ac{RMHMC} chain using a metric $\mtx{G}(\vct{x}) = \jacob{\vctfunc{g}}{\vct{x}}\tr \jacob{\vctfunc{g}}{\vct{x}}$ is equivalent to perform

%We will consider extensions of the standard \ac{HMC} approach to distributions defined on implicitly-defined manifolds in Chapter \ref{ch:differentiable-generative-models}. %embedded in a Euclidean space

%based which multiplicatively expands the trajectory by randomly either integrating forward or backwards in time a number of steps equivalent to the current trajectory length, recursively validating that all newly included

%n building a binary tree of states by repeatedly doubling the trajectory length by i

%To maintain reversibility this dynamic integration time scheme generates simulated trajectories by stepping both forwards and backwards in time. In \ac{NUTS} trajectories are expanded in a multiplicative fashion, on each iteration a current trajectory of $L$ steps doubled in length by randomly choosing a direction of integration and then integrating forward or backwards by $L$ steps. This process is repeated until a termination criteria is met and then a slice sampling scheme used to sample a state from the generated . Importantly this termination criteria needs to maintain reversibility



%An alternative scheme is to instead pertubatively update the momenta. If the momentum have a multivariate normal marginal $\nrm{\vct{0},\mtx{M}}$, then if $\vct{p}$ is the current momentum state and $\vct{n}$ is a vector independently generated from $\nrm{\vct{0},\mtx{M}}$, then an update of the form
%\begin{equation}\label{eq:horowitz-partial-momentum-update}
%  \vct{p}' \gets \gamma \vct{p} + \sqrt{1 - \gamma^2} \vct{n}
%\end{equation}
%for some $\gamma \in [0,1]$ will leave the marginal distribution on the momenta invariant. For $\gamma = 0$ this update corresponds to not updating the momenta at all and for $\gamma = 1$ to independently resampling on each transition. For intermediate $\gamma$ the momenta will be `partially resampled', with the new momenta correlated to the previous values. This update was originally proposed in \citep{horowitz1991generalized} and is generally termed either the \emph{generalised \ac{HMC} algorithm} or \emph{partial momentum resampling}. The motivation for the scheme is to try to reduce the random-walk behaviour introduced by independently resampling the momentum in each transition. 

% free parameters - step size, number of leapfrog steps
% NUTS
% Riemannian manifold
% Stan and PyMC3

\subsection{Simulated tempering}\label{subsec:simulated-tempering}

The final auxiliary variable method we consider, \emph{simulated tempering} \citep{marinari1992simulated}, simulates the dynamics of a thermodynamic system subject to a varying temperature. Simulated tempering was originally proposed to improve the exploration of highly-multimodal distributions defined by undirected models such as the Ising spin model. 

A particle of systems with a state described by a vector $\vct{x} \in \set{X}$ and a total energy determined by a function $\phi : \set{X} \to \reals$ will have an equilibrium distribution on the state at a temperature $T$ which has a density proportional to $\exp\lpa -\beta \phi(\vct{x})\rpa$ where $\beta = (kT)^{-1}$ is the \emph{inverse temperature} and $k$ is Boltzmann's constant. If the energy function $\phi$ is `rough' with multiple local minima, then as the temperature $T$ tends to zero and $\beta \to \infty$ the corresponding peaks in the density function become increasingly sharp and the mass of the distribution more tightly concentrated around these peaks. Conversely as the temperature $T$ increases and $\beta \to 0$, the density becomes increasingly flat across $\set{X}$.% the state space with the peaks becoming increasingly less pronounced.

\emph{Simulated annealing} \citep{kirkpatrick1983optimization,ackley1985learning}, is a stochastic optimisation method which uses this intuition about the properties of thermodynamical systems to improve the probability of an optimisation routine converging to a global optima in highly multimodal objectives. The objective function  to be minimised is identified with the energy function $\phi$ of the system and the variables being optimised with the state $\vct{x} \in \set{X}$. An increasing \emph{schedule} of $K$ inverse temperatures $\lbrace \beta_k \rbrace_{k=1}^K$ is chosen with $0 \leq \beta_1 \leq \beta_2 \leq \dots \leq \beta_K \leq \infty$. An initial value for the target variables $\vct{x}_0$ is (randomly) chosen and new values for the target variables are then computed iteratively for each $k \in \fset{1 \dots K}$ by applying a Metropolis--Hastings transition operator $\vct{x}_k \sim \transop_k(\cdot \gvn \vct{x}_{k-1})$ which leaves the distribution with density proportional to $\exp\lpa-\beta_k\phi(\vct{x})\rpa$ invariant. 

The hope is that the transitions at low inverse temperatures will be able to move freely around the target space due to the relatively flat form of the density function with lowered barriers between modes. Ideally the state will therefore tend to converge towards the modes with the largest mass. As the inverse temperature is increased the density function becomes increasingly peaked and the updates will tend to remain confined to one mode and as $\beta \to \infty$ will become concentrated near to the maximum of this mode. Although there is no guarantee this heuristic will find a global optima, empirically it has been found to be useful in practice in a range of applications.

\begin{figure}[t]
\centering
\includetikz{geometric-bridge-example}
\caption[Geometric density bridge example.]{Example of using an inverse temperature to geometrically bridge between unimodal base and bimodal target densities. Each curve shows the conditional density $\pden{\rvar{x}|\rvar{k}}$ corresponding to the joint target density \eqref{eq:simulated-tempering-joint-target-density} for $\rvar{k}=0$ to $\rvar{k}=5$ with $\beta_k = k / 5$ in this case.}
\label{fig:geometric-bridge-example}
\end{figure}

In simulated tempering, rather than using an inverse temperature to define an optimisation procedure instead a discrete index controlling the inverse temperature is introduced as an auxiliary variable in an \ac{MCMC} method. The variables $\rvct{x} \in \set{X}$ on which the target distribution $\tgtprob$ is defined are augmented with a discrete index variable $\rvar{k} \in \fset{0 \dots K}$.  A corresponding set of inverse temperature values $\fset{\beta_k}_{k=0}^K$ are specified, as with simulated annealing these chosen to form an ordered sequence but in this case over the interval $[0,1]$ with 
\begin{equation}
0 = \beta_0 < \beta_1 < \beta_2 < \dots < \beta_K =1.
\end{equation} 
A joint density on the target variables $\rvct{x}$ and temperature index $\rvar{k}$ is then defined as
\begin{equation}\label{eq:simulated-tempering-joint-target-density}
  \pden{\rvct{x},\rvar{k}}(\vct{x},k) = \frac{1}{C}
  \exp\lpa - \beta_k \phi(\vct{x}) - (1 - \beta_k) \psi(\vct{x}) + w_k\rpa.
\end{equation}
The values $\lbrace w_k \rbrace_{k=0}^K$ are a set of `prior' weights associated with each inverse temperature value, and which can be used to help shape the marginal distribution on the temperature index $\rvar{k}$. As in the preceding subsection the energy function $\phi : \set{X} \to \reals$ is defined as the negative logarithm of the unnormalised target density i.e. $\phi(\vct{x}) = -\log \utgtdens(\vct{x})$. 

The function $\psi : \set{X} \to \reals$ defines a corresponding energy function for a \emph{base distribution} $Q$ with {normalised} density $q(\vct{x}) = \exp\lpa-\psi(\vct{x})\rpa$ with respect to $\mu$. The base distribution is typically chosen to have a simple unimodal density with mass covering as many of the regions of high density under the target density in $\set{X}$ as possible. When the target distribution corresponds to the posterior in a Bayesian inference task, $Q$ is often chosen as the prior distribution on the target variables which will typically have a simple unimodal form and be much more diffuse than the posterior. If the state space $\set{X}$ consists of a finite set of values, the base distribution can be chosen to be uniform across $\set{X}$ in which case $\psi(\vct{x})$ is constant and can be omitted from \eqref{eq:simulated-tempering-joint-target-density}.

Importantly the conditional distribution $\prob{\rvct{x}|\rvar{k}}$ on the target variables $\rvct{x}$ for $\rvar{k} = 0$ ($\beta_0 = 0$) corresponds to the base distribution $Q$ and to the target distribution $\tgtprob$ for $\rvar{k} = K$ ($\beta_K = 1$). We can therefore use the $\rvct{x}$ components of sampled chain states for which $\rvar{k} = K$ to estimate expectations with respect to the target distribution $\tgtprob$. For intermediate values of $\rvar{k}$ the conditional distribution geometrically interpolates between $\tgtprob$ and $Q$. Figure \ref{fig:geometric-bridge-example} shows a simple example of this geometric bridging between a unimodal univariate base density and bimodal target density for $K=5$ inverse temperature values.

\begin{algorithm}[!t]
\caption{Simulated tempering.}
\label{alg:simulated-tempering}
\input{algorithms/simulated-tempering}
\end{algorithm}

In simulated tempering, a Markov chain with an invariant distribution corresponding to \eqref{eq:simulated-tempering-joint-target-density} is constructed by alternating updates of the target variables $\rvct{x}$ given the current value of temperature index $\rvar{k}$, with updates of the temperature index $\rvar{k}$ given the current value of the target variables $\rvct{x}$ as summarised in Algorithm \ref{alg:simulated-tempering}. For the transition operator $\transop_1$ updating the target variables, any of the previously discussed methods such as random-walk Metropolis, slice sampling or \ac{HMC} can be used. In the case of Metropolis--Hastings based updates, it may be desirable to adjust the proposal generating mechanism to depend on the current temperature index $\rvar{k}$ as for example we might generally expect for $\rvar{k}$ corresponding to lower inverse temperatures $\beta_{\rvar{k}}$ and so conditional densities $\pden{\rvct{x}|\rvar{k}}$ closer to the base density that larger moves can be made while maintaining reasonable accept rates; as $\rvar{k}$ remains fixed this can validly be done without breaking reversibility.

In the original description of the simulated tempering algorithm in \citep{marinari1992simulated}, the transitions to the index variable $\rvar{k}$ given fixed values of the target variables $\rvct{x}$ were performed using a random-walk Metropolis operator for $\transop_2$ which proposes to randomly increment or decrement $\rvar{k}$ by one (except at the end-points $\rvar{k}=0$ and $\rvar{k}=K$ where it always proposed to increment and decrement respectively). For large $K$ this can lead to slow mixing up and down the inverse temperature scale - if the marginal density $\pden{\rvar{k}}$ is uniform we would expect $\mathcal{O}(K^2)$ updates would be needed to traverse the full inverse temperature range. An alternative is to use a Gibbs sampling step with the conditional distribution $\prob{\rvar{k}|\rvct{x}}$ here being a multinomial distribution with density
\begin{equation}\label{eq:simulated-tempering-temp-index-conditional}
  \pden{\rvar{k}|\rvct{x}}(k \gvn \vct{x}) = 
  \frac{\exp\lpa \beta_k \lpa \psi(\vct{x}) - \phi(\vct{x}) \rpa + w_k \rpa}
  {\sum_{k'=0}^K \exp\lpa \beta_{k'} \lpa \psi(\vct{x}) - \phi(\vct{x}) \rpa + w_{k'} \rpa}
\end{equation}
which we can tractably generate independent samples from. For arbitrary $\lbrace \beta_k, w_k \rbrace_{k=0}^K$ this will require explicit summation over $K+1$ values to calculate the normalising constant and so the cost of generating an independent index will scale linearly with $K$. 

For $\beta_k = \frac{k}{K}$ and $w_k = \alpha \beta_k ~\forall k \in \lbrace 0\dots K\rbrace$ for some $\alpha \in \reals$, the normalising constant in \eqref{eq:simulated-tempering-temp-index-conditional} takes the form of a geometric series
%  \sum_{k=0}^K \exp\lpa \beta_{k} \lpa \psi(\vct{x}) - \phi(\vct{x}) \rpa + w_{k} \rpa =
\begin{equation}
  \sum_{k=0}^K \exp\lpa \frac{\psi(\vct{x}) - \phi(\vct{x}) + \alpha}{K} \rpa^k = 
  1 + \frac{\exp\lpa \psi(\vct{x}) - \phi(\vct{x}) + \alpha \rpa}{1 - \exp\lpa \frac{\psi(\vct{x}) - \phi(\vct{x}) + \alpha}{K} \rpa}.
\end{equation}
The conditional distribution $\prob{\rvar{k}|\rvct{x}}$ in this case has the form of a geometric distribution with parameter $\exp\lpa \frac{\psi(\vct{x}) - \phi(\vct{x}) + \alpha}{K} \rpa$ truncated to $\fset{0\,\dots K}$ which we can generate samples at a cost independent of $K$.

The marginal distribution $\prob{\rvar{k}}$ on the index variable $\rvar{k}$ has density
\begin{equation}\label{eq:simulated-tempering-temp-index-marginal}
  \pden{\rvar{k}}(k) = 
  \frac{\exp(w_k)}{C} \int_{\set{X}} \exp\lpa -\beta_k\phi(\vct{x}) -(1-\beta_k)\psi(\vct{x})\rpa\,\mu(\dr\vct{x}).
\end{equation}
As $\int_{\set{X}}\exp\lpa-\psi(\vct{x})\rpa\,\mu(\dr\vct{x})=1$ and $\int_{\set{X}}\exp\lpa-\phi(\vct{x})\rpa\,\mu(\dr\vct{x})=Z$ we have 
\begin{equation}
  \pden{\rvar{k}}(0) = \frac{\exp(w_0)}{C}
  \qquad\textrm{and}\qquad
  \pden{\rvar{k}}(K) = \frac{\exp(w_K) Z}{C}.
\end{equation}
If $Z$ is much more than one and $w_k = 0$ for all $k \in \lbrace 0 \dots K\rbrace$ then we would have $\pden{\rvar{k}}(K) \gg \pden{\rvar{k}}(0)$ and a simulated tempering chain will tend to spend many more iterations with $\rvar{k} = K$ than $\rvar{k} = 0$. This will give a large number of samples with which to estimate expectations with respect to $\tgtprob$ however it will also limit the gain from using simulated tempering over running a Markov chain in the original non-augmented target variable space, as the chain will rarely visit the lower inverse temperatures which aid exploration. Conversely if $Z$ is much less than one, we have $\pden{\rvar{k}}(K) \ll \pden{\rvar{k}}(0)$. In this case the chain will tend to remain at $\rvar{k}$ values corresponding to low inverse temperatures and so few samples are available for computing expectations with respect to $\tgtprob$.

If we could set $w_0 - w_K = \log Z$ we would have $\pden{\rvar{k}}(K) = \pden{\rvar{k}}(0)$ however for the target distributions of interest we will generally not be able to evaluate $Z$ and a similar result holds for the normalising constants of the conditional distributions $\prob{\rvct{x}|\rvar{k}}$ corresponding to intermediate inverse temperatures and so the appropriate values for $\lbrace w_k \rbrace_{k=1}^{K-1}$. In general therefore it will be difficult to identify reasonable values to set the weights $\lbrace w_k \rbrace_{k=0}^K$ to a-priori. This is typically solved in practice by using an iterative scheme \citep{marinari1992simulated,iba2001extended}: an initial pilot chain is run with $w_k = 0 ~\forall k$ to estimate the marginal density $\pden{\rvar{k}}$ by constructing a histogram of counts of samples for each $k$ and then this histogram used to set the weights so as to approximately flatten the marginal density. %This process can be iterated several times if necessary. %which might be necessary if for example the initial chain spends all of its iterations at one end of the inverse temperature range.

The relationship between the marginal density $\pden{\rvar{k}}$ and $Z$ although pres\-enting challenges in terms of choosing the weights $\lbrace w_k \rbrace_{k=0}^K$ also however demonstrates that simulated tempering chains can be used to estimate $Z$. In particular we have that
\begin{equation}\label{eq:simulated-tempering-norm-const-marginal-density}
  Z = \exp(w_0 -w_K) \frac{\pden{\rvar{k}}(K)}{\pden{\rvar{k}}(0)}.
\end{equation}
Given the sampled states $\lbrace \vct{x}^{(n)}, k^{(n)} \rbrace_{n=1}^N$ of a simulated tempering chain, one way to form a consistent estimate of $Z$ is therefore to compute the ratio of the counts of samples with $\rvar{k}= K$ to those with $\rvar{k} = 0$,
\begin{equation}\label{eq:simulated-tempering-norm-const-est-count}
  Z = \lim_{N \to \infty} \exp(w_0 - w_K) \frac{\sum_{n=1}^N \ind{\fset{K}}(k^{(n)})}{\sum_{n=1}^N \ind{\fset{0}}(k^{(n)})}.
\end{equation}
This estimate will typically have a high variance however as it uses information only from the subset of sampled states with $\rvar{k}=0$ or $\rvar{k}=K$. Expanding $\pden{\rvar{k}}$ as a marginalisation integral of the joint density \eqref{eq:simulated-tempering-joint-target-density} we can reformulate the identity in \eqref{eq:simulated-tempering-norm-const-marginal-density} as 
\begin{equation}\label{eq:simulated-tempering-norm-const-rao-blackwellised}
  Z = \exp(w_0 -w_K) 
  \frac
  {\int_{\set{X}}\pden{\rvar{k}|\rvct{x}}(K \gvn \vct{x})\,\pden{\rvct{x}}(\vct{x})\,\mu(\dr\vct{x})}
  {\int_{\set{X}}\pden{\rvar{k}|\rvct{x}}(0 \gvn \vct{x})\,\pden{\rvct{x}}(\vct{x})\,\mu(\dr\vct{x})}.
\end{equation}
This is an example of what is sometimes termed \emph{Rao-Blackwellisation} \citep{casella1996rao} and was used in \citep{carlson2016partition} to suggest a \emph{Rao-Blackwellised estimator} for the normalising constant $Z$ from the samples $\lbrace \vct{x}^{(s)}, k^{(s)} \rbrace_{s=1}^S$ of a simulated tempering chain
\begin{equation}\label{eq:simulated-tempering-norm-const-est-rb}
  Z = \lim_{N \to \infty} \exp(w_0 - w_K) \frac{\sum_{n=1}^N \pden{\rvar{k}|\rvct{x}}(K \gvn \vct{x}^{(n)})}{\sum_{n=1}^N \pden{\rvar{k}|\rvct{x}}(0 \gvn \vct{x}^{(n)})}.
\end{equation}
This estimator uses all of the sampled chain states and will typically be lower variance than the count-based estimator \eqref{eq:simulated-tempering-norm-const-est-count}. Importantly this estimator can still give reasonable estimates for $Z$ when there are no sampled states for which $\rvar{k}=0$ (or $\rvar{k}=K$) unlike the count-based estimator. This is particularly important when using an iterative scheme to choose the weights $\lbrace w_k \rbrace_{k=0}^K$ as if $Z \gg 1$ or $Z \ll 1$ an initial short pilot chain will typically remain confined to one end of the inverse temperature scale for all iterations, giving limited count-based information with which to update weights for subsequent iterations.

\section{Discussion}

The sampling approaches to approximate inference described in this chapter allow tractable estimation of the integrals involved in many inference problems. In cases where we can generate independent samples from the target distribution, the $\frac{1}{N}$ scaling of the variance of Monte Carlo estimates of expectations with the number of samples $N$ allows computation of estimates with sufficient accuracy for most practical purposes without the exponential blow-up in computation of quadrature methods with dimensionality. %Further simple Monte Carlo methods are trivially parallelisable meaning even if generation of each independent sample is relatively expensive, parallel compute devices such as \acp{GPU} and \ac{CPU} clusters can easily be exploited if available.

Generating independent samples from arbitrary distributions on high-dimensional spaces is often infeasible however. Transform sampling methods offer a scalable approach for only a few special cases such as the multivariate normal distribution. Rejection sampling is more generally applicable however the usually exponential decrease in the proportion of accepted samples with dimension means that it is only useful in relatively low-dimensional distributions. Simple importance sampling schemes similarly scale poorly with dimensionality, with mismatch bet\-ween the proposal and target distribution in high-dimensions meaning the variance of the resulting estimators is impractically high.

Although these Monte Carlo methods are not directly applicable to performing inference in the complex probabilistic models of interest, they are still useful building blocks and will appear as components of the methods we will discuss in the rest of this thesis. In Chapter \ref{ch:pseudo-marginal-methods} we will discuss \ac{MCMC} methods which use importance sampling estimators of the target density to construct the chain. The simulator models discussed in Chapter \ref{ch:differentiable-generative-models} can be considered an extension of the idea of transform sampling, with a complex series of deterministic operations transforming inputs from a pseudo-random number generator to simulated values for the variables in a probabilistic model. One of the standard approaches for performing approximate inference in simulator models is based on rejection sampling, and our discussion of the poor scaling of rejection sampling with dimensionality will be relevant when considering the limitations of these methods.

Markov chain Monte Carlo methods offer a more scalable approach to inference in complex probabilistic models and are the main focus of the work discussed in this thesis. The local perturbative updates typically employed in \ac{MCMC} methods avoid the curse of dimensionality effects which lead to the exponential blow up in the computational effort required by methods such as rejection sampling as the dimension increases. \ac{MCMC} methods such as random-walk Metropolis and Gibbs sampling typically require minimal implementation effort and have successfully applied in a wide range of settings. %The simple random-walk exploration supported by these methods particularly when the target distribution has a relatively simple geometry which is often the case when performing inference in simple parametric models of large \ac{IID} datasets.

For target distributions with more complex geometries however such as due to the non-linear relationships between variables often present in hierarchical models or the multimodal distributions typically arising from inference in undirected models, \ac{MCMC} methods such as random-walk Metropolis and Gibbs sampling can exhibit pathological behaviour that means impractically long chains are needed for \ac{MCMC} estimators to give useful results. In these cases methods which exploit more information about the geometry of the distribution in each update can offer significant improvements in efficiency and robustness.

The introduction of auxiliary variables in to the chain state has proved a particularly successful approach for proposing \ac{MCMC} methods which can accelerate the exploration of complex target distributions. We concluded this chapter by reviewing three auxiliary variable \ac{MCMC} methods that will be central to the contributions made in this thesis: slice sampling, Hamiltonian Monte Carlo and simulated tempering. 

Slice-sampling offers a very generally applicable approach for constructing Markov chains which are able to adapt the scale of proposed moves to the local geometry of the target distribution. The information controlling this adaptation comes from allowing multiple evaluations of the target density per update in slice sampling compared to for example the single target density evaluation per iteration of random-walk Metropolis methods. The overhead from these multiple density evaluations will mean that for target distribution in which the geometry of the density does not vary significantly across the space, well-tuned random-walk Metropolis updates will often be able to outperform slice sampling transition operators in terms of the computational cost per effective independent sample. However the ease of use of slice sampling methods, with typically minimal user tuning required of the free algorithmic parameters, and increased robustness to distributions with more complex geometries, are in our opinion often more important than a potential improvement in peak efficiency.

Hamiltonian Monte Carlo methods put a requirement of differentiability on the target density and so are not as widely applicable as slice sampling approaches. When available however gradient information can be a significant help in guiding the exploration of the target space by a \ac{MCMC} dynamic. Using reverse-mode automatic differentiation (as described in Appendix \ref{ch:computation-graphs}) code for evaluating the exact gradients of a density function can be automatically generated given just the definition of the original density function and the resulting gradient function evaluated at a cost which has only a constant factor overhead over the cost of the original density function evaluations. When optimally tuned \ac{HMC} methods can overcome the random-walk behaviour inherent to simpler \ac{MCMC} methods and so offer significantly improved performance in complex high-dimensional target distributions. Although implementation of \ac{HMC} algorithms is more complex than approaches such as Gibbs sampling and random-walk Metropolis and the tuning of the algorithm parameters can be vital for good performance, the availability of efficient, adaptive implementations in probabilistic programming frameworks such as Stan \citep{carpenter2016stan} and PyMC3 \citep{salvatier2016probabilistic} has supported the use of \ac{HMC} in a wide range of inference problems.

Simulated tempering offers a complementary approach to the improved local exploration afforded by slice sampling and \ac{HMC} methods by potentially improving the global exploration of challenging multimodal target distributions. As the updates to the target variables at a fixed inverse temperature can be performed using any valid Markov transition operator applicable to the original target distribution, both slice sampling and \ac{HMC} transition operators can be used within a simulated tempering chain and both potentially offer an improved ability to adapt to the varying geometry of the density on the target variables at different inverse temperatures compared to simpler methods such as random-walk Metropolis. In addition to the possible improved exploration of multimodal targets, the ability to use simulated tempering chains to estimate an unknown normalising constant of the target density, often corresponding to a model evidence term, offers a further distinct advantage over standard \ac{MCMC} methods. 

Although simulated tempering can provide several important benefits, use of the algorithm in statistical applications seems relatively rare in practice. This can perhaps be partially attributed to the need to tune the values of the free inverse temperature $\beta_k$ and prior weight $w_k$ parameters introduced in the algorithm, with any improvement in exploration of the target distribution strongly dependent on the simulated tempering chain being able to move up and down the inverse temperature range. Further the lack of standard implementations in frameworks such as Stan and PyMC3, and relative wastefulness of the standard approach of estimating expectations with respect to the target distribution by averaging over only sampled states corresponding to an inverse temperature of $\beta_K = 1$% (which when using a large number of inverse temperatures $K$ as often necessary for good performance in complex targets can be a small fraction of the total number of samples)
, add further discouragements to widespread use of the algorithm.

\section{Outline of contributions}

Having now completed our reviews of both the inference tasks and \ac{MCMC} methods underlying the work in this thesis, we are now in a position to outline the contributions made in the rest of the thesis.

Inference in hierarchical models is often a challenging task for \ac{MCMC} methods due to the strong dependencies between the global and local latent variables and resulting complex geometry of the target density on the model variables. We will sometimes only be directly interested in inferring plausible values for the global latent variables in the model but will typically be unable to analytically integrate out the local latent variables. The \emph{pseudo-marginal} framework shows how an unbiased estimator of the marginal density on the global latent variables can be used to construct a Metropolis--Hastings method for sampling values of the global latent variables. Pseudo-marginal Metropolis--Hastings methods however suffer from `sticking' pathologies where chains reject updates over long series of iterations and are challenging to tune.

In Chapter \ref{ch:pseudo-marginal-methods} we demonstrate that by including the auxiliary variables used in the density estimator in the chain state alternative transition operators can be used in a pseudo-marginal setting, including adaptive rejection-free methods like slice-sampling. The resulting \emph{auxiliary pseudo-marginal} methods are able to prevent the sticking artifacts common to existing pseudo-marginal methods, are easier to tune and in some cases give significant improvements in sampling efficiency.
%In Chapter \ref{ch:pseudo-marginal-methods} we discuss extensions to the pseudo-marginal framework, which allows \ac{MCMC} chains to be constructed using only an unbiased estimator of the density of the target distribution. We specifically consider inference in hierarchical latent variable models as discussed in Chapter \ref{ch:probabilistic-modelling} as a motivating example for the use of this framework, with 

We described simulator models as a challenging setting for approximate inference methods in Chapter \ref{ch:probabilistic-modelling} due to the lack of an explicit target density on the model variables. \emph{Approximate Bayesian computation} \acused{ABC} (\ac{ABC}) is a class of methods for performing inference in such models by conditioning on simulated observations being `close' rather than exactly equal to the observed data. \ac{ABC} methods based on both rejection sampling and pseudo-marginal Metropolis--Hastings have been proposed, but both suffer from curse of dimensionality effects that mean further approximation is typically required by reducing the simulated observations and data to lower-dimensional summary statistics.

In Chapter \ref{ch:differentiable-generative-models} we show that any generative model can be considered as a deterministic transformation of a vector of auxiliary variables from a known distribution. We use this intuition to demonstrate how \ac{MCMC} methods such as slice sampling and \ac{HMC} can be applied within an \ac{ABC} setting, the improved performance of these methods compared to approaches based on pseudo-marginal Metropolis--Hastings meaning that in some cases \ac{ABC} inference can be performed without the need for summary statistics. For a restricted class of \emph{differentiable generative models} we derive an expression for conditional expectations under the model in terms of an integral against a distribution defined on an implicitly-defined manifold. We use this to propose a novel constrained \ac{HMC} method for performing approximate inference in differentiable generative models without an explicit density function on the model variables. This method allows computationally tractable inference when conditioning high-dimensional simulated observations being arbitrarily close to observed data.

Simulated tempering provides an approach for tackling two of the key challenges identified in Chapter \ref{ch:probabilistic-modelling}: performing inference in multimodal distributions such as those defined by undirected models like the Boltz\-mann machine; estimating the model evidence normalising constant terms required for model comparison. However as noted above simulated tempering is used relatively rarely in practice. In the above discussion we suggested factors which may have discouraged more widespread adoption of the algorithm: the difficulty in choosing the set of inverse temperature and prior weight values to use, the relative inefficiency of using only a small proportion of the samples in a chain to compute estimates and the lack of support for simulated tempering methods in existing inference packages. 

In Chapter \ref{ch:continuous-tempering} we suggest approaches to overcome these issues. We propose using a continuous auxiliary variable to control the inverse temperature rather than a discrete index. This sidesteps the need to choose a set of inverse temperature values and allows the auxiliary variable to be jointly updated with the target variables in a \ac{HMC} update making it straightforward to use tempering within existing \ac{HMC}-based inference packages. Further we show how all of the samples in a tempered chain can be used to estimate expectations with respect to the target distribution. Finally we demonstrate that variational inference methods provide a natural approach for choosing the base distribution bridged to during tempering and show that cheap biased approximations to the normalising constant of the target density can be exploited to help flatten the marginal density on the inverse temperature.


%Having now reviewed both the background

%In the following chapters we will propose several novel methods for approximate inference based on the auxiliary variable \ac{MCMC} approach. A common aim of several of the approaches proposed is to demonstrate how robust auxiliary variable methods such as slice sampling and Hamiltonian Monte Carlo can be applied to 

%\ac{MCMC} methods exploit the intuition that locally approximating the target distribution --- for example within a small region around a point or varying along only one direction in the target space --- is usually a much more tractable task than finding an approximation which matches the target distribution globally. This is particularly relevant in high dimensional spaces where concentration of measure will mean the typical set of a distribution is usually concentrated in to small region of the space and even seemingly small mismatches between an approximation and target can lead to very little overlap between the target and approximation typical sets. 

%Although the local updates of \ac{MCMC} methods allow their application to inference in probabilistic models with much higher dimensionalities than are feasible using simple rejection and importance sampling schemes, the rate of convergence of \ac{MCMC} estimators is in general significantly more challenging to assess than for Monte Carlo estimates using independent samples and will typically be strongly dependent both on the details of the target distribution and Markov transition operator being used. Even for target distributions with densities with relatively simple geometries, the diffusive nature of the exploration of the state space by \ac{MCMC} methods such as random-walk Metropolis and Gibbs sampling will typically mean that the computational effort to generate chain states which are close to independent of each other will scale quadratically with the dimension; although still a significant improvement on the exponential blow-up of computational cost with dimension for quadrature methods, this cost can still become significant.

%For target distributions with more complex geometrical properties such as the strong non-linear dependencies between variables often present in hierarchical models or multimodality, simple \ac{MCMC} methods can be susceptible to pathological behaviour which means that impractically long chains will be needed for \ac{MCMC} estimators to give useful results. These pathologies can sometimes be ameliorated by tuning the free parameters of methods such as random-walk Metropolis or reparameterising the target distribution, however this increases the demands on user time and expertise. 

%In this chapter we have reviewed a range of sampling-based approaches to approximate inference which can be used to help tackle the inference problems introduced in Chapter \ref{ch:probabilistic-modelling}. 

%The Monte Carlo method is deceptively simple at first glance, reducing the problem of evaluating integrals of arbitrary functions against complex probability distributions defined on high-dimensional spaces to the computation of averages of functions of random samples. Underlying this very generally applicable approach however is the significant computational challenge of generating random samples from the complex target distributions of interest.

%We have particularly focussed on Markov chain Monte Carlo methods, unlike simpler Monte Carlo method such as rejection and importance sampling these able to scale to the complex high-dimensional target distributions of interest.

%Auxiliary variable methods such as slice sampling and Hamiltonian Monte Carlo offer  

%For target distributions with complex geometrical properties such as 

%Further this rate of convergence and so overall computational efficiency for popular \ac{MCMC} approaches such as random-walk Metropolis and Gibbs sampling can be highly dependent on algorithmic choices such as proposal step-sizes or parameterisation of the target distribution

%Markov chain theory shows how we can exploit such local approximations to make perturbative updates to a Markov chain state such that realisations of the chain converge to dependent samples from the target distribution of interest. Although theory can guarantee \ac{MCMC} estimates will eventually converge to the correct values it is usually difficult to assess the rate of that convergence in practical problems making it difficult to diagnose chain convergence or a lack thereof. This can make application of \ac{MCMC} methods more challenging from a user-perspective than simple Monte Carlo methods as some level of expertise is usually needed to diagnose and find solutions to convergence issues.

%Due in part probably to their ease of implementation, Metropolis--Hast\-ings methods based on simple proposal distribution such as Gaussian random-walk Metropolis methods are very commonly used in practice. Performance of such methods is however usually very dependent on the good choice of the algorithm free parameters such as the proposal width / step size, with poor choices leading to very slow mixing chains. In target distributions with a complex geometry, for example where the appropriate scale for state updates may differ signficiantly across the target space, Metropolis--Hastings methods using simple fixed proposals may be unable to mix well even when optimally tuned.

%however still requires identifying a proposal distribution with a density which (scaled by a constant) strictly upper bounds the target distribution density. In high-dimensional distributions it will generally be infeasible to find a suitable proposal distribution which is a sufficiently tight bound, with in general the proportion of accepted samples becoming exponentially small as the dimensionality is increased, making rejection sampling only applicable to low-dimensional distributions in practice. Both methods are however useful building blocks in more complex approaches. In Chapter \ref{ch:differentiable-generative-models} we will consider inference
%
%Importance sampling methods may seem initially to offer a solution to this issue, allowing consistent estimates of the values of arbitrary integrals (including marginal density estimations that may not be directly estimated with standard Monte Carlo approaches) providing we can generate independent sample from a distribution which meets the weak condition of having a density which is non-zero everywhere the integrand is non-zero. The general importance sampling estimator is formed as a ratio of two Monte Carlo estimates \eqref{eq:importance-sampling-mc-estimates}; providing these estimators have finite variance each will show the typical $\frac{1}{N}$ scaling of the estimator variance with the number of samples used. In general however unless the importance distribution used is very closely matched to the target distribution, simple importance sampling methods are also impractical in high-dimensional spaces as the constant factors in the variances of the Monte Carlo estimates can grow exponentially with dimension meaning an infeasibly large number of samples are needed to compute estimates of a useful level of accuracy.
%
%Markov chain Monte Carlo methods offer a more a scalable alternative to approximate inference in complex high-dimensional probabilistic models. \ac{MCMC} methods exploit the intuition that finding a good `local' approximation to the target distribution --- for example within a small region around a poin or varying along only one direction in the target space --- is usually a much more tractable task than finding an approximation which matches the target distribution globally, particularly in high dimensional spaces where concentration of measure will mean the typical set of a distribution is usually concentrated in to small region of the space and even seemingly small mismatches between an approximation and target can lead to very little overlap between the target and approximation typical sets. Markov chain theory shows how we can exploit such local approximations to make perturbative updates to a Markov chain state such that the realisation of the chain converge to generating dependent samples from the target distribution of interest. Although theory can guarantee \ac{MCMC} estimates will eventually converge to the correct values it is usually difficult to assess the rate of that convergence in practical problems making it difficult to diagnose chain convergence or a lack thereof. This can make application of \ac{MCMC} methods more challenging from a user-perspective than simple Monte Carlo methods as some level of expertise is usually needed to diagnose and find solutions to convergence issues.
%
%%Although applicable in a wide range of models, the 
%
%
%%Although applicable in a very wide range of settings, the extra free choices usually introduced by \ac{MCMC} methods can require significant user tuning to get acceptable performance and diagnosing convergence is significantly more challenging than for simple Monte Carlo methods.
%We discussed three general methods for constructing Markov chains which leave a target distribution invariant --- the Metropolis--Hastings method, Gibbs sampling and slice sampling. Each of the constructs offers its own advantages and disadvantages and no one method dominates the others in all aspects. It is also common to combine transition operators of different types, for exampling using different methods to update distinct subset of the variables in a model given the remaining variables, or use multiple updates to the same variables to potentially combine the good properties of the individual operators.
%
%Due in part probably to their ease of implementation, Metropolis--Hast\-ings methods based on simple proposal distribution such as Gaussian random-walk Metropolis methods are very commonly used in practice. Performance of such methods is however usually very dependent on the good choice of the algorithm free parameters such as the proposal width / step size, with poor choices leading to very slow mixing chains. In target distributions with a complex geometry, for example where the appropriate scale for state updates may differ signficiantly across the target space, Metropolis--Hastings methods using simple fixed proposals may be unable to mix well even when optimally tuned. One solution to this issue is to use proposals which exploit more information about the local geometry of the distribution such as the gradient-based Hamiltonian Monte Carlo methods we will discuss in Chapter \ref{ch:hamiltonian-monte-carlo}.
%
%Gibbs sampling methods are also very popular with general purpose implementations such as BUGS \citep{gilks1994language} and JAGS \citep{plummer2003jags} having supported their application to a wide range of models. Although requiring that we are able to decompose the target distribution into complete conditionals we can tractably generate independent samples from, in the models where it can be applied Gibbs sampling offers the advantage over Metropolis--Hastings methods of not requiring choosing and tuning the free parameters of the proposal distribution. As noted previously however the performance of Gibbs sampling methods is very dependent on the parameterisation of the target distribution, with parameterisations with strong dependencies between the variables tending to lead to very slowly mixing chains. Although this can be alleviated by reparameterisation or using block-updates to coupled variables in some cases, the resulting extra implementation and tuning requirements erode the simplicity advantage compared to competing methods.
%
%The slice sampling algorithms introduced at the end of the last section are more complex than the corresponding algorithms for Metropolis--Hastings and Gibbs sampling, and this added implementation complexity may explain in part the seemingly less widespread use of slice samp\-ling methods in practice\footnote{As a very rough metric at the time of writing the original 1953 Metropolis et al. publication \citep{metropolis1953equation} has 35,288 citations recorded on Google Scholar, the 1984 Geman and Geman publication \citep{geman1984stochastic} commonly cited as the original sourcce of the Gibbs sampling algorithm 20,485 citations and the 2003 Neal \citep{neal2003slice} slice sampling publication, 1,444 citations.}, although the relatively much more recent introduction of the algorithm is likely also a factor. The tradeoff achieved for this increased implementation complexity however are algorithms which usually require much less tuning than random-walk Metropolis methods to achieve reasonable performance and are more robust than both Gibbs sampling and random-walk Metropolis methods to target distributions with complex geometries. As noted in \citep{murray2010elliptical} due to the multiple target density evaluations per chain state update in slice sampling methods, often a well-tuned Metropolis--Hastings method will be able to achieve a greater efficiency in terms of effective samples per run time. However the sometimes significant user time required for tuning can often outweigh the gains in efficiency over slice sampling, and even if automatic adaptive tuning methods are used these will still often struggle to cope with distributions with locally varying geometry. One of the contributions of this thesis will be illustrating how slice sampling methods can often be applied to inference problems where Metropolis--Hastings methods are more commonly used with sometimes significant improvements in robustness and efficiency.
%
%[To do]

%The approximate inference methods we have reviewed in this chapter provide

% Monte Carlo VI
%   - ADVI
%   - BBVI
%   - Autoencoding variational Bayes (amortised inference)
% empirical Bayes
% expectation propagation ESS
% variational MCMC
% variational inf and MCMC bridging the gap

