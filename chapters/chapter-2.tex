\chapter{Approximate inference}\label{ch:approximate-inference}

In the previous chapter we argued that the key computational challenge in performing inference in probabilistic models is being able to evaluate integrals with respect to probability measures defined on high-dimensional spaces. Generally these integrals will not have analytic solutions and for models with even moderate numbers of unobserved variables, numerical quadrature approaches to evaluating integrals are computationally infeasible due to the exponential scaling of computation cost with dimension.

%\marginpar{Although this may seem a paradox, all exact science is dominated by the idea of approximation. When a man tells you that he knows the exact truth about anything, you are safe in inferring that he is an inexact man.\\---Bertrand Russell}
%\marginpar{Be approximately right rather than exactly wrong.\\--John W. Tukey}
\marginpar{Truth is much too complicated to allow anything but approximations.\\---John von Neumann}
In this chapter we will review some of the key algorithms proposed for computing \emph{approximate} solutions to inference problems. A unifying aspect to all of these methods is trading off some loss of the accuracy of the answers provided to inferential queries, for a potentially significant increase in computational tractability. The literature on \emph{approximate inference} methods is vast and so necessarily this chapter will only form a partial review of the available methods. We will therefore concentrate on those approaches which are directly relevant to this thesis.

Approximate inference methods can be coarsely divided in to two groups: methods in which the integrals with respect to the target measure are estimated by computing weighted sums over points sampled from a probability measure over the target state space and those in which a more tractable approximation to the target probability measure of interest is found by optimising the approximation to be `close' in some sense to the target measure.  In this chapter we will concentrate sampling-based approaches to approximate inference, focussing in particular on Markov chain Monte Carlo methods, as these form the key basis for the contributions discussed in later chapters.

Although they are not the main focus of this thesis we will make use of several optimisation-based approximate inference methods within the sampling approaches discussed in the following chapters. We review the ideas underlying these methods in Appendix \ref{app:optimisation-based-approximate-inference}. 

\section{Monte Carlo methods}

A key observation in the previous chapter was that inference at both the level of computing conditional expectations of uobserved variables in a model and in evaluating evidence terms to allow model comparison, will for most models correspond to being able to integrate functions against a probability density defined with respect to a base measure\footnote{There are models for which the corresponding probability measure is not absolutely continuous with respect to another measure and so cannot be represented by a density, however we will concentrate for now on the common case were a density exists.}. In particular we wish to be able to compute integrals of the form
\begin{equation}\label{eq:integral-against-density}
  \int_{\set{X}} f(\vct{x}) \, \dr \tgtprob(\vct{x}) =
  \int_{\set{X}} f(\vct{x}) \, \tgtdens(\vct{x}) \,\dr\mu(\vct{x})
\end{equation}
where $\tgtdens$ is the density of a target distribution $\tgtprob$ on a space $\set{X}$ with respect to a base measure $\mu$ and $f$ is a measurable function. For instance in the case of computing the \emph{posterior mean} in a Bayesian inference problem with observed variables $\rvct{y}$ and latent variables $\rvct{x}$ where the posterior density $\pden{\rvct{x}|\rvct{y}}$ is defined with respect to the $D$-dimensional Lebesgue measure, we would have  $\tgtdens(\vct{x}) = \pden{\rvct{x}|\rvct{y}}(\vct{x}\gvn\vct{y})$ for an observed $\vct{y}$, $\mu(\vct{x}) = \lebm{D}(\vct{x})$ and $f(\vct{x}) = \vct{x}$. Often we will only be able to evaluate $\tgtdens$ up to an unknown unnormalising constant i.e. $\tgtdens(\vct{x}) = \frac{1}{Z}\utgtdens(\vct{x})$ with we able to evaluate $\utgtdens$ pointwise but $Z$ intractable to compute. For example in a Bayesian inference setting $\utgtdens(\vct{x})$ would be the joint density $\pden{\rvct{x},\rvct{y}}(\vct{x},\vct{y})$ and $Z$ the model evidence $\pden{\rvct{y}}(\vct{y})$. When peforming inference in undirected models, we would instead have that $\utgtdens$ is the product of unnormalised factors and $Z$ the corresponding normaliser.

The key idea of the methods we will discuss in this section is that we can estimate \eqref{eq:integral-against-density} by generating a set of random samples from a probability distribution defined on $\set{X}$ and then computing a (potentially weighted) average of the value of the function $f$ evaluated at these sample points. The most direct approach is to sample independently from the probability distribution defined by the target density $\tgtdens$. As we will see this is not necessarily feasible to do for complex target densities defined on high dimensional spaces, however a  of related methods for generating and using random samples to approximate integrals with respect to target densities arising from complex probabilistic models have been developed.

\subsection{Monte Carlo integration}\label{subsec:monte-carlo-method}

\marginpar{The eponym of the Monte Carlo method is a Monocan casino, favoured haunt of the uncle of Stanis{\l}aw Ulam, one of the method's inventors.}
The framework that unifies all of the methods we will discuss in this section is the \emph{Monte Carlo} method for integration \citep{ulam1949monte}. We now review the key ideas and properties of Monte Carlo integration. 

Let $\rvct{x}$ be a random (vector) variable distributed according to the target density i.e. $\pden{\rvct{x}} = \td{\prob{\rvct{x}}}{\mu} = \tgtdens$. Given an arbitrary measurable function $f : \set{X} \to \reals$ we define a random variable $\rvar{f} = f(\rvct{x})$. Our task is to compute expectations $\expc{\rvar{f}} = \bar{f}$ corresponding to the integral \eqref{eq:integral-against-density}. We assume that $\expc{\rvar{f}}$ exists and both $\expc{\rvar{f}}$ and $\var{\rvar{f}}$ are finite.

%We wish to be able to compute expectations of arbitrary measurable functions $f$ of the random variable $\rvct{x}$, the composition defining a new random variable $\rvar{f} = f(\rvct{x})$ 
%\begin{equation}
%  \bar{f} = \expc{\rvar{f}} = \int_{\set{X}} f(\vct{x}) \,\tgtdens(\vct{x}) \,\dr\mu(\vct{x}),
%\end{equation}
%with the implicit assumption that this expectation exists and is finite. 

For now we assume we have a way of generating values of $N$ random variables $\lbrace \rvct{x}_n\rbrace_{n=1}^N$, each marginally distributed according to the target density i.e. $\pden{\rvct{x}_n} = \tgtdens ~\forall n \in \fset{1 \dots N}$. We will initially not require any further properties on the joint distribution across all $N$ variables. We define random variables  $\lbrace \rvar{f}_n \rbrace_{n=1}^N$ and $\hatf_N$ by
\begin{equation}
  \rvar{f}_n = f\lpa\rvct{x}_n\rpa \quad\forall n \in \fset{1\dots N}
  \quad\textrm{and}\quad
  \hatf_N = \frac{1}{N} \sum_{n=1}^N \rvar{f}_n.
\end{equation}
Due to linearity of the expectation operator, we have that
\begin{equation}
  \expc{\hatf_N} = 
  \frac{1}{N} \sum_{n=1}^N \expc{\rvar{f}_n} = 
  \frac{1}{N} \sum_{n=1}^N \bar{f} = 
  \bar{f}
\end{equation}
%\marginpar{The variance of a random variable $\rvar{x}$ is defined as $\var{\rvar{x}} =\expc{(\rvar{x} - \expc{\rvar{x}})^2}$.}
and so that in expectation $\hatf_N$ is equal to $\bar{f}$, i.e. realisations of $\hatf_N$ are \emph{unbiased estimators} of $\bar{f}$. Note that this result does not require any independence assumptions about the generated random variables. Now considering the variance of $\hatf_N$ we can show that
%\begin{align}
%  \var{\hatf_N} 
%  =\,& 
%  \expc{\lpa \frac{1}{N} \sum_{n=1}^N \lpa f(\rvct{x}_n) \rpa - \bar{f}\rpa^2}\\
%  =\,&
%  \frac{1}{N^2} \expc{\lpa \sum_{n=1}^N \lpa f(\rvct{x}_n) - \bar{f} \rpa\rpa^2}\\
%  =\,&
%  \frac{1}{N^2}\sum_{n=1}^N \expc{\lpa f(\rvct{x}_n) - \bar{f} \rpa^2} +\,\\
%  &
%  \frac{2}{N^2}\sum_{n=1}^N \sum_{m=1}^{n-1} 
%  \expc{\lpa f(\rvct{x}_n) - \bar{f} \rpa\lpa f(\rvct{x}_m) - \bar{f} \rpa}
%  \\
%  =\,&
%  \frac{\var{f(\rvct{x})}}{N}
%  \lpa 
%    1 +
%    \frac{2}{N}\sum_{n=1}^N \sum_{m=1}^{n-1} 
%      \frac{\expc{\lpa f(\rvct{x}_n) - \bar{f} \rpa\lpa f(\rvct{x}_m) - \bar{f} \rpa}}
%      {\var{f(\rvct{x})}}
%  \rpa
%\end{align}
\begin{align}\label{eq:monte-carlo-variance-general}
  \var{\hatf_N}
  =
  \frac{\var{\rvar{f}}}{N}
  \lpa 
    1 +\frac{2}{N}\sum_{n=1}^{N-1} \sum_{m=1}^{n-1} \frac{\cov{\rvar{f}_n, \rvar{f}_m}}{\var{\rvar{f}}}
  \rpa.
\end{align}
If the generated random variables $\lbrace \rvct{x}_n\rbrace$ and so $\lbrace \rvar{f}_n\rbrace$ are independent, then $\cov{\rvar{f}_n, \rvar{f}_m} = 0 ~\forall m\neq n$. In this case \eqref{eq:monte-carlo-variance-general} reduces to
\begin{equation}\label{eq:monte-carlo-variance-independent}
  \var{\hatf_N}
  =
  \frac{\var{\rvar{f}}}{N},
\end{equation}
i.e. the variance of the \emph{Monte Carlo estimate} $\hatf_N$ for $\bar{f}$ is inversely proportional to the number of generated random samples $N$. Importantly this scaling does not depend on the dimension of $\rvct{x}$. 

Therefore if we can generate a set of independent random variables from the target density, we can estimate expectations that asymptotically tend to the true value as $N$ increases, with a typical deviation from the true value (as measured by the standard deviation, i.e. the square root of variance) that is $\mathcal{O}\lpa N^{-\frac{1}{2}}\rpa$. In comparison a fourth-order quadrature method such as \emph{Simpson's rule} has an error that is $\mathcal{O}\lpa N^{-\frac{4}{D}}\rpa$ for a grid of $N$ points uniformly spaced across a $D$ dimensional space. Asymptotically for $D > 8$, Monte Carlo integration will therefore give better convergence than Simpson's rule, and even for smaller dimensions large constant factors in the Simpson's rule dependence can mean Monte Carlo performs better for practical $N$.

Note that computing Monte Carlo estimates from independent random variables is not optimal in terms of minimising $\var{\hatf_N}$ for a given $f$; the covariance terms in \eqref{eq:monte-carlo-variance-general} can be negative which can reduce the overall variance. A wide range of \emph{variance reduction} methods have been proposed to exploit this and produce lower variance of Monte Carlo estimates for a given $f$ \citep{kroese2011variance}. Although these methods can be very important in pratice for achieving an estimator with a practical variance for a specific $f$ of interest, we will generally concentrate on the case where we do not necessarily know $f$ in advance and so cannot easily exploit these methods. %More generally applicable are \emph{quasi-Monte Carlo} methods \citep{niederreiter1992random,morokoff1995quasi} which use specially constructed \emph{low-discrepancy sequences} to more evenly tile the sample space. The error of quasi-Monte Carlo is upper bounded by $\mathcal{O}(\log(N)^D N^{-1})$ errors which can improve efficiency in some cases. The methods a

\subsection{Pseudo-random number generation}

\marginpar{The generation of random numbers is too important to be left to chance.\\ ---Robert R. Coveyou}
Virtually all statistical computations involving random numbers in practice make use of \acp{PRNG}. Rather than generating samples from truly random processes\footnote{We consider a true random process as one in which it is impossible to precisely predict the next value in the sequence given the previous values.}, \acp{PRNG} produce deterministic\footnote{The sequences are determnistic in the sense that if the generator internal state is known all values in the sequence can be reconstructed exactly.} sequences of integers in a fixed range that nonetheless maintain many of the properties of a random sequence. 

\begin{figure}[!t]
\centering
\tikzsetnextfilename{linear-congruential-generator-sequence}
\pgfplotstableread{data/lcg-37-61-128-37.cvs}{\lcgfile}
\drawgrid[zero color=black!80, one color=Maroon, cell ht=0.22em, cell wd=0.22em]{\lcgfile}
\caption[Example linear congruential generator sequence.]{Binary representation of linear congruential generator sequence $s_{n+1} = 37s_n + 61 \kern-4pt\mod 128$. Columns left to right represents successive integer states in sequence. From least significant (bottom) to most significant (top), the bits in each column have patterns repeating with periods 2, 4, 8, 16, 32, 64, 128.}
\label{fig:example-lcg-sequence}
\end{figure}

In particular through careful choice of the updates, sequences with a very long period (number of iterations before the sequence begins repeating), a uniform distribution across the numbers in the sequence range and low correlation between successive states can be constructed. A very simple example of a class of \acp{PRNG} is the \emph{linear congruential generator} \citep{lehmer1951mathematical} which obeys the recurrent update
\begin{equation}\label{eq:lcg-update}
  s_{n+1} = (a s_n + c) \kern-4pt\mod m
  \quad \textrm{with} \quad
   0 < a < m,~ 0 \leq c < m,
\end{equation}
with $a$, $c$ and $m$ integer parameters. If $a$, $c$ and $m$ are chosen appropriately, iterating the update \eqref{eq:lcg-update} from an initial seed $0 \leq s_0 < m$, will produce a sequence of states which visits all the integers in $[0, m)$ before repeating. An example state sequence with $m=128$ is shown in Figure \ref{fig:example-lcg-sequence}. In practice, linear congruential generators produce sequences with poor statistical properties, particularly when used to generate random points in high dimensional spaces \citep{marsaglia1968random}, hence most modern numerical computing libraries use more robust variants such as the \emph{Mersenne-Twister} \citep{matsumoto1998mersenne}, which is used in all experiments in this thesis.

The raw output of a \ac{PRNG} is an integer sequence, with typically the sequence elements uniformly distributed over all integers in a range $[0, 2^n)$ for some $n \in \naturals$. All real values are represented at a finite precision on computers, typically using a floating point representation \citep{ieee2008standard} of \emph{single} (24-bit mantissa) or \emph{double} (53-bit mantissa) precision. Through an approriate linear transformation, the integer outputs of a \ac{PRNG} can be converted to floating-point values uniformly distributed across a finite interval. \ac{PRNG} implementations typically provide a primitive to generate floating-point values uniformly distributed on $[0, 1)$.

Given the ability to generate sequences of (effectively) independent samples from a uniform distribution $\mathcal{U}(0,1)$, the question is then how to use these values to produce random samples from arbitary densities. This will be the subject of the following sub-sections.

\subsection{Transform sampling}

Samples from many standard distributions can be generated by directly exploiting the transform of random variables relationships discussed in \ref{subsec:change-of-variables}. Let $\rvct{u}$ be a $D$-dimensional vector of independent random variables marginally distributed according to $\mathcal{U}(0,1)$ and $\vct{g} :  [0,1)^D \to \set{X}$ be a bijective map to a vector space $\set{X} \subseteq \reals^D$. If we define $\rvct{x} = \vct{g}(\rvct{u})$ and $\vct{h} = \vct{g}^{-1}$, then by applying \eqref{eq:change-of-variables-vector-bijective} we have that
\begin{equation}\label{eq:transform-sampling-uniform}
  \pden{\rvct{x}}(\vct{x}) = \left|\pd{\vct{h}(\vct{x})}{\vct{x}}\right|.
\end{equation}
\begin{figure}[!t]
\def\numgrid{11}
\resizebox{\textwidth}{!}{
\includetikz{box-muller-transform}
}
\caption[Visualisation of Box--Muller transform.]{Visualisation of Box--Muller transform. Left axis shows uniform grid on $\set{U} = [0,1]^2$ and right-axis shows grid points after mapping through $\vct{g}$ in transformed space $\set{X} = \reals^2$.}
\label{fig:box-muller-transform}
\end{figure}
For example for $D=2$, $\set{X} = \reals^2$ and a bijective map $\vct{g}$ defined by
\begin{equation}\label{eq:box-muller-transform}
\begin{split}
  \vct{g}\lpa\begin{array}{c} u_1\\u_2\end{array}\rpa &=
  \lsb\begin{array}{c} \sqrt{-2\log u_1}\cos(2\uppi u_1) \\ \sqrt{-2\log u_1}\sin(2\uppi u_2)\end{array}\rsb,
  \\
  \vct{h}\lpa\begin{array}{c} x_1\\x_2\end{array}\rpa &=
  \lsb\begin{array}{c} \exp\lpa-\frac{1}{2}(x_1^2+x_2^2)\rpa \\ \frac{1}{2\uppi}\arctan\lpa\frac{x_1}{x_2}\rpa\end{array}\rsb,
\end{split}
\end{equation}
then we have that the density of the transformed $\rvct{x} = \vct{g}(\rvct{u})$ is
\begin{equation}\label{eq:box-muller-transform-density}
  \pden{\rvct{x}}(\vct{x}) = 
  \frac{1}{\sqrt{2\uppi}}\exp\lpa-\frac{x_1^2}{2}\rpa
  \frac{1}{\sqrt{2\uppi}}\exp\lpa-\frac{x_2^2}{2}\rpa ,
\end{equation}
i.e. $\rvar{x}_1$ and $\rvar{x}_2$ are independent random variables with standard normal densities $\nrm{0,1}$. This is the \emph{Box--Muller transform} \citep{box1958note}, and allows generation of independent standard normal variables given a \ac{PRNG} primitive for sampling from $\mathcal{U}(0,1)$. A visualisation of the transformation of space applied by the method is shown in Figure \ref{fig:box-muller-transform}. 

%Due to the relatively high cost of the trigonometric function evaluations, more efficient alternatives to Box--Muller are usually used in practice to generate normal random variables such as a rejection sampling (see next sub-section) based variant \citep{marsaglia1968random} or the \emph{Ziggurat algorithm} \citep{marsaglia2000ziggurat}. % (which also generalises to other symmetric univariate distributions).

A general method for sampling from univarite distributions is to use an inverse \ac{CDF} transform. For a probability density $\tgtdens$ on a scalar random variable, the corresponding \ac{CDF} $r : \reals \to [0,1]$ is defined as
\begin{equation}
  r(x) = \int_{-\infty}^x \tgtdens(v) \,\dr v
  \implies
  \pd{r(x)}{x} = \tgtdens(x).
\end{equation}
If $\rvar{u}$ is a standard uniform random variable and $\rvar{x} = r^{-1}(\rvar{u})$ then
\begin{equation}
  \pden{\rvar{x}}(x) = \left|\pd{r(x)}{x}\right| = \tgtdens(x).
\end{equation}
To be able to use the inverse \ac{CDF} transform method we need to be able to evaluate $r^{-1}$, sometimes termed the \emph{quantile function}. Often neither the \ac{CDF} or quantile function of a univariate distribution will have closed form solutions however we can use standard polynomial approximation methods and iterative solvers to evaluate both to arbitary precision \citep{olver2013fast}. For some distributions such as the standard normal $\nrm{0,1}$ even though the \ac{CDF} and quantile function do not have an analytic form in terms of elementary functions it is common for numerical computing libraries to provide numerical approximations to both which are accurate to within small multiples of machine precision. %In densities where a standard library function for the \ac{CDF} is not available, Chebyshev polynomial approximations to the density can be used to efficient compute an approximation to the \ac{CDF} and an iterative solver used for the inversion \citep{olver2013fast}.
%numerical quadrature and iterative solving algorithms can be used to evaluate the inverse \ac{CDF} for arbitrary densities. The resulting high computational cost can be amortized by for example precomputing the inverse CDF over a fine grid and then using a spline interpolation to evaluate at arbitrary points.
Although the inverse \ac{CDF} transform method gives a general recipe for sampling from univariate densities, it is not easy to generalise to multivariate densities and even for univariate densities, alternatives can be simpler to implement and in some cases more numerically stable.

\subsection{Rejection sampling}

\begin{figure}[t]
\centering
\includetikz{rejection-sampling}
\caption[Visualisation of rejection sampling.]{Visualisation of rejection sampling. The green curve shows the (unnormalised) target density $\utgtdens$, the green region underneath representing the area we wish to sample points uniformly from. The dashed orange curve shows the scaled proposal density $M q$, with the orange (plus green) region representing the area we uniformly propose values from. Two example proposals are shown: $\diamond$ is under the target density and so accepted; $*$ is outside of the green region and so would be rejected.}
\label{fig:rejection-sampling}
\end{figure}

An important class of generic sampling methods, particularly due their use as a building block in other algorithms, is rejection sampling \citep{vonneumann1951various}. Rejection sampling uses the observation that to sample from a probablity density $\tgtdens : \set{X} \to [0, \infty]$ it is sufficient to uniformly sample from the volume under the graph of $\lpa\vct{x}, \tgtdens(\vct{x})\rpa$.

The key requirement in rejection sampling is to identify a \emph{proposal distribution} $Q$ which we can generate independent samples from and has a density $q=\pd{Q}{\lebm{}}$ that upper bounds the potentially unnormalised target density $\utgtdens$ across its full support $\set{X}$ when multiplied by a known constant $M$, i.e. $\utgtdens(x) \leq M q(x) ~\forall x \in \set{X}$. The requirement to be able to generate independent samples from $Q$ can be met for example by distributions amenable to transform sampling, e.g. the standard normal. The second requirement is generally more challenging and as we will see the efficiency of rejection sampling methods is very dependent on how tight the bound can be made.

\begin{algorithm}[!t]
\caption{Rejection sampling.}
\label{alg:rejection-sampling}
\input{algorithms/rejection-sampling}
\end{algorithm}

Algorithm \ref{alg:rejection-sampling} describes the rejection sampling method to produce a single independent sample from a target distributoin. A visualisation of how the algorithm works for a univariate target distribution is shown in Figure \ref{fig:rejection-sampling}. The overall aim is to generate points uniformly distributed across the green area under the (unnormalised) target density curve. This is achieved by generating points uniformly under the dashed orange curve corresponding to the scaled proposal density and then accepting only those which are below the green curve. To generate a point under the dashed orange curve we first generate an $x$ from the proposal distribution and then generate an auxilliary `height' variable by sampling uniformly from $[0, Mq(x)]$. If the sampled height is below the green curve we accept (as in the $\diamond$ example in Figure \ref{fig:rejection-sampling}) else we reject the sample (as in the $*$ example in Figure \ref{fig:rejection-sampling}).

\begin{figure}[t]
\centering
\includetikz{rejection-sampling-factor-graph}
\caption{Factor graph of rejection sampling process.}
\label{fig:rejection-sampling-factor-graph}
\end{figure}

Figure \ref{fig:rejection-sampling-factor-graph} shows the rejection sampling generative process as a directed factor graph, with $\rvct{x}$ be a random variable representing the proposal, $\rvar{u}$ the uniform auxiliary variable drawn to sample the `height' and $\rvar{a}$ a binary variable that indicates whether the proposal is accepted ($\rvar{a} = 1$) or not ($\rvar{a} = 0$). By marginalising out $\rvar{u}$ we have that that
\begin{equation}\label{eq:rejection-sampling-joint-prob}
  \pden{\rvct{x},\rvar{a}}(\vct{x},a) = 
  q(\vct{x}) \lpa\frac{\utgtdens(\vct{x})}{Mq(\vct{x})}\rpa^a
  \lpa1 - \frac{\utgtdens(\vct{x})}{Mq(\vct{x})}\rpa^{1-a},
\end{equation}
and further marginalising over the proposal $\rvct{x}$
\begin{equation}\label{eq:rejection-sampling-accept-prob}
  \pden{\rvar{a}}(a) = \lpa\frac{Z}{M}\rpa^a \lpa 1 - \frac{Z}{M}\rpa^{1-a}.
\end{equation}
Conditioning on the proposal being accepted we therefore have that
\begin{equation}
  \pden{\rvct{x}|\rvar{a}}(\vct{x}\gvn 1) =
  \frac{q(\vct{x})\frac{\utgtdens(\vct{x})}{Mq(\vct{x})}}{\frac{Z}{M}} = \frac{\utgtdens(\vct{x})}{Z} = \tgtdens(\vct{x}).
\end{equation}
Therefore the accepted proposals are distributed according to the target density as required. Further from \eqref{eq:rejection-sampling-accept-prob} we have that the $\pden{\rvar{a}}(1) = \frac{Z}{M}$. This suggests we can use the accept rate to estimate $Z$ but also hints at the difficulty in finding a $M$ which guarantees the upper bound requirement as for $\frac{Z}{M}$ to be a valid probability $M \geq Z$ i.e. $M$ needs to be an upper bound on the unknown normalising constant $Z$. This relationship also suggests it is desirable to set $M$ as small as possible to maximise the acceptance rate. %; for a fixed proposal density $q$ this will involve setting $M$ to a value such that $M q(\vct{x}) = \utgtdens(\vct{x})$ for at least one $\vct{x}$ (as for example in Figure \ref{fig:rejection-sampling}).

%For univariate target densities which are log-concave, \emph{adaptive rejection sampling} \citep{gilks1992adaptive} offers an efficient adaptive method 
Although rejection sampling can be an efficient method of sampling from univariate target distributions (particularly for distributions with log-concave densities where an adaptive variant is available \citep{gilks1992adaptive}), it generally scales very poorly with the dimensionality of the target distribution. This is as the ratio of the volume under the target density to the volume under the scaled proposal density (in terms of Figure \ref{fig:rejection-sampling} the ratio of the green area to the green plus orange regions), and so the probability of accepting a proposal, will tend become exponentially smaller as the dimensionality increases. This is an example of the so-called \emph{curse of dimensionality}. Therefore although rejection sampling can be a useful subroutine for generating random variables from low-dimensional densities, in general it is not a viable option for generating samples directly for high-dimensional Monte Carlo integration.

\subsection{Importance sampling}

So far we have considered methods for generating samples directly from a target distribution. Although samples can be of value in themselves for giving a representative set of plausible values from the target distribution (e.g. for visualisation purposes), usually the end goal is to estimate integrals of the form in \eqref{eq:integral-against-density}. 

\emph{Importance sampling} \citep{kahn1951estimation} is a Monte Carlo method which allows arbitrary integrals to be estimated. If $Q$ is a distribution, with density $q = \pd{Q}{\mu}$, which is absolutely continuous with respect to the target distribution (which requires that $\tgtdens(\vct{x}) > 0 \Rightarrow q(\vct{x}) > 0$), then importance sampling is based on the identity
\begin{equation}\label{eq:importance-sampling-integral}
  \bar{f} =
  \frac
  {\int_{\set{X}} f(\vct{x}) \,\utgtdens(\vct{x}) \,\dr\mu(\vct{x})}
  {\int_{\set{X}} \utgtdens(\vct{x}) \,\dr\mu(\vct{x})}
  =
  \frac
  {\int_{\set{X}} f(\vct{x}) \,\frac{\utgtdens(\vct{x})}{q(\vct{x})} \, q(\vct{x}) \,\dr\mu(\vct{x})}
  {\int_{\set{X}} \frac{\utgtdens(\vct{x})}{q(\vct{x})} \, q(\vct{x}) \,\dr\mu(\vct{x})}.
\end{equation}
Each of the numerator and denominator in \eqref{eq:importance-sampling-integral} take the form of an expectation of a measurable function of a random variable $\rvct{x}$ with distribution $Q$. Further the denominator is exactly equal to $Z = \int_{\set{X}}\utgtdens(\vct{x})\,\dr\mu(\vct{x})$. We therefore have that
\begin{equation}\label{eq:importance-sampling-expectations}
  Z \bar{f} = \expc{w(\rvct{x}) f(\rvct{x})}
  ~~\textrm{and}~~
  Z = \expc{w(\rvct{x})}
  ~~\textrm{with}~~
  w(\vct{x}) = \frac{\utgtdens(\vct{x})}{q(\vct{x})}.
\end{equation}
If we can generate random variables $\lbrace \rvct{x}_n \rbrace_{n=1}^N$ each with marginal density $q$ we can therefore form Monte Carlo estimates of both the numerator and denominator. We define $\hat{\rvar{Z}}_N$ and $\hat{\rvar{g}}_N$ as
\begin{equation}\label{eq:importance-sampling-mc-estimates}
  \hat{\rvar{Z}}_N = \frac{1}{N} \sum_{n=1}^N w\lpa\rvct{x}_n\rpa
  ~~\textrm{and}~~
  \hat{\rvar{g}}_N = \frac{1}{\hat{\rvar{Z}}} \sum_{n=1}^N w\lpa\rvct{x}_n\rpa f\lpa\rvct{x}_n\rpa.
\end{equation}
By the same argument as Section \ref{subsec:monte-carlo-method}, $\expc{\hat{\rvar{Z}}_N} = Z$ and $\expc{\hat{\rvar{g}}_N} = Z\bar{f}$. We can therefore use importance sampling to form an unbiased estimate of the unknown normalising constant $Z$. 

If we define $\hatf_N = \hat{\rvar{g}}_N / \hat{\rvar{Z}}_N$, then this is a biased\footnote{In cases where the normalising constant $Z$ is known, we can instead use $w(\vct{x}) = \frac{\tgtdens(\vct{x})}{q(\vct{x})}$ in which case the ratio estimator is not required and an unbiased estimates can be calculated. As the problems we are interested in will generally have unknown $Z$ we do not consider this case further} estimator for $\bar{f}$ as in general the expectation of the ratio of two random variables is not equal to the ratios of their expectations. However if both the numerator and denominator have finite variance, i.e. $\var{\hat{\rvar{Z}}_N} < \infty$ and $\var{\hat{\rvar{g}}_N} < \infty$, then $\hatf_N$ is a \emph{consistent} estimator for $\bar{f}$ i.e. $\lim_{N\to\infty} \expc{\hatf_N} = \bar{f}$.

\begin{figure}[t]
\centering
\pgfplotsset{cycle list/Dark2-3}
\begin{subfigure}[b]{.48\linewidth}
\centering
\includetikz{importance-sampling-1}
\end{subfigure}
\begin{subfigure}[b]{.48\linewidth}
\centering
\includetikz{importance-sampling-2}
\end{subfigure}
\caption[Visualisation of importance sampling.]{Visualisation of importance sampling. On both axes the green curve shows the unnormalised target density $\utgtdens$, the dashed orange curve the density $q$ values are sampled from and the dotted violet curve the importance weighting function $w(x) = \frac{\utgtdens(x)}{q(x)}$ to estimate expectations with respect to the target density using samples from $q$. In the left axis the $q$ chosen is undispersed compared to $\utgtdens$ leading to very large $w$ values in the right tail. In constrast in the right axis, the broader $q$ leads to less extreme variation in $w$.}
\label{fig:importance-sampling}
\end{figure}

The $w(\rvar{x}_n)$ values are typically termed the \emph{importance weights}. If a few of the weights are very large, the weighted sums in \eqref{eq:importance-sampling-mc-estimates} will be dominated by those few values, reducing the effective number of samples in the Monte Carlo estimates. This can particularly be a problem if the are regions of $\set{X}$ with low probability under $q$ where $\tgtdens(\vct{x}) \gg q(\vct{x})$. As sampling points in these regions will be a rare event, a large number of samples may be needed to diagnose the issue adding further difficulty. A general recommendation is to use densities $q$ with tails as least as heavy of those of $\tgtdens$, and in general the closer the match between $q$ and $\tgtdens$ the better \citep{mackay2003information,owen2013importance}. Figure \ref{fig:importance-sampling} shows a visualisation of the effect of the choice of $q$ on the importance weights.

%A heuristic that can be used to assess the quality of importance sampling estimates is the (importance sampling) \emph{effective sample size} \citep{kong1992note,owen2013importance}. It approximately quantifies how many independent samples from the target $\tgtdens$ would be required to get a Monte Carlo estimate with a similar variance to that achieved using an importance sampling estimator with weights $\lbrace w(\rvct{x}_n) \rbrace_{n=1}^N$ given \ac{iid} $\lbrace \rvct{x}_n\rbrace_{n=1}^N$ generated from $q$. It can be calculated as
%\begin{equation}\label{eq:is-effective-sample-size}
%  \rvar{N}_{\textrm{eff}} = 
%  \lpa \sum_{n=1}^N \bar{\rvar{w}}_n^2 \rpa^{-1}
%  \quad\textrm{where}\quad \bar{\rvar{w}}_n  = \frac{w\lpa\rvct{x}_n\rpa}{\sum_{m=1}^N w\lpa\rvct{x}_m\rpa},
%\end{equation}
%i.e. as the reciprocal of the sum of squares of the normalised importance weights. If $\rvar{N}_{\textrm{eff}} \ll N$ this can suggest an issue with the choice of sampling density $q$. The diagnostic is not fool proof however as it is based on a finite sample size, and it may be that rare extreme importance weights due to e.g. a mode of the target $\tgtdens$ in the tails of $q$, are not encountered in a particular run giving a misleadingly high $\rvar{N}_{\textrm{eff}}$.

When previously discussing rejection sampling, we introduced an auxiliary binary accept indicator variable, $\rvar{a}$, associated with each proposed sample $\rvct{x}$ (see Figure \ref{fig:rejection-sampling-factor-graph}). If we generate $N$ independent proposal -- indicator pairs $\lbrace \rvct{x}_n, \rvar{a}_n \rbrace_{n=1}^N$ then the number of accepted proposals is $\rvar{N}_{\textrm{acc}} = \sum_{n=1}^N \rvar{a}_n$. Conditioned on $\rvar{N}_{\textrm{acc}}$ being a value more than one, the generated rejection sampling variables $\lbrace \rvct{x}_n, \rvar{a}_n \rbrace_{n=1}^N$ can be used to form an \emph{unbiased} Monte Carlo estimate of $\bar{f}$ using the estimator
\begin{equation}\label{eq:rejection-sampler-mc-estimator}
  \hatf_N^{\,\textsc{rs}} = \frac{\sum_{n=1}^N\rvar{a}_n f\lpa\rvct{x}_n\rpa}{\sum_{m=1}^N \rvar{a}_m},
\end{equation}
which just correponds to computing the empirical mean of the accepted proposals i.e. the standard Monte Carlo estimator. In comparison importance sampling forms a biased but consistent estimator for $\bar{f}$ from $N$ samples $\lbrace \rvct{x}_n \rbrace_{n=1}^N$ from a density $q$ using the estimator
\begin{equation}\label{eq:rejection-sampler-mc-estimator}
  \hatf_N^{\,\textsc{is}} = \frac{\sum_{n=1}^N w\lpa\rvct{x}_n\rpa f\lpa\rvct{x}_n\rpa}{\sum_{m=1}^N w\lpa\rvct{x}_m\rpa}.
\end{equation}
From this perspective the accept indicators $\rvar{a}_n$ in rejection sampling can be seen to act like binary importance weights, in contrast importance sampling using `soft' weights which mean all sampled $\rvct{x}_n$ make a contribution to the estimator (assuming $w(\vct{x}) \neq 0 ~\forall \vct{x} \in \set{X}$). However this correspondence is only loose. The rejection sampling estimator $\hatf_N^{\,\textsc{rs}}$ is unbiased unlike $\hatf_N^{\,\textsc{is}}$, but this unbiasedness relies on conditioning on a non-zero value for $\rvar{N}_{\textrm{acc}}$ (i.e. the number of accepted samples to generate) and continuing to propose points until this condition is met. In contrast importance sampling generates a fixed number of samples from $q$ and does not use any auxiliary random variables.

Unlike rejection sampling, there is no need in importance sampling for $q$ to upper-bound the target density (when multiplied by a constant). This allows more freedom in the choice of $q$ however it is still important to choose $q$ to be as close as possible to the target while remaining tractable to generate samples from. In general for target densities defined on high-dimensional spaces, it can be difficult to find an appropriate $q$ such that the variation in importance weights is not too extreme \citep{mackay2003information}. % As we will see later however the importance sampling framework can be combined with other methods we will discuss in the following sections to allow it to be scaled to high dimensional problems.

%The estimator $\hat{f}$ is however the ratio of two Monte Carlo estimates. It is therefore not an unbiased estimator for $\bar{f}$, however it is consistent, i.e. in the limit of $N \to \infty$ it converges to $\bar{f}$, providing both $\hat{Z}$ and $\hat{N}$ have finite variance.

\section{Markov chain Monte Carlo}\label{subsec:markov-chain-monte-carlo}

\begin{figure}[t]
\centering
\includetikz{markov-chain-factor-graph}
\caption[Markov chain factor graph.]{Markov chain factor graph. The initial state $\rvct{x}_0$ is sampled according to a density $q$ and each subsequent state $\rvct{x}_n$ is then generated from a transition density $\fwdtrans_n$ conditioned on the previous state $\rvct{x}_{n-1}$.}
\label{fig:markov-chain-factor-graph}
\end{figure}

The sampling methods considered in the previous section used independent random variables to form Monte Carlo estimates. When introducing the Monte Carlo method we saw that is was not necessary for the random variables used in a Monte Carlo estimator to be independent. While it can be impractically computationally expensive to generate independent samples from complex high-dimensional target distributions, simulating a stochastic process which converges in distribution to the target and produces a sequence of \emph{dependent} random variables is often a more tractable task. This is the idea exploited by \ac{MCMC} methods.

A \emph{Markov chain} is an ordered sequence of random variables $\lbrace\rvct{x}_n\rbrace_{n=0}^N$ which have the \emph{Markov property} --- for all $n \in \fset{1 \dots N}$, $\rvct{x}_n$ is conditionally independent of $\lbrace \rvct{x}_n \rbrace_{m < n -1}$ given $\rvct{x}_{n-1}$. This conditional independence structure is visualised as a factor graph in Figure \ref{fig:markov-chain-factor-graph}.

For a Markov chain defined on a general measurable state space $(\set{X}, \sset{F})$, the probability distribution of a state $\rvct{x}_n$ given the state $\rvct{x}_{n-1}$ is specified for each $n \in \fset{1\dots N}$ by a \emph{transition operator}, $\fwdtransop_n : \sset{F} \times \set{X} \to [0, 1]$. In particular the transition operators define a series of regular conditional distributions for each $n \in \fset{1 \dots N}$
\begin{equation}\label{eq:markov-kernel-definition}
  \prob{\rvct{x}_n|\rvct{x}_{n-1}}(\set{A} \gvn \vct{x}) =
  \fwdtransop_n\lpa\set{A} \gvn \vct{x}\rpa
  \quad 
  \forall \set{A} \in \sset{F},~
  \vct{x} \in \set{X}.
\end{equation}
We will often assume that the chain is \emph{homogeneous}, i.e. that the same transition operator is used for all steps $\fwdtransop_n = \fwdtransop ~\forall n\in\fset{1 \dots N}$. 

%A Markov chain is generated using a \emph{Markov kernel} or \emph{transition operator}, which is a regular conditional probability $\prob{\rvct{x}^{(n+1)}}(\set{A} \gvn \rvct{x}_n)$, which defines the probability of the state $\rvct{x}^{(n+1)}$ being in $\set{A}$ conditioned on the state $\rvct{x}_n$. 
%A Markov chain is generated using a \emph{transition density}\footnote{More generally we can define a \emph{Markov kernel} as a regular conditional probability $\prob{\rvct{x}^{(n+1)}}(\set{A} \gvn \rvct{x}_n)$, which defines the probability of the state $\rvct{x}^{(n+1)}$ being in $\set{A}$ conditioned on the state $\rvct{x}_n$. The transition density corresponding to this Markov kernel may not be well defined, for example in the common case where the Markov kernel includes a singular measure term corresponding to remaining at the current state. However as we did previously when discussing deterministic factors, in the interests of readability we will informally treat the Dirac delta as defining the density of a singular measure, and refer to a transition density even when this is not strictly defined.} which defines the probability density of the next state given the current state. We will use the following shorthand notation for transition densities 
%\begin{equation}\label{eq:time-dependent-transition}
%  \fwdtrans_n\lpa \vct{x}' \gvn \vct{x}\rpa = \pden{\rvct{x}_n|\rvct{x}_{n-1}}(\vct{x}'\gvn%\vct{x}).
%\end{equation}
%If the transition density is the same for all steps, which we will assume in most cases, the Markov %chain is said to be \emph{homogeneous} and we will drop the subscript on the transition density.

The key property required of a transition operator for use in \ac{MCMC} methods is that the target distribution $\tgtprob$ is \emph{invariant} under the transition, that is it satisfies
\begin{equation}
  \label{eq:invariant-distribution}
  \tgtprob(\set{A}) = \int_{\set{X}} \fwdtransop(\set{A} \gvn \vct{x}) \,\dr\tgtprob(\vct{x})
  \quad\forall \set{A} \in \sset{F},  
\end{equation}
The invariance property means that if a chain state $\rvct{x}_n$ is distributed according to the target $\tgtprob$, all subsequent chain states $\rvct{x}_{n+1},\,\rvct{x}_{n+2}\dots$ will also be marginally distributed according to the target. Therefore given a single random sample $\rvct{x}_{0}$ from the target distribution, a series of dependent states marginally distributed according to the target could be generated and used to form Monte Carlo estimates of expectations.

Being able to generate even one exact sample from a complex high-dimensional target distribution is generally infeasible. Importantly however the marginal distribution on the chain state $\prob{\rvct{x}_n}$ of a Markov chain with a transition operator which leaves the target distribution invariant will converge to the target distribution irrespective of the distribution of the intial chain state if the target distribution is the \emph{unique} invariant distribution of the chain.%, in which case it is said to be the chain's \emph{stationary distribution}.

To have a unique invariant distribution, a chain must be \emph{irreducible} and \emph{aperiodic} \citep{tierney1994markov}. For a chain on a measurable space $(\set{X},\sset{F})$, irreducibility is defined with respect to a measure $\nu$, which could but does not necessarily need to be the target distribution $\tgtprob$. A chain is $\nu$-irreducible if starting at any point in $\set{X}$ there is a non-zero probability of moving to any set with positive $\nu$-measure in a finite number of steps, i.e.
\begin{equation}\label{eq:irreducibility-criteria}
\begin{split}
  \forall \vct{x} \in \set{X},\, \set{A} \in \sset{F} : \nu(\set{A}) > 0
  ~~\exists\, m \in \integers^+ :
  \prob{\rvct{x}_m|\rvct{x}_0}(\set{A} \gvn \vct{x}) > 0. 
\end{split}
\end{equation}  
A chain with invariant distribution $\tgtprob$ is periodic (and aperiodic otherwise) if disjoint regions of the state space are visited cyclically, i.e. there exists an integer $r > 1$ and an ordered set of $r$ disjoint $\tgtprob$-positive subsets of $\set{X}$, $\lbrace \set{A}_i \rbrace_{i=1}^r$ such that $\fwdtransop(\set{A}_{j} \gvn \vct{x}) = 1 ~\forall \vct{x} \in \set{A}_i,~ i \in \fset{1 \dots r}, ~j = (i + 1) \kern-2pt\mod r$.

If we can construct a $\nu$-irreducible and aperiodic Markov chain $\lbrace \rvct{x}_n \rbrace_{n=0}^N$ which has the target distribution $\tgtprob$ as its invariant distribution, then a \ac{MCMC} estimator $\hatf_N = \frac{1}{N}\sum_{n=1}^N f(\rvct{x}_n)$ converges almost surely as $N \to \infty$ to $\bar{f} = \int_{\set{X}} f \dr\tgtprob$ for all starting states except for a $\nu$-null set\footnote{The `except for a $\nu$-null set' caveat can be removed by requiring the stronger property of \emph{Harris recurrence} \citep{harris1956existence}.}% It has been argued \cite{chan1994discussion} that the floating-point implementations used in practice for \ac{MCMC} methods naturally avoid such `measure-theoretic pathologies' though see \citep{roberts2006harris} for a counter argument.}% The \ac{MCMC} implementations we will use in practice will generally ensure Harris recurrence.}%, and even if not the pathology of a chain starting in the `bad' $P$-null set should be easy to identify.}
 \citep{meyn1993markov}. This convergence of \emph{time-averages} (i.e. over states at different steps of the Markov chain) to \emph{space-averages} (i.e. with respect to the stationary distribution across the state space), is termed \emph{ergodicity} and is a consequence of the \emph{Birkhoff--Khinchin ergodic theorem} \citep{birkhoff1931proof}.

Although irreducibility and aperiodicity of a Markov chain which leaves the target distribution invariant are sufficient for convergence of \ac{MCMC} estimators, this does not tell us anything about the rate of that convergence and so how to quantify the error introduced by computing estimates with a Markov chain simulated for only a finite number of steps. Stronger notions of ergodicity can be used to help quantify convergence; we will concentrate on \emph{geometric ergodicity} here. We first define a notion of distance between two measures $\mu$ and $\nu$ on a measurable space $(\set{X},\sset{F})$, the \emph{total variation distance}, as
\begin{equation}\label{eq:total-variation-measures}
  \left\Vert \mu - \nu \right\Vert_{\textsc{tv}} = \sup_{\set{A} \in \sset{F}} \left| \mu(\set{A}) - \nu(\set{A}) \right|.
\end{equation}
For a $\nu$-irreducible and aperiodic chain with invariant distribution $P$ our earlier statement that the distribution on the chain state converges to $P$ can now be restated more precisely as that for $\nu$-almost all initial states $\rvct{x}_0 = \vct{x}$, $\lim_{n\to\infty} \left\Vert \prob{\rvct{x}_n|\rvct{x}_0}\lpa \cdot \gvn \vct{x}\rpa - \tgtprob \right\Vert_{\textsc{tv}} = 0$. Geometric ergodicity makes a stronger statement that the convergence in total variation distance conditioned is geometric in $n$, i.e. that
\vspace{-2mm}
\begin{equation}\label{eq:geometric-ergodicity}
  \left\Vert \prob{\rvct{x}_n|\rvct{x}_0}\lpa \cdot \gvn \vct{x}\rpa - \tgtprob \right\Vert_{\textsc{tv}} \leq M(\vct{x}) r^n
\end{equation}
for a positive measurable function $M$ which depends on the initial chain state $\vct{x}$ and rate constant $r \in [0, 1)$. For chains which are geometrically ergodic, we can derive an expression for the \emph{asymptoptic variance} of an \ac{MCMC} estimator $\hatf_N$ related to the variance of a simple Monte Carlo estimator previously considered in Section \ref{subsec:monte-carlo-method}.

\marginpar{A stochastic process is stationary if the joint distribution of the states at any set of time points does not change if all those times are shifted by a constant.}
%\marginpar{A Markov process $\lbrace \rvct{x}_t \rbrace_{t \in \set{T}}$ is stationary if $\prob{\rvct{x}_0} = \prob{\rvct{x}_t} ~\forall t \in \set{T}$ and $\prob{\rvct{x}_0,\rvct{x}_t} = \prob{\rvct{x}_{s},\rvct{x}_{s+t}} ~\forall s, t, s+t \in \set{T}$.}
As in Section \ref{subsec:monte-carlo-method} we define $\rvar{f}_n = f(\rvct{x}_n)$ and $\hatf_N = \frac{1}{N} \sum_{n=1}^N \rvar{f}_n$, with here the $\lbrace \rvct{x}_n \rbrace_{n=1}^N$ the states of a Markov chain. For a homogeneous Markov chain with a unique invariant distribution $P$ which is \emph{stationary}, the marginal density on the states $\prob{\rvct{x}_n}$ is equal to $P$ for all $n$ and we can use the expression for the variance of a general Monte Carlo estimator (which did not assume independence of the random variables) stated earlier in \eqref{eq:monte-carlo-variance-general}. Further the stationarity of the chain means that the covariance $\cov{\rvar{f}_n, \rvar{f}_m}$ depends only on the difference $n - m$, and so the variance of the estimator simplifies to
\begin{equation}\label{eq:monte-carlo-variance-stationary}
  \var{\hatf_N}
  =
  \frac{\var{\rvar{f}}}{N}
  \lpa 
    1 + 2\sum_{n=1}^{N-1} \lpa \frac{N-n}{N} \frac{\cov{\rvar{f}_0, \rvar{f}_n}}{\var{\rvar{f}}}\rpa
  \rpa.
\end{equation}
If we multiply both sides of \eqref{eq:monte-carlo-variance-stationary} by $N$ and define $\rho_{n} = \frac{\cov{\rvar{f}_0, \rvar{f}_n}}{\var{\rvar{f}}}$ (the lag $n$ autocorrelations of $\rvar{f}$), under the assumption that $\sum_{n=1}^\infty |\rho_n| < \infty$ in the limit of $N \to \infty$ we have that
\begin{equation}\label{eq:asymptotic-variance}
  \lim_{N\to\infty} \lpa N \, \var{\hatf_N} \rpa = \var{\rvar{f}} \lpa 1 + 2\sum_{n=1}^\infty \rho_n\rpa.
\end{equation}
Now considering a chain which is geometrically ergodic from its initial state, if $\expc{|\rvar{f}|^{2+\delta}}$ is finite for some $\delta > 0$ then it can be shown \citep{chan1994discussion,geyer1998markov,roberts2004general} that \eqref{eq:asymptotic-variance} is also the asymptoptic variance for a \ac{MCMC} estimator calculated using the chain states.

%\footnote{Note this is unrelated to the previous definition for importance sampling.}
This motivates a definition of the \ac{ESS} for an \ac{MCMC} estimator $\hatf_N$ computed using a geometrically ergodic chain as
\begin{equation}\label{eq:effective-sample-size-mcmc}
  N_{\textrm{eff}} = \frac{N}{1 + 2\sum_{n=1}^\infty \rho_n}.
\end{equation} 
The \ac{ESS} quantifies the number of independent samples that would be required in a Monte Carlo estimator to give an equivalent variance to the \ac{MCMC} estimator $\hatf_N$ \emph{in the asymptoptic limit $N \to \infty$}. In practice we cannot evaluate the exact autocorrelations and so we can only compute an estimated \ac{ESS}, $\hat{N}_{\textrm{eff}}$, from one or more simulated chains with the estimation method needing to be carefully chosen to ensure reasonable values \citep{thompson2010comparison}. Although the assumption of geometric ergodicity can often be hard to verify in practice and \ac{ESS} estimates can give misleading results in chains far from convergence, when used appropriately estimated \acp{ESS} can still be a useful heuristic for evaluating and comparing the effiency of Markov chain estimators and are often available as a standard diagnostic in \ac{MCMC} software packages \citep{plummer2006coda,carpenter2016stan,salvatier2016probabilistic}.

So far we have not discussed how to construct a transition operator giving a chain with the required invariant distribution. As a notational convenience we will consider the transition operator as being specified by a conditional density we term the \emph{transition density} $\fwdtrans : \set{X} \times \set{X}  \to [0, \infty)$ which is defined with respect to a base measure $\mu$ (which we will later assume to be the same as that which the target density we wish to integrate against is defined with respect to, hence the reuse of notation). The transition operator is then 
\begin{equation}
  \fwdtransop(\set{A} \gvn \vct{x}) =
  \int_{\set{A}} \fwdtrans(\vct{x}' \gvn \vct{x}) \,\dr\mu(\vct{x}')
  \quad \forall \set{A} \in \sset{F},~ \vct{x} \in \set{X}. 
\end{equation}
In practice the probability measure defined by a transition operator will often have a singular component, for example corresponding to a non-zero probability of the chain remaining in the current state. In this case $\fwdtransop$ is not absolutely continuous with respect to $\mu$ and a transition density is not strictly well defined. As we did in the previous chapter however we will informally use Dirac deltas to represent a `density' of singular measures, and so still consider a transition density as existing. The requirement that the transtion operator leaves the target distribution invariant, can then be expressed in terms of the target density $\tgtdens$ and transition density $\fwdtrans$ as
\begin{equation}
  \label{eq:invariant-density}
  \tgtdens(\vct{x}') = 
  \int_{\set{X}} \fwdtrans\lpa \vct{x}' \gvn \vct{x}\rpa \,\tgtdens(\vct{x}) \,\dr\mu(\vct{x}) 
  \quad\forall\vct{x}'\in \set{X}.
\end{equation}
Finding a transition density which leaves the target density invariant by satisfying \eqref{eq:invariant-density} seems difficult in general as it involves evaluating an integral against the target density - precisely the computational task which we have been forced to seek approximate solutions to. We can make progress by considering the joint density of a pair of successive states for a chain with invariant distribution $P$ that has converged to stationarity. Then we have that
\begin{equation}\label{eq:chain-state-pair-joint-density-stationary}
  \pden{\rvct{x}_{n}, \vct{x}_{n-1}}(\vct{x}',\vct{x}) = 
  \pden{\rvct{x}_{n}|\vct{x}_{n-1}}(\vct{x}'\gvn\vct{x}) \,\pden{\rvct{x}_{n-1}}(\vct{x}) =
  \fwdtrans(\vct{x}'\gvn\vct{x}) \,\tgtdens(\vct{x}).
\end{equation}
We can also consider factorising this joint density into the product of the marginal density of the current state $\pden{\rvct{x}_{n}}$ and the conditional density of the previous state given the current state $\pden{\rvct{x}_{n-1}|\vct{x}_{n}}$. Due to stationarity $\pden{\rvct{x}_n}$ is also equal to $\tgtdens$ and so we have that $\pden{\rvct{x}_{n-1}|\vct{x}_{n}}$ must be the density of a transition operator which also leaves $\tgtprob$ invariant, corresponding to a time reversed version of the original (stationary) Markov chain\footnote{The time reversal of a Markov chain is always itself a a Markov chain irrespective of stationarity (as the definining conditional independence structure is symmetric with respect to the direction of time), however the reverse of a homogeneous Markov chain which is not stationary will not in general itself be homogeneous.}. If we therefore denote $\bwdtrans=\pden{\rvct{x}_{n-1}|\vct{x}_{n}}$ (and which we will term the \emph{backward transition density} in contrast to $\fwdtrans$ which in this context we will qualify as the \emph{forward transition density}), we have that
\begin{equation}\label{eq:foward-backward-transition-density-balance}
  \fwdtrans(\vct{x}'\gvn\vct{x}) \,\tgtdens(\vct{x}) = 
  \bwdtrans(\vct{x}\gvn\vct{x}') \,\tgtdens(\vct{x}')
  ~~\forall \vct{x} \in \set{X},\, \vct{x}' \in \set{X}.
\end{equation}
Integrating both sides with respect to $\vct{x}$, we have that $\forall \vct{x}' \in \set{X}$
\begin{equation}\label{eq:backward-transition-density-invariance-derivation}
\begin{split}
  \int_{\set{X}} \fwdtrans(\vct{x}'\gvn\vct{x}) \,\tgtdens(\vct{x}) \,\dr\mu(\vct{x}) 
  &= 
  \int_{\set{X}} \bwdtrans(\vct{x}\gvn\vct{x}') \,\tgtdens(\vct{x}')  \,\dr\mu(\vct{x})
  \\
  &=
  \int_{\set{X}} \bwdtrans(\vct{x}\gvn\vct{x}')  \,\dr\mu(\vct{x}) \,\tgtdens(\vct{x}')
  =
  \tgtdens(\vct{x}'),
\end{split}
\end{equation}
and so that \eqref{eq:invariant-density} is satisfied, with the last inequality arising due to $\bwdtrans$ being a normalised density on its first argument. Therefore if we can find a pair of transition densities, $\fwdtrans$ and $\bwdtrans$, satisfying \eqref{eq:foward-backward-transition-density-balance}, then the transition operator specified by $\fwdtrans$ will leave the target distribution $\tgtprob$ invariant (and by an equivalent argument so will the transition operator specified by $\bwdtrans$). We can further simplify \eqref{eq:foward-backward-transition-density-balance} by requiring that $\fwdtrans = \bwdtrans = \trans$, i.e. that both forward and backward transition densities (and corresponding operators) take the same form and so that the chain at stationarity is \emph{reversible}, in which case have that
\begin{equation}\label{eq:detailed-balance}
  \trans(\vct{x}'\gvn\vct{x}) \,\tgtdens(\vct{x}) = 
  \trans(\vct{x}\gvn\vct{x}') \,\tgtdens(\vct{x}')
  ~~\forall \vct{x} \in \set{X},\, \vct{x}' \in \set{X}.
\end{equation}
This is often termed the \emph{detailed balance} condition. Importantly both the detailed balance \eqref{eq:detailed-balance} and \emph{generalised balance} \eqref{eq:foward-backward-transition-density-balance} conditions can also be written in terms of the unormalised density $\utgtdens$ by multiplying both sides by $Z$, and so can be checked even when $Z$ is unknown.

%(a criticism made of some of these results is that choice of `reference' reversible chain to compare to can be somewhat arbitrary \citep{mira2000non})
The restriction to reversible transition operators in detailed balance, while sufficient for \eqref{eq:invariant-density} to hold is not necessary. Markov chains which satisfy the generalised balance condition but not detailed balance are termed \emph{non-reversible}, and there are theoretical results suggesting that non-reversible Markov chains can sometimes achieve significantly improved convergence compared to related reversible chains \citep{diaconis2000analysis,neal2004improving,ichiki2013violation}. While there are several general purpose frameworks for specifying reversible transition operators which leave a target distribution invariant, developing methods for constructing irreversible transition operators with a desired invariant distribution has proven more challenging. The approaches proposed to date are generally limited in practice to special cases such as finite state spaces \citep{suwa2010markov,turitsyn2011irreversible,sun2010improving} or chains with tractable invariant distributions such as multivariate normal \citep{bierkens2016non}. 

Nonetheless non-reversible Markov chains are still commonly used in \ac{MCMC} applications. Given a set of transition operators which each individually leave a target distribution invariant, the sequential composition of the transition operators will necessarily by induction also leave the target distribution invariant. Even if the individual transition operators are all reversible, the overall sequential composition will generally not be (instead having an adjoint `backward' operator corresponding to applying the individual transitions in the reversed order). Sequentially combining several reversible transition operators is common in \ac{MCMC} implementations, though this is more often the result of each individual operator not meaning the requirements for ergodicity in isolation and so needing to be combined with other operators, rather than due to a specific aim of introducing irreversibility.

Having now introduced the key theory underlying \ac{MCMC} methods, we will now discuss practical implementations of the approach. In the following sub-sections we will review four frameworks for constructing reversible transition operators which leave a target distribution invariant: the \emph{Metropolis--Hastings} algorithm, \emph{Gibbs sampling}, \emph{slice sampling} and \emph{Hamiltonian Monte Carlo}. These methods are central to the contributions introduced in the subsequent chapters in this thesis.

\subsection{Metropolis--Hastings}

\begin{figure}[t]
\centering
\includetikz{metropolis-hastings}
\caption[Visualisation of Metropolis--Hastings algorithm.]{Visualisation of Metropolis--Hastings algorithm in a univariate target density. The green curves shows the unnormalised target density. The arrows indicate the current chain state. The orange curves show the density of proposed moves from this state, with the left axis using a narrower proposal than the right. The violet curves show the proposal density scaled by the acceptance probability of the proposed move, this reducing the probability of transitions to states with lower density than the current state. The orange region between the violet and orange curves represents the probability mass reallocated to rejections by the downscaling by the acceptance function. The broader proposal in the right axis has an increased probability of making a move to the other mode in the target density but at a cost of an increased rejection probability.}
\label{fig:metropolis-hastings}
\end{figure}

\marginpar{Although the algorithm has come to be commonly known by Edward Metropolis' name as first author on the 1953 paper \citep{metropolis1953equation}, it is believed that Arianna and Marshall Rosenbluth, two of the other co-authors, were the main contributors to the development of the algorithm \citep{gubernatis2005marshall}.}
The seminal \emph{Metropolis--Hastings} algorithm provides a general framework for constructing Markov chains with a desired invariant distribution and is ubitiqous in \ac{MCMC} methodology. The original Rosenbluth--Teller--Metropolis variant of the algorithm \citep{metropolis1953equation} dates to the very beginnings of the Monte Carlo method, having being first implemented on Los Alamos' MANIAC\footnote{\emph{Mathematical Analyzer, Numerical Integrator and Computer.}} one of the earliest programmable computers. The method was generalised in a key paper by Hastings \citep{hastings1970monte}, and the optimality among several competing alternatives of the form now used demonstrated by Peskun \cite{peskun1973optimum}. An extension to Markov chains on trans-dimensional spaces was proposed by Green \citep{green1995reversible}.

\begin{algorithm}[!t]
\caption{Metropolis--Hastings transition.}
\label{alg:metropolis-hastings}
\input{algorithms/metropolis-hastings}
\end{algorithm}

An outline of the method is given in Algorithm \ref{alg:metropolis-hastings} and a visualisation of its application to a univariate target distribution shown in Figure \ref{fig:metropolis-hastings}. The key idea is to propose updates to the state using an arbitrary transition operator and then correct for this transition operator not necessarily leaving the target distribution invariant by stochastically accepting or rejecting the proposal. If a proposal is rejected the chain remains at the current state, otherwise the chain state takes on the proposed value. The transition density corresponding to Algorithm \ref{alg:metropolis-hastings} is
\begin{equation}\label{eq:metropolis-hastings-transition-density}
\begin{split}
  \trans(\vct{x}' \gvn \vct{x}) &=
  \alpha(\vct{x}'\gvn \vct{x})\,q(\vct{x}'\gvn \vct{x}) + \, \\
  &\phantom{=}  
  \lpa 1 - 
  \int_{\set{X}} \alpha(\vct{x}^*\gvn \vct{x})\,q(\vct{x}^*\gvn \vct{x}) \,\dr\mu(\vct{x}^*)
  \rpa
  \delta(\vct{x}' - \vct{x}),
\end{split}
\end{equation}
with the \emph{acceptance probability} $\alpha : \set{X} \times \set{X} \to [0,1]$ defined as
\begin{equation}\label{eq:metropolis-hastings-acceptance-probability}
  \alpha(\vct{x}'\gvn \vct{x}) =
  \min\lbr 1,\, \frac{q(\vct{x}\gvn\vct{x}')\,\tgtdens(\vct{x}')}{q(\vct{x}'\gvn\vct{x})\,\tgtdens(\vct{x})} \rbr =
  \min\lbr 1,\, \frac{q(\vct{x}\gvn\vct{x}')\,\utgtdens(\vct{x}')}{q(\vct{x}'\gvn\vct{x})\,\utgtdens(\vct{x})} \rbr,
\end{equation}
and  $q : \set{X} \times \set{X} \to [0, \infty)$ the  \emph{proposal density}. %The overall algorithm defines a reversible transition operator which leaves the target distribution $\tgtprob$ invariant as we will now show.

The original Rosenbluth--Teller--Metropolis algorithm used a symmetric proposal density $q(\vct{x}' \gvn \vct{x}) = q(\vct{x} \gvn \vct{x}') ~\forall \vct{x} \in \set{X},\,\vct{x}'\in\set{X}$ (with the extension to the non-symmetric case being due to Hastings \citep{hastings1970monte}), in which case the acceptance probability definition simplifies to
\begin{equation}\label{eq:metropolis-acceptance-probability}
  \alpha(\vct{x}'\gvn \vct{x}) =
  \min\lbr 1,\, \frac{\tgtdens(\vct{x}')}{\tgtdens(\vct{x})} \rbr =
  \min\lbr 1,\, \frac{\utgtdens(\vct{x}')}{\utgtdens(\vct{x})} \rbr.
\end{equation}
Note that in both \eqref{eq:metropolis-hastings-acceptance-probability} and \eqref{eq:metropolis-acceptance-probability} the target density only appears as a ratio and so only need be known up to a constant.

For the purposes of verifying the detailed balance condition \eqref{eq:detailed-balance}, the density of \emph{self-transitions}, i.e. a transition to the same state, can be ignored as \eqref{eq:detailed-balance} is trivially satisfied for $\vct{x}' = \vct{x}$. Considering therefore the cases $\vct{x} \neq \vct{x}'$ where the Dirac delta term representing the singular component corresponding to rejected proposals can be neglected, we have $\forall \vct{x} \in \set{X},\,\vct{x}' \in \set{X} : \vct{x} \neq \vct{x}$
\begin{align}\label{eq:metropolis-hastings-detailed-balance-derivation}
  \trans(\vct{x}'\gvn\vct{x})\,\tgtdens(\vct{x}) &=
  \min\lbr 
    1,\, 
    \frac
      {q(\vct{x}\gvn\vct{x}')\,\tgtdens(\vct{x}')}
      {q(\vct{x}'\gvn\vct{x})\,\tgtdens(\vct{x})} 
  \rbr\,
  q(\vct{x}'\gvn \vct{x})\,\tgtdens(\vct{x})
  \\
  &=
  \min\lbr 
    q(\vct{x}'\gvn \vct{x})\,\tgtdens(\vct{x}),\, 
    q(\vct{x}\gvn\vct{x}')\,\tgtdens(\vct{x}')
  \rbr
  \\
  &=
  \min\lbr 
    \frac
      {q(\vct{x}'\gvn\vct{x})\,\tgtdens(\vct{x})}
      {q(\vct{x}\gvn\vct{x}')\,\tgtdens(\vct{x}')} 
    ,\, 1
  \rbr\,
  q(\vct{x}\gvn \vct{x}')\,\tgtdens(\vct{x}')
  \\
  &=
  \trans(\vct{x}\gvn\vct{x}')\,\tgtdens(\vct{x}').
\end{align}
Therefore the detailed balance condition is satisfied, and the Metropolis--Hastings transition operator leaves the target distribution $\tgtprob$ invariant.

An important special case for chains on a Euclidean state space $\set{X} = \reals^D$, is when the proposal transition operator is deterministic and corresponds to a differentiable involution of the current state. Let $\vct{\phi} : \set{X} \to \set{X}$ be an involution, i.e. $\vct{\phi} \circ \vct{\phi}(\vct{x}) = \vct{x} ~\forall \set{X}$ with Jacobian determinant $\jacobproddet{\vct{\phi}}{\vct{x}} = \left|\pd{\vct{\phi}(\vct{x})}{\vct{x}}\right|$ which is defined and non-zero $\tgtprob$-almost everywhere. Then if we define a transition operator via the transition density
\begin{equation}\label{eq:metropolis-hastings-transition-deterministic-proposal}
\begin{split}
  \trans(\vct{x}' \gvn \vct{x}) &=
  \delta\lpa\vct{x}' - \vct{\phi}(\vct{x})\rpa \alpha(\vct{x}) +
  \delta(\vct{x}' - \vct{x}) \lpa 1 - \alpha(\vct{x}) \rpa,
  \\
  \alpha(\vct{x}) &=
  \min\lbr 
    1,\,\frac{\tgtdens \circ \vct{\phi}(\vct{x})}{\tgtdens(\vct{x})} \jacobproddet{\vct{\phi}}{\vct{x}}
  \rbr,
\end{split}
\end{equation}
then this transition operator will leave the target distribution $\tgtprob$ invariant. This deterministic transition operator variant is as a special case of the trans-dimensional Metropolis--Hastings extension introduced by Green \citep{green1995reversible,geyer2003metropolis}. To generate from this transition operator from a current state $\vct{x}$ we compute the proposed move $\vct{\phi}(\vct{x})$ and accept the move with probablity $\alpha(\vct{x})$. We can demonstrate that this transition operator leaves $\tgtprob$ invariant by directly verifying \eqref{eq:invariant-density}
\begin{align}
  \label{eq:mh-det-proposal-derivation-1}
  &\int_{\set{X}} \trans(\vct{x}' \gvn \vct{x}) \, \tgtdens(\vct{x}) \,\dr\vct{x}
  \\
  \label{eq:mh-det-proposal-derivation-2}
  &=
  \int_{\set{X}} 
    \delta\lpa\vct{x}' - \vct{\phi}(\vct{x})\rpa \,\alpha(\vct{x}) \, \tgtdens(\vct{x}) +
    \delta(\vct{x}' - \vct{x}) \lpa 1 - \alpha(\vct{x}) \rpa \, \tgtdens(\vct{x})
  \,\dr\vct{x}
  \\
  \label{eq:mh-det-proposal-derivation-3}
  &=
  \int_{\set{X}} 
    \delta\lpa\vct{x}' - \vct{y}\rpa \,\alpha\circ\vct{\phi}(\vct{y}) \, 
    \tgtdens\circ\vct{\phi}(\vct{y}) \, \jacobproddet{\vct{\phi}}{\vct{y}} \,\dr\vct{y} +
  \lpa 1 - \alpha(\vct{x}') \rpa \, \tgtdens(\vct{x}')
  \\
  \label{eq:mh-det-proposal-derivation-4}
  &=
  p(\vct{x}') +
  \alpha\circ\vct{\phi}(\vct{x}') \, \tgtdens\circ\vct{\phi}(\vct{x}') \, 
  \jacobproddet{\vct{\phi}}{\vct{x}'} -
  \alpha(\vct{x}') \, \tgtdens(\vct{x}').
\end{align}
In going from \eqref{eq:mh-det-proposal-derivation-3} to \eqref{eq:mh-det-proposal-derivation-4} we use a change of variables $\vct{y} = \vct{\phi}(\vct{x})$ in the integral. As $\vct{\phi}$ is an involution we have that $\vct{\phi}\circ\vct{\phi}(\vct{x}') = \vct{x}'$ and $\jacobianproddet{\vct{\phi}}\circ\vct{\phi}(\vct{x}') = \jacobproddet{\vct{\phi}}{\vct{x}'}^{-1}$ and so
\begin{equation*}\label{eq:mh-det-proposal-derivation-5}
  \alpha\circ\vct{\phi}(\vct{x}')  \tgtdens\circ\vct{\phi}(\vct{x}') 
  \jacobproddet{\vct{\phi}}{\vct{x}'} =
  \min\lbr 
    \tgtdens\circ\vct{\phi}(\vct{x}') \jacobproddet{\vct{\phi}}{\vct{x}'},
    p(\vct{x}')
  \rbr 
  =
  \alpha(\vct{x}')\tgtdens(\vct{x}').
\end{equation*}
The last two terms in \eqref{eq:mh-det-proposal-derivation-4} therefore cancel and we have that \eqref{eq:invariant-density} is satisfied by the transition operator defined by \eqref{eq:metropolis-hastings-transition-deterministic-proposal}. 

Although this transition operator leaves the target distribution $P$ invariant, it is clear that it will not generate an ergodic Markov chain. Starting from a point $\vct{x}$ the next chain state will be either $\vct{\phi}(\vct{x})$ if the proposed move is accepted or $\vct{x}$ if rejected. In the former case the next proposed move will be to $\vct{\phi} \circ \vct{\phi}(\vct{x}) = \vct{x}$ i.e. back to the original state. Therefore the chain will visit a maximum of two states. However as noted previously we can sequentially compose individual transition operators which all leave a target distribution invariant. Therefore a deterministic proposal Metropolis--Hastings transition can be combined with other transition operators to ensure the chain is irreducible and aperiodic.

In general for a Metropolis--Hastings transition operator to be irreducible, it is necessary that the proposal operator is irreducible \citep{tierney1994markov}, however this is not sufficient. For a target density which is positive everywhere on $\set{X} = \reals^D$, then a sufficient but not necessary condition for irreducibility is that the proposal density is positive everywhere \citep{roberts2004general}. If the set of points with a non-zero probability of rejection has non-zero $\tgtprob$-measure, then the transition operator is aperiodic \citep{tierney1994markov}.

A common choice of proposal density when the target distribution is defined on $\reals^D$ is a multivariate normal density centred at the current state i.e. $q(\vct{x}'\gvn\vct{x}) = \nrm{\vct{x}' \gvn \vct{x}, \mtx{\Sigma}}$ which satisfies the positivity condition for irreducibility. In general we would achieve optimal performance with a proposal density covariance $\mtx{\Sigma}$ which is proportional to the covariance of the target distribution \citep{rosenthal2011optimal}. In practice we do not have access to the true covariance and so typically an isotropic proposal density is used with covariance $\mtx{\Sigma} = \sigma^2\mathbf{I}$ controlled by a single scale parameter $\sigma$, often termed the \emph{step size} or \emph{proposal width}. This proposal density is symmetric so the simplified acceptance rule \eqref{eq:metropolis-acceptance-probability} can be used, further the proposal density depends only on the difference $\vct{x}' - \vct{x}$ with Metropolis--Hastings methods having these properties often termed \emph{random-walk Metropolis}. 

Random walk Metropolis methods have been extensively theoretically studied, with sufficient conditions known in some cases to ensure geometric ergodicity of a chain \citep{mengersen1996rates,roberts1996geometric} though these can be hard to verify in practical problems. There has also been much work on practical guidelines and methods for tuning the free parameters in the algorithm, including approaches for tuning the step-size using acceptance rates \citep{gelman1997weak,roberts2001optimal} and adaptive variants which automatically estimate a non- isotropic proposal covariance \citep{haario2001adaptive,rosenthal2011optimal}.

In general the choice of proposal density will be key in determining the efficiency of Metropolis--Hastings \ac{MCMC} methods. Ideally we want to be able to propose large moves in the state space to reduce the depencies between successive chain states and so increase the number of effective samples, however this needs to be balanced with maintaining a reasonable acceptance probability with large proposed moves often having a low acceptance probability. Figure \ref{fig:metropolis-hastings} gives an illustration of this tradeoff in a one-dimensional example. 

\begin{figure}[!t]
\centering
\includetikz{concentration-of-measure-example}
\caption[Concentration of measure in high dimensions.]{Illustration of concentration of measure in a multivariate normal distribution. The plots shows the probability density of the distance from the origin $\rvar{r} = \left\Vert\rvct{x}\right\Vert_2$ of a $D$-dimensional multivariate normal random vector $\rvct{x} \sim \nrm{\vct{0},\mathbf{I}}$ for different dimensionalities $D$. As the dimension increases most of the mass concentrates away from the origin around a spherical shell of radius $\sqrt{D}$. For a multivariate normal random vector with mean $\vct{\mu}$ and covariance $\mtx{\Sigma}$ this generalises to the mass being mainly in an ellipsoidal shell aligned with the eigenvectors of $\mtx{\Sigma}$ and centred at $\vct{\mu}$.}
\label{fig:concentration-of-measure-gaussian}
\end{figure}

In high-dimensional spaces this issue is much more severe due to the phenomenon of \emph{concentration of measure}: in probability distributions defined on high-dimensional spaces most of the probability mass will tend to be concentrated into a `small' subset of the space \citep{mackay2003information,barp2017geometry}. An illustration of this phenomenon for the multivariate normal distribution is shown in Figure \ref{fig:concentration-of-measure-gaussian}, where the mass in high dimensions is mostly located in a thin ellipsoidal shell. The region where most of the mass concentrates, termed the \emph{typical set} of the distribution, will for the target distributions of interest generally have a significantly more complex geometry. Finding proposals which can make large moves in such settings is challenging: moves in most directions will have a probability of acceptance which exponentially drops to zero as the distance away from the current state is increased and so simple proposal densities which ignore the geometry the typical set such as those used in random-walk Metropolis will need to make very small moves to have a reasonable probability of acceptance \cite{betancourt2017conceptual}.

%To tackle this issue methods have been proposed which exploit more information about the target distribution than just the point evaluations of the density in the Metropolis--Hasting acceptance probability term. \ac{HMC} methods, which make use of the \emph{gradient} of the target density, are a particularly important class of such methods and a central focus of this thesis. We will discuss \ac{HMC} methods in detail in Chapter \ref{ch:hamiltonian-monte-carlo}.

%which can be efficiently automatically calculated using the reverse-mode automatic differentiation approach discussed in the previous chapter (\S\ref{subsec:computation-graphs}),

\subsection{Gibbs sampling}

\begin{figure}[!t]
\centering
\includetikz{gibbs-sampling}
\caption[Gibbs sampling schematic.]{Schematic of Gibbs sampling transition in a bivariate normal target distribution (ellipses indicate constant density contours). Given an initial state $\rvct{x} = (x_1,x_2)$, the $\rvar{x}_1$ (horizontal) co-ordinate is first updated by independently sampling from the normal conditional $\pden{\rvar{x}_1|\rvar{x}_2}(\cdot\gvn x_2)$, represented by the orange curve. The new partially updated state is then $\rvct{x} = (x_1^*, x_2)$. The second $\rvar{x}_2$ (vertical) co-ordinate is then independently resampled from the normal conditional $\pden{\rvar{x}_2|\rvar{x}_1}(\cdot\gvn x_1^*)$, shown by the green curve. The final updated state is then $\rvct{x} = (x_1^*, x_2^*)$.}
\label{fig:gibbs-sampling}
\end{figure}

\begin{algorithm}[!t]
\caption{Sequential scan Gibbs transition.}
\label{alg:gibbs-sampling}
\input{algorithms/sequential-scan-gibbs}
\end{algorithm}

\emph{Gibbs sampling} \citep{geman1984stochastic,gelfand1990sampling}, originally proposed by Geman and Geman for image restoration using a Markov random field image model, is based on the observation that a valid transition operator for a joint target distribution across many variables, is one which updates only a subset of the variables and leaves the conditional distribution on that subset given the rest invariant. Although if used in isolation a transition operator which only updates some components of the state will not give an ergodic chain, as discussed previously multiple transition operators can be combined together to achieve ergodicity. 

More specifically the original formulation of Gibbs sampling defines a Markov chain by sequentially independently resampling each individual variable in the model from its conditional distribution given the current values of the remaining variables. If $\set{I}$ is an index set over the individual variables in the vector target state $\rvct{x}$, then for each $i \in \set{I}$ we partition the state $\rvct{x}$ into the $i^{\rm th}$ variable $\rvar{x}_i$ and a vector containing all the remaining variable values $\rvct{x}_{\backslash i}$. For each $i \in \set{I}$ the target density can be factorised in to the marginal density $\tgtdens_{\backslash i}$ on $\rvct{x}_{\backslash i}$ and conditional density $\tgtdens_i$ on $\rvar{x}_i$ given $\rvct{x}_{\backslash i}$, i.e.
\begin{equation}\label{eq:gibbs-sampling-complete-conditional-factorisation}
  \tgtdens(\vct{x}) = \tgtdens_{i}(x_i \gvn \vct{x}_{\backslash i}) \, \tgtdens_{\backslash i}(\vct{x}_{\backslash i}),
\end{equation}
with the conditional densities $\lbrace \tgtdens_i \rbrace_{i \in \set{I}}$ termed the \emph{complete conditionals} of the target density. If each of these complete conditionals corresponds to a distribution we can generate samples from (for example using a transform method or rejection sampling) then we can apply the sequential Gibbs sampling transition operator defined in Algorithm \ref{alg:gibbs-sampling} and visualised for a bivariate example in Figure \ref{fig:gibbs-sampling}.

The sequential Gibbs transition is irreducible and aperiodic under mild conditions \citep{roberts1994simple,chan1993asymptotic}. Rather than using a deterministic sequential scan through the variables, an alternative is to randomly sample without replacement the variable to update on each iteration; unlike the sequential scan version this defines a reversible transition operator. The random update variant is more amenable to theoretical analysis and comes with some stronger guarantees, however in practice the ease of implementation of the sequential scan variant and computational benefits in terms of memory access locality mean it seems to be more often used in practice \citep{he2016scan}. A compromise between the completely random updates and a sequential scan is to randomly permute the update order after each complete scan.

A apparent advantage of Gibbs sampling over Metropolis--Hastings is the lack of a proposal density which needs to be tuned. This is has helped popularise `black-box' implementations of Gibbs sampling such as the probabilistic modelling packages BUGS \citep{gilks1994language} and JAGS \citep{plummer2003jags}. A well-known issue with Gibbs sampling however is that its performance is highly dependent on the parameterisation used for the target density \citep{raftery1991many}, with strong correlations between variables leading to large dependencies between successive states and slow convergence to stationarity. This can be alleviated in some cases by using a suitable reparameterisation to reduce dependencies between variables, however this restores the difficulty of tuning free parameters.

The updates do not necessarily need to be performed by sampling from complete conditionals of single variables - in some cases the complete conditional of a vector variables has a tractable form which can be sampled from as a `block'; this motivates the name \emph{block Gibbs samling} for such variants. By accounting for the dependencies between the variables in a block this can help alleviate some of the issues with highly correlated targets where applicable.

Compound terms such as \emph{Metropolis-within-Gibbs} are sometimes used to refer to methods which sequentially apply Metropolis transition operators which each update only a subset of variables in the target distribution. We will however consider the defining feature of Gibbs sampling as being exact sampling from one or more conditionals rather than sequentially applying transition operators which update only subsets of variables and so will only refer to `Gibbs sampling' in that context.

\subsection{Slice sampling}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{.46\linewidth}
\centering
\includetikz{slice-sampling-1}
\vspace{3mm}
\end{subfigure}
\begin{subfigure}[b]{.46\linewidth}
\centering
\includetikz{slice-sampling-2}
\end{subfigure}
\caption[Schematic of linear slice sampling.]{Schematic of linear slice sampling, showing `plan' (left) and `cross-sectional' (right) views of a bivariate target density. Orange curve (left) and line (right) indicates a constant density slice $\set{S}_h$. The black square indicates current target state value $\vct{x}$ and the dashed line is \emph{slice line}, the one-dimensional linear sub-space aligned with the vector $\vct{v}$ which a new value from the state will be sampled on. The extents of the dashed line segment represent the initial bracket new proposed states will be drawn from. Points are proposed on the slice line by drawing a value uniformly from the current bracket. The red circle represents an initial proposed point which is not in the slice and so the right bracket edge is shrunk to this point. The violet circle shows a second sampled point from the new reduced bracket, this point within the slice and so returned as the updated target state.}
\label{fig:slice-sampling}
\end{figure}


\begin{algorithm}[!t]
\caption{Linear slice sampling transition.}
\label{alg:linear-slice-sampling}
\input{algorithms/linear-slice-sampling}
\end{algorithm}

The next class of \ac{MCMC} methods we will consider is \emph{slice sampling}. Slice sampling is a family of methods which exploit the same observation as used to motivate rejection sampling - to sample from a target distribution it is sufficient to uniformly sample from the volume beneath a graph of the target density function. Rather than attempt to generate independent points from this volume as in rejection sampling, slice sampling instead constructs a transition operator which leaves the uniform distribution on this volume invariant.

The method we will concentrate on here was proposed by Neal \citep{neal1997markov,neal2003slice}. Murray, Adams and Mackay later proposed \emph{elliptical slice sampling} \citep{murray2010elliptical}, an extension of Neal's slice sampling method which is particularly effective for target distributions which are well approximated by a multivariate normal.
% which leverages the ellipsoidal geometry of the typical set of multivariate normal distributions to construct a slice sampling transition operator
% with a related algorithm which uses per data-point auxiliary variables in Bayesian inference problems developed by Damien, Wakefield and Walker \citep{damien1999gibbs}

Slice sampling defines a Markov chain on an augmented state space by introducing an auxiliary \emph{height} variable $\rvar{h} \in [0, \infty)$ in addition to the original target state $\rvct{x} \in \set{X}$. The conditional density on the height variable is $\pden{\rvar{h}|\rvct{x}}(h\gvn\rvct{x}) = \frac{1}{\utgtdens(\vct{x})} \ind{[0,\utgtdens(\vct{x}))}(h) $, i.e. uniform over the interval between zero and the unnormalised target density value. The joint density on the augmented space is then
\begin{equation}\label{eq:slice-sampling-augmented-target}
  \pden{\rvct{x},\rvar{h}}(\vct{x},h) = 
  \frac{1}{\utgtdens(\vct{x})} \ind{[0,\utgtdens(\vct{x}))}(h)\, \frac{\utgtdens(\vct{x})}{Z} =
  \frac{1}{Z} \ind{[0,\utgtdens(\vct{x}))}(h).
\end{equation}
Marginalising this joint density over $\rvct{x}$ recovers the original target density i.e. $\pden{\rvct{x}} = \tgtdens$. Therefore if we can construct a Markov chain with \eqref{eq:slice-sampling-augmented-target} as its unique invariant distribution, the $\rvct{x}$ components of the chain state will be dependent samples from the target distribution for a chain at stationarity and so can be used to compute \ac{MCMC} estimates. Several of the methods we will introduce later in this thesis will also exploit this idea of defining a Markov chain on an augmented state space by introducing auxiliary variables.

The overall slice sampling transition is then formed of the sequential composition of a transition operator which updates $\rvar{h}$ given $\rvct{x}$ and a second operator which updates $\rvct{x}$ given $\rvar{h}$, each leaving the distributions corresponding to the conditional densities $\pden{\rvar{h}|\rvct{x}}$ and $\pden{\rvct{x}|\rvar{h}}$ respectively invariant, and so by the same argument as for Gibbs sampling the overall transition leaving the target distribution invariant. By construction the conditional density $\pden{\rvar{h}|\rvct{x}}$ is a simple uniform density and so the first transition operator is a Gibbs sampling type update in which the height variable is independently resampled from $\mathcal{U}\lpa 0, \utgtdens(\vct{x})\rpa$, where $\vct{x}$ is the current value of the target state $\rvct{x}$.

The conditional density $\pden{\rvct{x}|\rvar{h}}(\vct{x}\gvn h)$ is also locally uniform, equal to a positive constant whenever $\utgtdens(\vct{x}) > h$ and zero elsewhere. However we can only evaluate the density up to an unknown constant as we cannot compute the Lebesgue measure of the set $\set{S}_h = \fset{\vct{x} \in \set{X} : \utgtdens(\vct{x}) > h}$ that the density is non-zero over. In general $\set{S}_h$, which is the eponymous \emph{slice} of slice sampling (so called as it represents a slice through the volume under the density curve at a fixed height $h$), will have a complex geometry including potentially consisting of several disconnected components in the case of multimodal densities. The complexity of the slices generally prevents us therefore from independently sampling a new value for $\rvct{x}$ uniformly from $\set{S}_h$ and so we cannot use a full Gibbs sampling scheme corresponding to sequentially independently sampling from $\pden{\rvar{h}|\rvct{x}}$ and $\pden{\rvct{x}|\rvar{h}}$.

A key contribution of \citep{neal2003slice} was to introduce an elegant method for constructing a transition operator which leaves $\pden{\rvct{x}|\rvar{h}}$ invariant. In particular the method proposed has few free parameters to tune, has an efficiency which is relatively robust to the choices of the free choices that are introduced, and will for smooth target densities always move the target state by some amount (in contrast to the potential for rejections in Metropolis--Hastings methods). This method is summarised in Algorithm \ref{alg:linear-slice-sampling} and a schematic visualisation of the process shown in Figure \ref{fig:slice-sampling}.

An important first step in the algorithm is reducing the problem of generating a point uniformly on the multidimensional slice $\set{S}_h$ to making a move on a one-dimensional linear subspace of this slice  (motivating our naming of this algorithm \emph{linear slice sampling}) which includes the current $\rvct{x}$ state. In the original description of the algorithm in \citep{neal2003slice} the one-dimensional subspace is chosen to be axis-aligned, correponding to updating a single component of the target state. 

In this case the restriction of the slice on to the one-dimensional subspace is entirely specified by the conditional density on the chosen variable component given the current values of the remaining components in the state. Slice sampling transitions for each variable in the target state can then be applied sequentially akin to Gibb sampling, but with the advantage over Gibbs of not requiring the complete conditionals to be of a tractable form we can generate exact samples from. If conditional independency structure in the target density means the complete conditionals depend only on local subsets of variables in the target state using updates of this form has the advantage of exploiting this locality. As with Gibbs sampling however applying slice sampling in this manner makes performance strongly dependent on the parameterisation of the target density, with large magnitude correlations likely to lead to slow exploration of the space. 

\begin{figure}
\centering
\begin{subfigure}[b]{.46\linewidth}
\centering
  \includetikz{axis-aligned-slice-sampler-bivariate-normal}
  \vspace{-6mm}
  \caption{Axis-aligned updates}
  \label{sfig:axis-aligned-linear-slice-sampler}
\end{subfigure}
~~
\begin{subfigure}[b]{.46\linewidth}
\centering
  \includetikz{random-direction-slice-sampler-bivariate-normal}
  \vspace{-6mm}
  \caption{Random direction updates}
  \label{sfig:random-direction-linear-slice-sampler}
\end{subfigure}
  \caption[Linear slice sampler comparison.]{Samples generated using \subref{sfig:axis-aligned-linear-slice-sampler} axis-aligned versus \subref{sfig:random-direction-linear-slice-sampler} random-direction  linear slice sampling in a correlated bivariate normal distribution. In both cases 1000 transitions where performed (with random selection of axis to update on each iteration in \subref{sfig:axis-aligned-linear-slice-sampler}) with every second sampled state shown.  the maximum number of step out iterations is $M=4$ and the initial bracket width is fixed at $w=1$. The dotted ellipse shows the contour of the target density which contains 0.99 of the mass. The random direction chain is able to explore the typical set of the target distribution more effectively in this case with the axis-aligned updates leading to slower diffusion along the major axis of the elliptical contour.}
  \label{fig:linear-slice-sampler-comparison}
\end{figure}

In \citep{neal2003slice} various specifically multivariate extensions of the algorithm are suggested which could help counter this issue, however they add significant implementation complexity compared to the basic algorithm. A simple alternative is to define the one-dimensional subspace as being the line defined by a randomly chosen vector and passing through the current value of $\rvct{x}$. If this vector is generated independently of the current state this is sufficient to ensure the overall transition retains the correct invariant distribution. 

If little is known about the target distribution a reasonable default choice is to sample a unit vector of the required dimensionality by generating a random zero-mean isotropic covariance multivariate normal vector and then scaling it to unit norm; if an approximate covariance matrix $\hat{\mtx{\Sigma}}$ is known for the target density then instead generating the vector from $\nrm{\vct{0},\mtx{\Sigma}}$ prior to normalising might be a better choice (as it favours moves aligned with the principle eigenvectors of $\mtx{\Sigma}$) however in this case elliptical slice sampling will often be a better choice. 

This random-direction slice sampling variant is discussed in comparison to elliptical slice sampling in \citep{murray2010elliptical}. It is also bears resemblance to the scheme proposed in \citep{chen1998toward} which uses the same auxiliary variable formulation as slice sampling, but there the random direction is chosen in $\set{X}\times[0,\infty)$ i.e. to update both $\rvct{x}$ and $\rvar{h}$ and not used with the remainder of Neal's slice sampling algorithm. An example comparison of appying axis-aligned and random-direction linear slice sampling updates to a strongly positively correlated bivariate normal target distribution is shown in Figure \ref{fig:linear-slice-sampler-comparison}. In this toy example the random-direction updates are able to more effectively explore the target distribution.

The generation of the vector $\vct{v}$ determining the one-dimensional sub-space of the slice the update is performed on is represented in Algorithm \ref{alg:linear-slice-sampling} by Line \ref{algline:slice-sample-direction} by $\vct{v}$ being generated from a density $q$. As well as specifying the \emph{direction} of the slice line, the vector $\vct{v}$ also specifies a scale along this line. In Neal's description of the algorithm this is represented by the explicit \emph{bracket width} parameter $w$. Here instead we assume this parameter is implicitly defined by the Euclidean norm of the vector $\vct{v}$, through suitable choice of $q$ this allowing for direction dependent scales and also the possibility of randomisation of the scale; as we will see shortly however compared to for example random-walk Metropolis updates with a normal proposal, linear slice sampling is much less sensitive to the choice of scale parameters, therefore a single fixed scale will often be sufficient.

Once the slice line direction and scale has been chosen, the remainder of the algorithm can be split into two stages: selection of an initial bracket on the slice line and including the point corresponding to the current state; iteratively uniformly sampling points within the current bracket, accepting the point if it is within the slice $\set{S}_h$ otherwise shrinking the bracket and repeating. The algorithm proposed by Neal ensures both these stages are performed reversibly such that the detailed balance condition \eqref{eq:detailed-balance} is maintained.

The \emph{slice bracket} defines a contiguous interval $\lambda \in [b_l,b_u]$ on the slice line $\vct{x}^*(\lambda) = \vct{x}_n + \lambda \vct{v}$ and always includes the point $\lambda = 0$ corresponding to the current state. The initial bracket is chosen by sampling a upper bound $b_u$ uniformly from $[0, 1]$ and then setting $b_l \gets b_u - 1$; in the $\lambda$ slice line coordinate system this corresponds to a bracket width of one, however in general the slice line vector $\vct{v}$ can have non-unit length and so defines the initial bracket width in the target variable space. Randomising the positioning of the current state within the bracket ensures reversibility as the resulting bracket would have an equal probability (density) of being selected from any other point in the bracket (which the final accepted point will be within).

In general only a subset of the points in the current slice bracket will be within the slice $\set{S}_h$. As new states are proposed by sampling a point uniformly from the current bracket, the probability of such a proposal being in the slice and so accepted will be equal to the proportion of the bracket that intersects with the slice $\set{S}_h$. In general therefore it is desirable for the bracket to include as much of the slice as possible while not making the proportion of the bracket intersecting with the slice too small such that many points need to be proposed before one on the slice is found. The magnitude of $\vct{v}$ determines the initial bracket extents and so should generally chosen based on any knowledge of the typical `scale' of the target density. Often however we will have little prior knowledge about such scaling however and the typical scale will often vary signifcantly across the target space, and so we may choose an initial bracket which includes only a small proportion of the intersection of the slice with the slice line.

The stepping out routine proposed by \citep{neal2003slice} and detailed in Lines \ref{algline:slice-sample-step-out-start} to \ref{algline:slice-sample-step-out-end} in Algorithm \ref{alg:linear-slice-sampling} is designed to counter this issue. The initial slice bracket $[b_l, b_u]$ is iteratively `stepped-out' by incrementing / decrementing the upper / lower bracket bounds until the corresponding endpoint of the bracket lies outside the slice or a pre-determined maximum number of steps out have been performed. Ideally the step out routine will return a bracket which contains all of the intersection of the slice with slice line while not also including too great a proportion of off slice points; in general the slice may be non-convex or consist of multiple disconnected components and so the intersection of the slice line with the slice may consist of multiple disconnected intervals in which case the stepping out routine will likely only expand the slice to include a subset of these intervals. The adaptivity provided by the stepping out routine will still however generally help to make the performance of the sampler much less sensitive to the choice of the bracket scale in contrast to for example random-walk Metropolis algorithms which typically use a single fixed scale. 

Analagously to the randomisation of the initial bracket positioning, in the stepping out routine if a maximum number of step out iterations $M$ is set, the resulting step `budget' is randomly allocated between increments of the upper bound $b_u$ and decrements of the lower bound $b_l$ such that final extended bracket generated by the step out routine would have an equal probability of being generated from any point within the generated bracket interval. If $M$ is set to zero this correponds to not performing any stepping out and simply using the initial sampled bracket; although reducing the robustness of the algorithm to the choice of the initial bracket width this option has the advantage of minimising the number of target density evaluations by not requiring additional density evaluations at the bracket endpoints during the step-out routine. An alternative `doubling' step-out routine was also proposed in \citep{neal2003slice}. This has the advantage of exponentially expanding the slice bracket compared to the linear growth of the step-out routine described in Algorithm \ref{alg:linear-slice-sampling} and so can be more efficient in target distributions where the typical scales of the density varies across several orders of magnitude. The doubling procedure requires a more complex subsequent procedure for sampling points in the resulting bracket however to ensure reversibility. 

Once the initial bracket has been generated and potentially stepped out, the remainder of the algorithm consists of sampling a point within the slice bracket which is within the slice $\set{S}_h$. This is done in an iterative manner by first sampling a point uniformly from the current bracket and checking if it is within the slice or not. If the proposed point is in the slice, the corresponding value for the target variables is returned at the new state. Otherwise the proposed point is set as the new upper or lower bound of the bracket such that the point corresponding to the current state remains within the bracket. This shrinks the bracket by removing an interval where it is known at least some regions of are not within the slice. A new point is then sampled uniformly from the new smaller bracket and the procedure repeats until an acceptable point in the slice is found.

The iterative shrinking of the slice bracket implemented by this procedure introduces a further level of adaptivity in to the slice sampling algorithm, meaning that even if only a small proportion of the initial bracket lies within the slice only relatively few iterations will be needed still till the bracket is shrunk sufficiently for there to be a high probability of proposing a point within the bracket. By ensuring the point corresponding to the current state always remains within the current bracket, reversibility is maintained.

\begin{algorithm}[!t]
\caption{Elliptical slice sampling transition.}
\label{alg:elliptical-slice-sampling}
\input{algorithms/elliptical-slice-sampling}
\end{algorithm}

An alternative to the linear slice sampling procedure just described, is the \emph{elliptical slice sampling} method proposed in \citep{murray2010elliptical} and described in Algorithm \ref{alg:elliptical-slice-sampling}. As suggested by the name, in elliptical slice sampling rather than proposing points on a line instead an elliptical path in the target space is defined and new points proposed on this ellipse. 

Elliptical slice sampling is intended for use in target distributions which are reasonably well approximated by a known multivariate normal distribution with density $\nrm{\vct{\mu},\mtx{\Sigma}}$. This Gaussian density might correspond to a multivariate normal prior distribution on model latent variables where the dependence between the latent and observed variables is only weak and so the posterior remains well approximated by the prior or a Gaussian approximation fitted directly to the target distribution using one of the optimisation based approximate inference strategies we will discuss in the following section \citep{nishihara2014parallel}. 

In each elliptical slice sampling transition an auxiliary vector $\vct{v}$ is independently sampled from the distribution with density $\nrm{\vct{\mu},\mtx{\Sigma}}$. If the target distribution was exactly described by the multivariate normal density we could use this independent draw directly as the new chain state (though obviously in this case there would be no advantage in formulating as an \ac{MCMC} method). In reality the target distribution will only approximately described by the multivariate normal density $\nrm{\vct{\mu},\mtx{\Sigma}}$ and so we wish to instead use this independent draw to define a Markov transition operator that will potentially move the state to a point nearly independent of the current state, but is also able to back off to more conservative proposals closer to the current chain state. This is achieved by defining an elliptical path in target space centred at $\vct{\mu}$, passing through the current chain state $\vct{x}_n$ and the auxiliary vector $\vct{v}$ and parameterised by an angular variable $\theta$
\begin{equation}\label{eq:elliptical-path}
  \vct{x}^*(\theta) = (\vct{x}_n - \vct{\mu}) \cos\theta + (\vct{v} - \vct{\mu}) \sin\theta + \vct{\mu}
\end{equation}
If we generated $\theta$ uniformly from $\mathcal{U}(0,2\pi)$ then the corresponding proposed transition $\vct{x}^*(\theta)$ would exactly leave a distribution with density $\nrm{\vct{\mu},\mtx{\Sigma}}$ invariant. As we instead wish to leave the target distribution invariant, a slice sampling algorithm is used to find a $\theta$  which accounts for the difference between the target distribution and multivariate normal approximation. An auxiliary slice height variable $h$ is sampled uniformly from $\mathcal{U}\lpa 0,\utgtdens(\vct{x}_n) / \nrm{\vct{x}_n \gvn \vct{\mu}, \mtx{\Sigma}}\rpa$ and used to define a slice $S_h = \lbr \vct{x} \in \set{X} : \utgtdens(\vct{x}_n) / \nrm{\vct{x}_n \gvn \vct{\mu}, \mtx{\Sigma}} < h \rbr$. Similar to the linear slice sampling algorithm, a bracket $[\theta_l, \theta_u]$ on the elliptical path is randomly placed around $\theta = 0$ corresponding to the current state $\vct{x}_n$. Unlike the requirement to choose a suitable initial bracket width in linear slice sampling however, we can define the initial bracket in elliptical slice sampling to include the entire elliptical path i.e. $\theta_l = \theta_u - 2\pi$; we only need to randomise the `cut-point' defining the initial end-points of the bracket to ensure reversibility. This removes the need to choose an initial bracket width (defined by $|\vct{v}|$ in our description of the linear slice algorithm) and for any step out procedure, and so beyond choosing the multivariate normal approximation elliptical slice sampling does not have any free settings which need to be tuned.

Once the initial bracket is defined, a directly analagous iterative procedure to that used in the linear slice sampling algorithm is used to find a $\theta$ value corresponding to a point in the slice while using rejected proposed points to shrink the bracket. As with linear slice sampling, providing the target density is a smooth function and so the intersection of the elliptical path with the slice is a non-zero measure set, then the state moved to by the elliptical slice sampling transition operator will never be equal to the previous state.

\subsection{Hamiltonian Monte Carlo}

\begin{algorithm}[!t]
\caption{Hamiltonian Monte Carlo transition.}
\label{alg:hamiltonian-monte-carlo}
\input{algorithms/hamiltonian-monte-carlo}
\end{algorithm}

The \ac{MCMC} methods discussed in the previous three subsections required only that we are able to evaluate a (unnormalised) density function for the target distribution of interest. For distributions defined on real-valued variables the target density function $\utgtdens$ will often be differentiable - that is the gradient $\pd{\utgtdens}{\vct{x}}$ exists $P$-almost everywhere. In these cases it is natural to consider using the gradient to help guide updates to the state towards regions with higher probability density.

A particularly powerful \ac{MCMC} method utilising gradient information is \ac{HMC} \citep{duane1987hybrid,neal2011mcmc} originally termed \emph{Hybrid Monte Carlo}. Like slice-sampling, \ac{HMC} introduces auxiliary variables in to the chain state and constructs a Markov transition operator which leaves a distribution on the augmented space invariant which has the target distribution as its marginal distribution on the original variables. In particular a set of \emph{momentum} variables are introduced and a Hamiltonian dynamic simulated on the extended system. The end point of the simulated trajectory is used as a proposed update to the state and this move accepted or rejected in a Metropolis--Hasting step. 

The standard variant of \ac{HMC} requires the target density of interest $p$ to be defined with respect to the Lebesgue measure on a Euclidean space $\set{X} = \reals^D$. Target densities with bounded support on $\reals^D$ add complication to the algorithm by requiring checks that proposed updates to the state remain within the support, however often a change of variables can be performed with a bijective transformation that maps to a density with unbounded support, for example taking a log-transform of a positive variable, with the change of variables formula \eqref{eq:change-of-variables-vector-bijective} used to compute the resulting density on the transformed space. 
\begin{equation}\label{eq:boltzmann-gibbs-density}
  p(\vct{x}) = \frac{1}{Z} \exp\lpa -\phi(\vct{x}) \rpa
\end{equation}

\section{Summary}

The sampling approaches to approximate inference described in this section allow tractable estimation of the integrals involved in many inference problems. In cases where we can tractably generate independent samples from the target distribution, the $\frac{1}{N}$ scaling of the variance of Monte Carlo estimates of expectations with the number of samples $N$, independent of the dimensionality of the space being integrated over, allows computation of estimates which are sufficiently accurate for many practical purposes without the infeasible exponential blow-up in computation of quadrature methods. Further simple Monte Carlo methods are trivially parallelisable meaning even if generation of each independent sample is relatively expensive, parallel compute devices such as \acp{GPU} and \ac{CPU} clusters can easily be exploited if available.

Generating independent samples from distributions on high-dimensional spaces with complex dependencies between the variables is generally non-tractable however. Transform sampling methods offer a scalable approach for generating independent samples for a few special cases such as the multivariate normal distribution. Rejection sampling is more generally applicable however still requires idendifying a proposal distribution with a density which (scaled by a constant) strictly upper bounds the target distribution density. In high-dimensional distributions it will generally be infeasible to find a suitable proposal distribution which is a sufficiently tight bound, with in general the proportion of accepted samples becoming exponentially small as the dimensionality is increased, making rejection sampling only applicable to low-dimensional distributions in practice.

Importance sampling methods may seem initially to offer a solution to this issue, allowing consistent estimates of the values of arbitrary integrals (including marginal density estimations that may not be directly estimated with standard Monte Carlo approaches) providing we can generate independent sample from a distribution which meets the weak condition of having a density which is non-zero everywhere the integrand is non-zero. The general importance sampling estimator is formed as a ratio of two Monte Carlo estimates \eqref{eq:importance-sampling-mc-estimates}; providing these estimators have finite variance each will show the typical $\frac{1}{N}$ scaling of the estimator variance with the number of samples used. In general however unless the importance distribution used is very closely matched to the target distribution, simple importance sampling methods are also impractical in high-dimensional spaces as the constant factors in the variances of the Monte Carlo estimates can grow exponentially with dimension meaning an infeasibly large number of samples are needed to compute estimates of a useful level of accuracy.

Markov chain Monte Carlo methods offer a more a scalable alternative to approximate inference in complex high-dimensional probabilistic models. \ac{MCMC} methods exploit the intuition that finding a good `local' approximation to the target distribution --- for example within a small region around a poin or varying along only one direction in the target space --- is usually a much more tractable task than finding an approximation which matches the target distribution globally, particularly in high dimensional spaces where concentration of measure will mean the typical set of a distribution is usually concentrated in to small region of the space and even seemingly small mismatches between an approximation and target can lead to very little overlap between the target and approximation typical sets. Markov chain theory shows how we can exploit such local approximations to make perturbative updates to a Markov chain state such that the realisation of the chain converge to generating dependent samples from the target distribution of interest. Although theory can guarantee \ac{MCMC} estimates will eventually converge to the correct values it is usually difficult to assess the rate of that convergence in practical problems making it difficult to diagnose chain convergence or a lack thereof. This can make application of \ac{MCMC} methods more challenging from a user-perspective than simple Monte Carlo methods as some level of expertise is usually needed to diagnose and find solutions to convergence issues.

%Although applicable in a wide range of models, the 


%Although applicable in a very wide range of settings, the extra free choices usually introduced by \ac{MCMC} methods can require significant user tuning to get acceptable performance and diagnosing convergence is significantly more challenging than for simple Monte Carlo methods.
We discussed three general methods for constructing Markov chains which leave a target distribution invariant --- the Metropolis--Hastings method, Gibbs sampling and slice sampling. Each of the constructs offers its own advantages and disadvantages and no one method dominates the others in all aspects. It is also common to combine transition operators of different types, for exampling using different methods to update distinct subset of the variables in a model given the remaining variables, or use multiple updates to the same variables to potentially combine the good properties of the individual operators.

Due in part probably to their ease of implementation, Metropolis--Hast\-ings methods based on simple proposal distribution such as Gaussian random-walk Metropolis methods are very commonly used in practice. Performance of such methods is however usually very dependent on the good choice of the algorithm free parameters such as the proposal width / step size, with poor choices leading to very slow mixing chains. In target distributions with a complex geometry, for example where the appropriate scale for state updates may differ signficiantly across the target space, Metropolis--Hastings methods using simple fixed proposals may be unable to mix well even when optimally tuned. One solution to this issue is to use proposals which exploit more information about the local geometry of the distribution such as the gradient-based Hamiltonian Monte Carlo methods we will discuss in Chapter \ref{ch:hamiltonian-monte-carlo}.

Gibbs sampling methods are also very popular with general purpose implementations such as BUGS \citep{gilks1994language} and JAGS \citep{plummer2003jags} having supported their application to a wide range of models. Although requiring that we are able to decompose the target distribution into complete conditionals we can tractably generate independent samples from, in the models where it can be applied Gibbs sampling offers the advantage over Metropolis--Hastings methods of not requiring choosing and tuning the free parameters of the proposal distribution. As noted previously however the performance of Gibbs sampling methods is very dependent on the parameterisation of the target distribution, with parameterisations with strong dependencies between the variables tending to lead to very slowly mixing chains. Although this can be alleviated by reparameterisation or using block-updates to coupled variables in some cases, the resulting extra implementation and tuning requirements erode the simplicity advantage compared to competing methods.

The slice sampling algorithms introduced at the end of the last section are more complex than the corresponding algorithms for Metropolis--Hastings and Gibbs sampling, and this added implementation complexity may explain in part the seemingly less widespread use of slice samp\-ling methods in practice\footnote{As a very rough metric at the time of writing the original 1953 Metropolis et al. publication \citep{metropolis1953equation} has 35,288 citations recorded on Google Scholar, the 1984 Geman and Geman publication \citep{geman1984stochastic} commonly cited as the original sourcce of the Gibbs sampling algorithm 20,485 citations and the 2003 Neal \citep{neal2003slice} slice sampling publication, 1,444 citations.}, although the relatively much more recent introduction of the algorithm is likely also a factor. The tradeoff achieved for this increased implementation complexity however are algorithms which usually require much less tuning than random-walk Metropolis methods to achieve reasonable performance and are more robust than both Gibbs sampling and random-walk Metropolis methods to target distributions with complex geometries. As noted in \citep{murray2010elliptical} due to the multiple target density evaluations per chain state update in slice sampling methods, often a well-tuned Metropolis--Hastings method will be able to achieve a greater efficiency in terms of effective samples per run time. However the sometimes significant user time required for tuning can often outweigh the gains in efficiency over slice sampling, and even if automatic adaptive tuning methods are used these will still often struggle to cope with distributions with locally varying geometry. One of the contributions of this thesis will be illustrating how slice sampling methods can often be applied to inference problems where Metropolis--Hastings methods are more commonly used with sometimes significant improvements in robustness and efficiency.

[To do]

%The approximate inference methods we have reviewed in this chapter provide

% Monte Carlo VI
%   - ADVI
%   - BBVI
%   - Autoencoding variational Bayes (amortised inference)
% empirical Bayes
% expectation propagation ESS
% variational MCMC
% variational inf and MCMC bridging the gap
