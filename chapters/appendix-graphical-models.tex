\chapter{Graphical models}\label{app:graphical-models}

\marginpar{Graphical models = statistics × graph theory × computer science\\---Zoubin Ghahramani}
When working with probabilistic models involving large numbers of random variables, it will often be the case that not all the variables are jointly dependent on each other but that instead there are more local conditional relationships between them. Graphical models, which use graphs to describe the dependencies between random variables, are a useful framework for visualising the structure in complex probabilistic models and for giving a graph-theoretic basis for establishing the dependence between sets of random variables. Central to graphical models is the concept of conditional independence. Let $\rvar{x} : \set{S} \to \set{X}$, $\rvar{y} : \set{S} \to \set{Y}$ and $\rvar{z} : \set{S} \to \set{Z}$ be three random variables with corresponding $\sigma$-algebras, $\sset{F}_{\rvar{x}}$, $\sset{F}_{\rvar{y}}$ and $\sset{F}_{\rvar{z}}$ respectively. Following from our earlier definition of (unconditional) independence of random variables in \eqref{eq:independent-rvars}, we say that \emph{$\rvar{x}$ and $\rvar{y}$ are conditionally independent given $\rvar{z}$}, denoted $\rvar{x} \perp \rvar{y} \gvn \rvar{z}$, if for $\prob{\rvar{z}}$-almost all $z \in \set{Z}$
\begin{equation}\label{eq:conditional-independence-property}
  \prob{\rvar{x},\rvar{y}|\rvar{z}}(\set{A},\set{B} \gvn z) =
  \prob{\rvar{x}|\rvar{z}}(\set{A} \gvn z)\,
  \prob{\rvar{y}|\rvar{z}}(\set{B} \gvn z)
  ~\,\forall 
  \set{A} \in \sset{F}_{\rvar{x}},\,
  \set{B} \in \sset{F}_{\rvar{y}}.
\end{equation}
If a joint density on the random variables exists, a sufficient condition for $\rvar{x} \perp \rvar{y} \gvn \rvar{z}$ is that the conditional density $\pden{\rvar{x},\rvar{y}|\rvar{z}}$ factorises as
\begin{equation}\label{eq:conditional-independence-densities}
  \pden{\rvar{x},\rvar{y}|\rvar{z}}(x,y \gvn z) =
  \pden{\rvar{x}|\rvar{z}}(x \gvn z)
  \pden{\rvar{y}|\rvar{z}}(y \gvn z)
  \quad \forall 
  x \in \set{X},\,
  y \in \set{Y},\,
  z \in \set{Z}.
\end{equation}
This definition can be naturally extended to conditional independence when conditioning on more than one random variable, for example
\begin{equation}\label{eq:conditional-independence-many-vars}
  \rvar{v} \perp \rvar{x} \gvn \rvar{y},\,\rvar{z} \implies
  \pden{\rvar{v},\rvar{x}|\rvar{y},\rvar{z}}(v,x \gvn y,z) =
  \pden{\rvar{v}|\rvar{y},\rvar{z}}(v \gvn y,z)
  \pden{\rvar{x}|\rvar{y},\rvar{z}}(x \gvn y,z).
\end{equation}

% conditional independence and independence

\section{Directed and undirected models}

\begin{figure}[!t]
\centering
\begin{subfigure}[b]{.5\linewidth}
\vskip 0pt
\centering
\includetikz{directed-graphical-model}
%\vskip 5pt
\caption{Directed graphical model.}
\label{sfig:example-directed-graphical-model}
\end{subfigure}%
\begin{subfigure}[b]{.5\linewidth}
\vskip 0pt
\centering
\includetikz{undirected-graphical-model}
%\vskip 5pt
\caption{Undirected graphical model.}
\label{sfig:example-undirected-graphical-model}
\end{subfigure}%
\caption[Directed and undirected graphical models.]{Examples of directed and undirected graphical models. Circular nodes represent random variables in the model, with edges between them indicating dependencies between variables.}% \subref{sfig:example-directed-graphical-model} Shows a directed graphical model on three random variables. The graph indicates that $\rvar{x}_1 \perp \rvar{x}_2 \gvn \emptyset$. \subref{sfig:example-undirected-graphical-model} Shows a undirected graphical model on four random variables. The model indicates that $\rvar{x}_1 \perp \rvar{x}_4 \gvn \rvar{x}_2,\rvar{x}_3$ and $\rvar{x}_2\perp \rvar{x}_3 \gvn \rvar{x}_1,\rvar{x}_4$.}
\label{fig:example-graphical-models}
\end{figure}

Several different graphical frameworks have been proposed for representing conditional independency relationships (and other information) in probabilistic models. 

\emph{Directed graphical models} \citep{pearl1988probabilistic}, also known as \emph{Bayesian networks}, represent probabilistic models as \emph{directed acyclic graphs} (i.e. a directed graph in which there are no directed cycles), with the nodes in the graph representing random variables in the model and the edges of the graph defining a factorisation of the joint density over these variables into a product of conditional and marginal densities. In particular a conditional density factor is included for each node with parents (on the node random variable value given the parent variable values) and a marginal density factor for each root node without any parents.

An example directed graphical model for three random variables, $\rvar{x}_1$, $\rvar{x}_2$ and $\rvar{x}_3$, is shown in Figure \ref{sfig:example-directed-graphical-model}. The graph implies that the joint density can be factorised as
\begin{equation}\label{eq:example-directed-graphical-model-factorisation}
  \pden{\rvar{x}_1,\rvar{x}_2,\rvar{x}_3}(x_1,x_2,x_3) = 
  \pden{\rvar{x}_3|\rvar{x}_1,\rvar{x}_2}(x_3 \gvn x_1,x_2) \,
  \pden{\rvar{x}_1}(x_1) \, \pden{\rvar{x}_2}(x_2).
\end{equation}
\marginpar{Ancestral sampling in a directed graphical model corresponds to first sampling values from all the root nodes from their marginal densities, then iteratively sampling from the conditional densities on each node for which all the parents nodes already have sampled values to condition on.}
Note that this factorisation would not be valid for all joint densities on the three variables; in particular we have that $\rvar{x}_1$ and $\rvar{x}_2$ are (unconditionally) independent and so that the joint density $\pden{\rvar{x}_1,\rvar{x}_2}$ can be written as the product of the two marginals $\pden{\rvar{x}_1}$ and $\pden{\rvar{x}_2}$.

Directed graphical models are a natural way of specifying \emph{generative models} - i.e. probabilistic models which can be used to generate simulated observable quantities. Typically the factorisation specified by a directed graphical model gives a straightforward method to generate values from the joint density via \emph{ancestral sampling}.

An alternative formalism for graphically representing probabilistic models is that of \emph{undirected graphical models} \citep{kindermann1980markov}, which are also known as \emph{Markov random fields}. As with directed graphical models, each node in the graph represents a random variable, but here the edges connecting nodes are undirected. Rather than describing a factorisation of a joint density into conditional and marginal densities, an undirected graphical model indicates the factorisation of a joint density into a product of clique potentials on each of the maximal cliques in the graph. A \emph{clique} is a fully connected component of the graph - i.e. a subset of nodes in the graph such that all pairs of nodes in the subset are connected by an edge. A \emph{maximal clique} is a clique which is not a strict subset of any other clique. A \emph{clique potential} is a non-negative function of the values of the random variables in the clique; it does necessarily correspond to any conditional or marginal probabilty density. 

An example undirected graphical model on four random variables, $\rvar{x}_1$, $\rvar{x}_2$, $\rvar{x}_3$ and $\rvar{x}_4$, is shown in Figure \ref{sfig:example-undirected-graphical-model}. Here the (maximal) cliques correspond to all the connected pairs of nodes. If $\psi_{a,b}$ denotes the clique potential on the pair $(a,\,b)$ then the graphical model implies the joint density can be factorised as
%  \pden{\rvar{x}_1,\rvar{x}_2,\rvar{x}_3,\rvar{x}_4}(x_1,x_2,x_3,x_4) =\\
\begin{equation}\label{eq:example-undirected-graphical-model-factorisation}
\begin{split}
  \frac{1}{Z} 
  \psi_{\rvar{x}_1,\rvar{x}_2}(x_1,x_2)
  \psi_{\rvar{x}_1,\rvar{x}_3}(x_1,x_3)
  \psi_{\rvar{x}_2,\rvar{x}_4}(x_2,x_4)
  \psi_{\rvar{x}_3,\rvar{x}_4}(x_3,x_4),
\end{split}
\end{equation}
with $Z$ a normalising constant such that the density integrates to 1 and so defines a valid probability measure.

Undirected graphical models are a natural representation for models of systems of mutually interacting components. For example they are commonly used in models of images to represent dependencies between pixel values and models of magnetic interactions in particle lattices. 

Unlike directed models, generating joint configurations of the random variables in an undirected graphical model from the implied joint distribution is typically a non-trivial task, with no general equivalent to ancestral sampling. Further the joint density can typically only be evaluated up to an unknown normalising constant, with the integral needed to evaluate this constant often intractable for models involving a large number of variables or complex potentials. As we will see, these properties mean that inference in distributions defined by undirected graphical models is often particularly challenging.

\marginpar{
D-separation:\\
%Given a directed graphical model, a pair of random variables $\rvar{x}$ and $\rvar{y}$ in the model and a conditioning set of random variables $\set{C}$, 
$\rvar{x} \perp \rvar{y} \gvn \set{C} \iff$ all paths in the graph between $\rvar{x}$ and $\rvar{y}$ are blocked. A path is blocked if at least one of the following holds: 1. The path includes a $\rightarrow\kern-3.5pt\bigcirc\kern-3.5pt\rightarrow$ node or a $\leftarrow\kern-3.5pt\bigcirc\kern-3.5pt\rightarrow$ node in $\set{C}$. \\2. The path includes a $\rightarrow\kern-3.5pt\bigcirc\kern-3.5pt\leftarrow$ node and neither the node or its descendants are in $\set{C}$.\\[2ex]

U-separation:\\
%Given an undirected graphical model, a pair of random variables 
$\rvar{x}$ and $\rvar{y}$ in the model and a conditioning set of random variables $\set{C}$, $\rvar{x} \perp \rvar{y} \gvn \set{C} \iff$ at least one random variable node on every path between $\rvar{x}$ and $\rvar{y}$ is in $\set{C}$.
}
As suggested at the start of this section, both directed and undirected graphical models encode conditional independence properties of probabilistic models. In particular the rules of \emph{D-separation} for directed graphical models and \emph{U-separation} for undirected model give graph-based algorithmic descriptions of how to determine whether a pair of random variables are conditionally independent for a given conditioning set of random variables. 

For example the directed graphical model in Figure \ref{sfig:example-directed-graphical-model} encodes the (un)conditional independence property $\rvar{x}_1 \perp \rvar{x}_2 \gvn \emptyset = \rvar{x}_1 \perp \rvar{x}_2$ i.e. that $\rvar{x}_1$ and $\rvar{x}_2$ are independent if the value of $\rvar{x}_3$ is \emph{not} conditioned on. The undirected graphical model in Figure \ref{sfig:example-undirected-graphical-model} encodes the conditional independence properties $\rvar{x}_1 \perp \rvar{x}_4 \gvn \rvar{x_2},\,\rvar{x}_3$ and $\rvar{x}_2 \perp \rvar{x}_3 \gvn \rvar{x_1},\,\rvar{x}_4$.

Although there are methods to convert a directed graphical model to an undirected one and vice versa, in general these transformations are lossy - not all of the conditional independence relationships encoded in the original graph will necessarily be maintained in the transformed graph. For example there is no undirected graphical model which will represent the exact set of conditional independence properties represented by the directed graphical model in Figure \ref{sfig:example-directed-graphical-model}. Likewise there is no directed graphical model which will represent the exact set of conditional independence properties represented by the undirected graphical model in Figure \ref{sfig:example-undirected-graphical-model}. Further there are distributions with dependency structures and factorisations which cannot be uniquely represented by either directed or undirected graphical models \citep{frey2002extending}.

\section{Factor graphs}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{.45\linewidth}
\vskip 0pt
\centering
\includetikz{directed-factor-graph}
%\vskip 5pt
\caption{A factor graph equivalent of the directed model in Figure \ref{sfig:example-directed-graphical-model}.}
\label{sfig:example-directed-factor-graph}
\end{subfigure}%
 \hspace*{\fill}
\begin{subfigure}[b]{.45\linewidth}
\vskip 0pt
\centering
\includetikz{undirected-factor-graph}
%\vskip 5pt
\caption{A factor graph equivalent of the undirected model in Figure \ref{sfig:example-undirected-graphical-model}.}
\label{sfig:example-undirected-factor-graph}
\end{subfigure}%
\caption[Factor graph examples.]{Examples of factor graphs corresponding to the directed and undirected graphical models in Figure \ref{fig:example-graphical-models}. Square black nodes correspond to individual factors depending on the connected variables (represented by circular nodes) in the joint density.}
\label{fig:example-factor-graphs}
\end{figure}

An alternative graphical model formalism which overcomes some of the limitations of directed and undirected graphical models is that of factor graphs \citep{frey1997factor,frey2002extending}. In factor graphs, in addition to nodes representing random variables, represented as in directed and undirected graphical models by circular nodes, a second class of nodes, denoted by filled squares (\tikzset{external/export next=false}\tikz{\node[factor] {};}), are introduced which represent individual factors in the joint density across the random variables represented in the model.
	
Factors may be either directed or undirected. Undirected factors, denoted by factor nodes in which all edges connecting to variable nodes are undirected, correspond to a factor in the joint density which depends on all of the variables with nodes connected to the factor, but without any requirement that the factor corresponds to a conditional or marginal probability density. Directed factors, denoted by factor nodes in which at least one edge from the factor node to a variable node is directed, correspond to a conditional density on the variables pointed to by directed edges given the values of the variables connected to the the factor node by undirected edges (if there are no such variables then the factor instead corresponds to a marginal density).

Edges between nodes in a factor graph are always between nodes of disparate types i.e. between factor and variable nodes, but never between factor and factor or variable and variable nodes. As with directed graphical models, factor graphs with directed factors must not contain any directed cycles (i.e. a connected loop of edges in which one of every pair of edges connected to a factor on the loop is directed and all of the directed edges point in the same sense around the loop). 

In the original extension of undirected factor graphs \citep{frey1997factor} to include directivity \citep{frey2002extending}, it was proposed to allow multiple directed factors to connect via directed edges to the same variable node, representing multiple factors in a conditional density on that varaiable. This generalisation introduces extra normalisation requirements and looses the interpretation of a directed factor as directly representing a conditional density, and so we will here only use directed factor graphs in which there is at most one directed edge connecting from a factor to a node.

Whether two variables are conditionally independent given a set of other variables can be checked from a factor graph by checking if all paths (i.e. connected series of edges and nodes) between the two corresponding variables nodes in the factor graph are \emph{blocked}. A path is blocked if at least one of the following conditions is satisfied \citep{frey2002extending}
\begin{enumerate}
  \item One of the variable nodes in the path is in the conditioning set.
  \item One of the directed factor nodes in the path has two connected undirected edges in the path and there is no second directed path from the node to a variable node in the conditioning set.
\end{enumerate}

Both directed and undirected graphical models can always be losslessly converted to a factor graph, i.e. such that by applying the above blocking rules after the transformation we obtain exactly the same set of conditional independency properties as present in the original graph, and thus they have a superset of the capacity to represent conditional indendence properties as either of these two alternative frameworks. For example, factor graph equivalents of the directed and undirected graphical model examples in Figure \ref{fig:example-graphical-models} are shown in Figure \ref{fig:example-factor-graphs}. 

\begin{figure}[!t]
\vskip 0pt
\centering
\includetikz{boltzmann-machine-factor-graph}
%\vskip 5pt
\caption[Boltzmann machine factor graph.]{Five unit Boltzmann machine factor graph showing explicit factorisation of distribution into pairwise and single variable potentials.}
\label{fig:boltzmann-machine-factor-graph}
\end{figure}

As well as allowing representations of mixed graphs with both directed and undirected factors which cannot be represented with either directed or undirected graphical models, factor graphs are also able to include finer-grained information about the factorisation of the joint density than either of the other two model types by explicitly indicating the presence of individual factors. For instance Figure \ref{fig:boltzmann-machine-factor-graph} shows the factor graph for a \emph{Boltzmann machine} distribution, sometimes called a \emph{pairwise binary Markov random field} or \emph{Ising model}, on five binary random variables $\lbrace \rvar{s}_i \rbrace_{i=1}^5$. A Boltzmann machine distribution can be factored in to a product of pairwise weighted interactions $\exp(\rvar{s}_i W_{ij} \rvar{s}_j)$ and single variable bias potentials $\exp(b_i \rvar{s}_i)$, each of which are explicitly represented by labelled factors in Figure \ref{fig:boltzmann-machine-factor-graph}. A corresponding undirected graphical model representation would have a single clique involving all five variables, and so would not indicate any information about the factorisation of the joint density.

\begin{figure}[!t]
\vskip 0pt
\centering
\includetikz{radon-hierarchical-linear-regression-factor-graph}
%\vskip 5pt
\caption[Hierarchical linear regression model factor graph.]{Hierarchical linear regression model factor graph showing examples of extended factor graph notation.}% including deterministic factor nodes (\tikz{\node[op] {};}), shading to indicate observed variables, plate notation (rounded rectangle) and constant nodes ($\vct{x}^{(i)}$ and $c^{(i)}$).}
\label{fig:hier-lin-regression-factor-graph}
\end{figure}

In Figure \ref{fig:hier-lin-regression-factor-graph} we illustrate some additional useful factor graph notation we will use in this thesis. We use a factor graph corresponding to a hierarchical linear regression model which will be discussed in more detail later in the thesis as a motivating example. The exact meaning of the model and its various factors are unimportant to the discussion of notation here so will be skipped for now. %We give some brief details now so the subsequent discussion of the notation used in Figure \ref{fig:hier-lin-regression-factor-graph} makes sense. The model is for a data set of $N$ real valued output values $\lbr y^{(i)} \rbr_{i=1}^N$ which are assumed to be \ac{iid} given $N$ corresponding input value pairs $\lbr (\vct{x}^{(i)},\,c^{(i)}) \rbr_{i=1}^N$ where each $\vct{x}^{(i)} \in \reals^D$ is a $D$-dimensional vector of real valued regressors, and each $c^{(i)} \in \fset{1\dots K}$ is an integer representation of a categorical regressor. A linear input-output relationship is assumed with 

It will often be useful to be able to explicitly represent deterministic functions applied to the random variables in a factor graph. For this purpose we introduce an additional node type denoted by an unfilled diamond (\tikzset{external/export next=false}\tikz{\node[op] {};}). The semantics of this node type are very similar to standard directed factor nodes. Variables acting as inputs to the function are connected to the node by undirected edges and the variable corresponding to the function output indicated by a directed edge from the node to the relevant variable. Like standard factor nodes, the deterministic factor nodes only ever connect to variable nodes. The operations performed by the function on the inputs will usually be included as a label adjacent to the node as illustrated by the example in Figure \ref{fig:hier-lin-regression-factor-graph}.

A deterministic factor node can informally\footnote{A Dirac delta cannot strictly define a density as it is not the Radon--Nikodyn derivative of an absolutely continuous measure, however it can be informally treated as the density of a singular Dirac measure $f(0) = \int f(x) \,\dr\delta(x) \simeq \int f(x) \delta(x) \,\dr x$.} be considered equivalent to a directed factor node with a degenerate Dirac delta conditional density on the output variable which concentrates all the probability mass at the output of the function applied to the inputs variables. The previously discussed rules for evaluating conditional independency properties in factor graphs can be directly extended to account for the new node type by just considering it as a directed factor node.

Optionally constant values used in a model may be included in a factor graph as plain nodes indicated only by a label. The $\vct{x}^{(i)}$ and $c^{(i)}$ nodes in Figure \ref{fig:hier-lin-regression-factor-graph} are an example of this notation.

A commonly used convention in factor graphs (and other graphical models) is \emph{plate notation} \citep{buntine1994operations}, with an example of a plate shown by the rounded rectangle bounding some of the nodes in Figure \ref{fig:hier-lin-regression-factor-graph}. Plates are used to indicate a subgraph in the model which is replicated multiple times (with the replications being indexed over a set which is typically indicated in the lower right corner of the plate as in Figure \ref{fig:hier-lin-regression-factor-graph}). The subgraph entirely contained on the plate is assumed to be replicated the relevant number of times, with any edges crossing into the plate from variable nodes outside of the plate being repeated once for each subgraph replication. Plates are commonly used to represent a model component repeated across multiple data items.

Each of the factors in Figure \ref{fig:hier-lin-regression-factor-graph} is labelled with a shorthand for a probability density function corresponding to the conditional or marginal density factor associated with the node. Definitions for the shorthand notations that are used for densities in this thesis are given in Tables \ref{tab:standard-distributions-unbounded} and \ref{tab:standard-distributions-bounded}. The dependence of the factors on the value of the random variable the density is defined on is omitted in the labels for brevity.

A final additional notation used in Figure \ref{fig:hier-lin-regression-factor-graph} is the use of a shaded variable node (corresponding to $\rvar{y}^{(i)}$) to indicate a random variable corresponding to an observable quantity in the model. %In general as we will see in the subsequent discussion of inference, the random variables corresponding to observable quantities in a model will be conditioned on observed data.

\section{Computation graphs}\label{sec:computation-graphs}

A final graph based tool we will make use of in this theis is that of \emph{computation graphs} \citep{bauer1974computational}. In particular computation graphs (via associated software frameworks \citep{theano2016theano}) will be used to allow automatic differentiation of complex probabilistic models used in later chapters. Computation graphs are not typically considered in the context of probabilistic graphical models, but they share many of the same features and as we will see are closely related to directed factor graphs.

A \emph{computation graph}, sometimes insead termed a \emph{computational graph} or \emph{data flow graph}, represents the computations involved in evaluating a mathematical expression. In this thesis we will distinguish between two types of nodes in a computation graph. \emph{Variable nodes} correspond to variables which hold either inputs to the computation or intermediate results corresponding to the outputs of sub-expressions. \emph{Operation nodes} describe how non-input variable nodes are computed as functions of other variable nodes. In other presentations of computation graphs often the operation nodes are instead implicitly represented by directed edges between variable nodes. However analagously to the more explicit factorisation afforded by directed factor graphs compared to directed graphical models, directly representing operations as nodes allows finer grained information about the decomposition of the operations associated with a computation graph to be included.

As with directed graphical models and directed factor graphs, computation graphs cannot contain directed cycles. This does not preclude recursive and recurrent computations however as these can always be unrolled to form a directed acyclic graph. The `mathematical expressions' a computation graph is constructed to evaluate can be arbitarily complex - a computation graph corresponding to the evaluation of any numerical algorithm can always be constructed including use of arbitrary nested flow control and branching statements.

\begin{figure}[!t]
\vskip 0pt
\centering
\includetikz{normal-log-density-computation-graph}
\vskip 0pt
\caption[Example computation graph.]{Example computation graph corresponding to calculation of the negative log density of a univariate normal distribution.
%\\ 
%$c = -\log\nrm{x \gvn m,\,\exp(u)^2} = \frac{1}{2}\lpa\frac{x - m}{\exp(u)}\rpa^2 + u + \frac{1}{2}\log(2\uppi)$.
}
\label{fig:gaussian-density-computation-graph}
\end{figure}

An example of a computation graph representing the calculation of the negative log density of a univariate normal distribution, i.e.
\begin{equation}\label{eq:gaussian-nld-computation-graph-expression}
  \rvar{c} =
  \frac{1}{2}\lpa\frac{\rvar{x} - \rvar{m}}{\rvar{s}}\rpa^2 + \log\rvar{s} + \frac{1}{2}\log(2\uppi)
\end{equation}
is shown in Figure \ref{fig:gaussian-density-computation-graph}. The graph inputs have chosen to be the value of the random variable ($\rvar{x}$) to evaluate the density at and the mean ($\rvar{m}$) and the standard deviation ($\rvar{s}$) parameters of the density. %We could have equally chosen to parameterise the density in terms of its standard deviation or variance, but as we will see later when discussing inference it is often useful to work in terms of unconstrained variables (the standard deviation and variance both necessarily being non-negative).

Variable nodes in the computation graph have been represented by labelled circles and operation nodes with labelled diamonds. Undirected edges connecting from a variable node to an operation node correspond to the inputs to the operation, and directed edges from an operation node to variable nodes to the outputs of the operation.

The computation graph associated with a given expression is not uniquely defined. There will usually be multiple possible orderings in which operations can be applied to achieve the same result (up to differences due to non-exact floating point computation). Similarly what should be considered a single operation to be represented by a node in the computation graph as opposed to being split up into a sub-graph of multiple operations is a matter of choice. For example in Figure \ref{fig:gaussian-density-computation-graph} the addition of the constant $\frac{1}{2}\log(2\uppi)$ could have been included at various other points in the graph and the operation $\frac{1}{2}\rvar{z}^2$ could have been split in to separate multiplication and exponentation operations.

\section{Automatic differentiation}\label{sec:automatic-differentiation}

The main motivation for representing expressions as computation graphs is to formalise an efficient general procedure termed automatic differentiation for automatically calculating derivatives of the output of an expression with respect to its inputs \citep{nolan1953analytical,beda1959programs}. The key ideas in automatic differentiation are to use the chain rule to decompose the derivatives into products and sums of the partial derivatives of the output of each individual operation in the expression with respect to its input, and to use an efficient recursive accumulation of these partial derivative sum-products corresponding to a traversal of the computation graph such that multiple derivatives can be efficiently calculated together.

Depending on how the computation graph is traversed to accumulate the derivative terms, different modes of automatic differentiation are possible. Of most use in this thesis will be \emph{reverse-mode accumulation} \citep{speelpenning1980compiling}, in which the derivatives of an output node with respect to all input nodes are accumulated by a reverse pass through the computation graph from the output node to inputs.

\begin{figure}[!t]
\vskip 0pt
\centering
\includetikz{normal-log-density-computation-graph-reverse-mode-ad}
\vskip 0pt
\caption[Reverse-mode automatic differentiation.]{Visualisation of applying reverse-mode automatic differentiation to the computation graph in Figure \ref{fig:gaussian-density-computation-graph} to calculate the derivatives of the negative log density of a univariate normal distribution.}
\label{fig:gaussian-density-gradient-computation-graph}
\end{figure}

As an example the partial derivatives of the expression for univariate normal log density given in \eqref{eq:gaussian-nld-computation-graph-expression} with respect to $\rvar{x}$, $\rvar{m}$ and $\rvar{s}$ can be decomposed using the chain rule in terms of the intermediate variables in the computation graph shown in Figure \ref{fig:gaussian-density-computation-graph} as
\begin{align}
  \pd{\rvar{c}}{\rvar{x}} &=
  \pd{\rvar{c}}{\rvar{a}} 
  \pd{\rvar{a}}{\rvar{z}}
  \pd{\rvar{z}}{\rvar{y}}
  \pd{\rvar{y}}{\rvar{x}},
  \\
  \pd{\rvar{c}}{\rvar{m}} &=
  \pd{\rvar{c}}{\rvar{a}} 
  \pd{\rvar{a}}{\rvar{z}}
  \pd{\rvar{z}}{\rvar{y}}
  \pd{\rvar{y}}{\rvar{m}},
  \\
  \pd{\rvar{c}}{\rvar{s}} &=
  \pd{\rvar{c}}{\rvar{a}} 
  \pd{\rvar{a}}{\rvar{z}}
  \pd{\rvar{z}}{\rvar{s}} +
  \pd{\rvar{c}}{\rvar{b}} 
  \pd{\rvar{b}}{\rvar{u}}
  \pd{\rvar{u}}{\rvar{s}}.
\end{align}
We can immediately see that some of the chains of products of partial derivatives are repeated in the different derivative expressions - for example $\pd{\rvar{c}}{\rvar{a}} \pd{\rvar{a}}{\rvar{z}}$ appears in the expressions for all three derivatives. Reverse-mode accumulation is effectively an automatic way of exploiting these possibilities for reusing calculations.

Figure \ref{fig:gaussian-density-gradient-computation-graph} shows a visualisation of reverse-mode accumulation applied to the computation graph in Figure \ref{fig:gaussian-density-computation-graph}. The first step is for a \emph{forward pass} through the graph to be performed, i.e. values are provided for each of the input variables and then each of the intermediate and output variables calculated from the incoming operation applied to their parent values. Importantly the values of all variables in the graph calculated during the forward pass must be maintained in memory. 

\begin{algorithm}[!t]
\caption{Reverse-mode automatic differentiation.}
\label{alg:reverse-mode-ad}
\input{algorithms/reverse-mode-automatic-differentiation}
\end{algorithm}

The \emph{reverse pass} recursively calculates the values of the partial derivatives of the relevant output node with respect to each variable node in the graph - we will term these intermediate derivatives \emph{accumulators} denoted with barred symbols in Figure \ref{fig:gaussian-density-gradient-computation-graph} e.g. $\bar{\rvar{a}} = \pd{\rvar{c}}{\rvar{a}}$. The reverse pass begins by seeding an accumulator for the output node to one (i.e. $\bar{\rvar{c}} = \pd{\rvar{c}}{\rvar{c}} = 1$ in Figure \ref{fig:gaussian-density-gradient-computation-graph}). 

Accumulators for the input variables of an operation are calculated by multiplying the accumulator for the operation output by the partial derivatives of the operation output with respect to each input variable. For non-linear operations multiplying by the operator partial derivatives will require access to the value of the input variables calculated in the forward pass. If a variable is an input to multiple operations, the derivative terms from each operation are added together in the relevant accumulator, as for example shown for $\bar{\rvar{s}}$ in Figure \ref{fig:gaussian-density-gradient-computation-graph}. By recursively applying these product and sum operations, the derivatives of the output with respect to all variables in the graph can be calculated. A general description of the method for computation graphs with a single output node and multiple inputs is given in Algorithm \ref{alg:reverse-mode-ad}.

This reverse accumulation method allows computation of numerically exact (up to floating point error) derivatives of a single output variable in a computation graph with respect to \emph{all input variables} with a computational cost, in terms of the number of atomic operations which need to be performed, that is a constant factor of the cost of the evaluation of the original expression represented by the computation graph in the forward pass. The constant factor is typically two to three and at most six \citep{baydin2015automatic}. This efficient computational cost is balanced by the requirement that the values of all intermediate variables in the computation graph evaluated in the forward pass through the graph must be stored in memory for the derivative accumulation in a reverse pass, which for large computational graphs can become a bottleneck.

To calculate the full Jacobian from a computation graph representing a function with $M$ inputs $\fset{\rvar{x}_i}_{i=1}^M$ and $N$ outputs $\fset{\rvar{y}_i}_{i=1}^N$, i.e. the $N \times M$ matrix $\mtx{J}$ with entries $J_{i,j} = \pd{\rvar{y}_i}{\rvar{x}_j}$, we can do a single forward pass and $N$ reverse passes each time accumulating the derivatives of one output variable with respect to all inputs. This leads to an overall computational cost that is $\mathcal{O}(N)$ times the cost of a single (forward) function evalaution to evaluate the full Jacobian. As each of the reverse passes can trivially be run in parallel (in addition to any parallelisation of the operations in the forward and reverse passes themselves), this $\mathcal{O}(N)$ factor in the operation count need not corresponds to an equivalent increase in compute time.

An alternative to reverse-mode accumulation is \emph{forward-mode accumulation} \citep{wengert1964simple}, which insteads accumulates partial derivatives with respect to a single input variable alongside the forward pass through the graph. In contrast to reverse-mode, this allows calculation of the partial derivatives of all output variables with respect to a single input variable at a computational cost that is a constant factor of the cost of the evaluation of the original expression in the forward pass. Forward-mode accumulation therefore allows evaluation of the Jacobian of a function with $M$ inputs and $N$ outputs at an overall computational cost that is $\mathcal{O}(M)$ times the cost of a single function evaluation. 

For functions with $M \gg N$, e.g. scalar valued functions of multiple inputs, reverse-mode accumulation is generally therefore signficantly more efficient at computing the Jacobian. Forward-mode accumulation is however useful for evaluating the Jacobian of functions with $N \gg M$, and also has the advantage over reverse-mode accumulation of avoiding the requirement to store the values of intermediate variables from the forward pass for the reverse pass(es).

%Implementations of mathematical software libraries providing reverse-mode automatic differentiation have two key additional requirements beyond the standard ability to evaluate the arithmetic operations and elementary functions provided - for each primitive operation provided by the library a method of evaluating the product of the 

% static graph versus dynamic graph implementations
% side advantages of graph compilation, device agnosticism
% tensor based operations

The direct overlap in our notation to represent variable and operation nodes in computation graphs and that used to represent (random) variable nodes and deterministic factor nodes in factor graphs is intentional. Although often the operations associated with a deterministic node in a factor graph will be more complex than the operations usually represented by nodes in a computation graph, this is only a matter of granularity of reprensentation - fundamentally they perform the same role. Importantly this means we can treat subgraphs of a factor graphs consisting of only variable and deterministic factor nodes as computation graphs and if the operations performed by the deterministic nodes are differentiable, use reverse-mode automatic differentiation to efficiently propagate derivatives through these sub-graphs.

%Similarly while we often consider random variables as stochastic quantities, formally they are defined as measurable functions on the sample space, and likewise the variable nodes in a computation graph represent functional transforms of their ancestor nodes. 

Like directed graphical models, a directed factor graph naturally specifies a generative process via ancestral sampling, with values for the random variables in the graph successively calculated in a forward pass consisting of a combination of deterministic and stochastic operations on the values of parent variables. A computation graph likewise specifies a generative process, how to compute the expression outputs given inputs, computed via a forward pass through the graph with the main differences being here that the inputs to that process are assumed to be given rather than sampled from marginal densities and the intermediate operations are all deterministic.

\section{Simulators}\label{subsec:simulators}

\begin{figure}[!t]
\centering
\begin{subfigure}[b]{\linewidth}
\vskip 0pt
\centering
\hrule
\vskip 3pt
%\begin{minipage}{0.7\linewidth}
\begin{algorithmic}
\small
\State $\rvct{y}_0 \sim \nrm{\vct{\nu},\mtx{\Psi}}$
\State $\rvct{z} \sim \nrm{\vct{\mu},\mtx{\Sigma}}$
\For {$t \in \lbr 1 \dots T\rbr$}
  \State $\rvct{n}_{t-1} \sim \nrm{\vct{0},\idmtx}$
  \State $\rvct{y}_{t} \gets \rvct{y}_{t-1} + h \, \vct{m}(\rvct{y}_{t-1},\rvct{z}) + \sqrt{h} \, \mtx{S}(\rvct{y}_{t-1},\rvct{z})\rvct{n}_{t-1}$
\EndFor
\end{algorithmic}
%\end{minipage}
\vskip 3pt
\hrule
\vskip 3pt
\captionsetup{justification=centering}
\caption{Pseudo-code for Euler--Maruyama simulation of \ac{SDE} model. } %
\label{sfig:sim-model-code} %
\end{subfigure}%
\\[2ex]
\begin{subfigure}[b]{\linewidth}
\vskip 0pt
\centering
\includetikz{euler-maruyama-simulator-factor-graph}
%\vskip 5pt
\caption{Directed factor graph of 3 time steps of \ac{SDE} simulation.}
\label{sfig:sim-model-factor-graph}
\end{subfigure}%
\caption[Simulator model example.]{Example of a simulator model corresponding to Euler--Maruyma integration of a set of \acfp{SDE}, \\$\dr \rvct{y}(t) = \vct{m}\lpa\rvct{y}(t),\rvct{z}\rpa \,\dr t + \mtx{S}\lpa\rvct{y}(t),\rvct{z}\rpa \, \dr\rvct{n}(t)$, specified as pseudo-code in \subref{sfig:sim-model-code} and a directed factor graph in \subref{sfig:sim-model-factor-graph}. In the pseudo-code the notation $\sim$ followed by a distribution shorthand represents generating a value from the associated distribution and assigning it to a variable.} %
\label{fig:simulator-model-example}
\end{figure}

Rather than specifying a generative model via a directed factor graph (or graphical model), it is common for complex models to instead be specified procedurally in code as a \emph{simulator}. Often such simulators may involve a mechanistic model of a physical process for example described by a set of \acfp{SDE}. Any stochasticity in a simulator model will be introduced via draws from a pseudo-random number generator in the programming language used to specify the model. Given these random inputs, the output of the simulator is then calculated as a series of determinstic operations performed to the inputs and so can be described by a computation graph. The overall composition of directed factor nodes specifying the generation of random inputs from known densities by the random number generator and computation graph describing the operations performed by the simulator code together therefore define a directed factor graph from which we can extract a joint density on all the variables in the models as the product of all factors (with implicit Dirac delta terms on the outputs of deterministic factors).  An example of a simulator model corresponding to Euler--Maruyma approximate integration \citep{kloeden1992numerical} of a set of \acp{SDE} is shown as both pseudo-code and a factor graph in Figure \ref{fig:simulator-model-example}.

A key difference of simulator models from more typical probabilistic models is that the variables corresponding to observables in the factor graph of a simulator model may be the output of deterministic factors rather than probabilistic directed factors. %As we will see later in the thesis this can complicate inference in such models. 