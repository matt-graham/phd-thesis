\chapter{Generative model parameterisation}\label{app:generative-model-parameterisation}

\tikzexternalexportnextfalse
\begin{figure}[t]
\centering
\includetikz{unbounded-unit-variance-densities}
\vspace{-1mm}
\caption[Unbounded unit variance densities.]{Unit variance densities with unbounded support.}
\label{fig:unit-variance-densities}
\end{figure}

\tikzexternaldisable
\begin{table}[t]
\centering
\begin{tabular}{rr}
  \toprule
  \textsf{Original factor} & \textsf{Reparametrisation} \\
  \midrule
  \tikz{
    \node[latent] (x) {$\rvar{v}$} ; %
    \factor[left=of x, xshift=-3mm] {p-x} {$\nrm{\mu,\sigma^2}$} {} {x} ; %
  } &
  \tikz{
    \node[latent] (u) {$\rvar{u}$} ; %
    \node[latent, right=of u, xshift=13mm] (x) {$\rvar{v}$} ; %
    \factor[left=of u, xshift=-3mm] {p-u} 
      {$\nrm{0,1}$} {} {u} ; %
    \op[left=of x, xshift=-6mm] {u-x} 
      {$\mu + \sigma \rvar{u}$} {u} {x} ; %
  } 
  \\
  \tikz{
    \node[latent] (x) {$\rvar{v}$} ; %
    \factor[left=of x, xshift=-3mm] {p-x} {$\textrm{LogNorm}(\mu,\sigma^2)$} {} {x} ; %
  } &
  \tikz{
    \node[latent] (u) {$\rvar{u}$} ; %
    \node[latent, right=of u, xshift=13mm] (x) {$\rvar{v}$} ; %
    \factor[left=of u, xshift=-3mm] {p-u} 
      {$\nrm{0,1}$} {} {u} ; %
    \op[left=of x, xshift=-6mm] {u-x} 
      {$\exp(\mu + \sigma \rvar{u})$} {u} {x} ; %
  } 
  \\
%  \tikz{
%    \node[latent] (x) {$\rvct{v}$} ; %
%    \factor[left=of x, xshift=-3mm] {p-x} {$\nrm{\vct{\mu},\mtx{\Sigma}}$} {} {x} ; %
%  } &
%  \tikz{
%    \node[latent] (u) {$\rvct{u}$} ; %
%    \node[latent, right=of u, xshift=13mm] (x) {$\rvct{v}$} ; %
%    \factor[left=of u, xshift=-3mm] {p-u} 
%      {$\nrm{\vct{0},\idmtx}$} {} {u} ; %
%    \op[left=of x, xshift=-6mm] {u-x} 
%      {$\vct{\mu} + \chol(\mtx{\Sigma}) \rvct{u}$} {u} {x} ; %
%  } 
%  \\
  \tikz{
    \node[latent] (x) {$\rvar{v}$} ; %
    \factor[left=of x, xshift=-3mm] {p-x} {$\mathrm{Exp}(\lambda)$} {} {x} ; %
  } &
  \tikz{
    \node[latent] (u) {$\rvar{u}$} ; %
    \node[latent, right=of u, xshift=13mm] (x) {$\rvar{v}$} ; %
    \factor[left=of u, xshift=-3mm] {p-u} 
      {$\mathrm{Logistic}\lpa 0, \frac{\sqrt{3}}{\uppi}\rpa$} {} {u} ; %
    \op[left=of x, xshift=-6mm] {u-x} 
      {$\frac{1}{\lambda}\log\lpa 1 + \exp\lpa\frac{\uppi\rvar{u}}{\sqrt{3}}\rpa\rpa$} {u} {x} ; %
  } 
  \\
  \tikz{
    \node[latent] (x) {$\rvar{v}$} ; %
    \factor[left=of x, xshift=-3mm] {p-x} {$\unif{a,b}$} {} {x} ; %
  } &
  \tikz{
    \node[latent] (u) {$\rvar{u}$} ; %
    \node[latent, right=of u, xshift=13mm] (x) {$\rvar{v}$} ; %
    \factor[left=of u, xshift=-3mm] {p-u} 
      {$\mathrm{Logistic}\lpa 0, \frac{\sqrt{3}}{\uppi}\rpa$} {} {u} ; %
    \op[left=of x, xshift=-6mm] {u-x} 
      {$a + (b-a)\lpa 1 + \exp\lpa\frac{\uppi\rvar{u}}{\sqrt{3}}\rpa\rpa^{-1}$} {u} {x} ; %
  } 
  \\
  \tikz{
    \node[latent] (x) {$\rvar{v}$} ; %
    \factor[left=of x, xshift=-3mm] {p-x} {$\mathcal{C}_{\geq 0}(\gamma)$} {} {x} ; %
  } &
  \tikz{
    \node[latent] (u) {$\rvar{u}$} ; %
    \node[latent, right=of u, xshift=13mm] (x) {$\rvar{v}$} ; %
    \factor[left=of u, xshift=-3mm] {p-u} 
      {$\mathrm{InvCosh}( 0, 1)$} {} {u} ; %
    \op[left=of x, xshift=-6mm] {u-x} 
      {$\gamma\exp\lpa \frac{\uppi \rvar{u}}{2} \rpa$} {u} {x} ; %
  } 
  \\
  \bottomrule
\end{tabular}
\caption[Standardisation reparametrisations.]{Reparameterisations of random variables with some common parametric distributions as deterministic transformations of unit-variance unbounded support random variables.}
\label{tab:standardisation-reparametrisations}
\end{table}
\tikzexternalenable

As the inference methods we propose in Chapter \ref{ch:differentiable-generative-models} work in the generator input space, we can reparameterise the input space to endow it with a favourable form for inference. For example we will generally reparametrise input variables with bounded support to transformed variables with unbounded support, for example reparameterising in terms of the logarithm of a strictly positive variable. In general working with unbounded variables will simplify \ac{MCMC} inference by preventing the need to check transitions respect bounding constraints. Probabilistic programming frameworks such as Stan \citep{carpenter2016stan} and PyMC3 \citep{salvatier2016probabilistic} use a range of such transformations when performing inference.

As well as transforming to variables with unbounded support, another useful heuristic is to parameterise a model as far as possible in terms of inputs variables which have unit variance. Three examples of potentially suitable distributions with unit variance and unbounded support are the standard normal $\nrm{0,1}$, inverse hyperbolic cosine (or hyperbolic secant) distribution $\textrm{InvCosh}(0,1)$ and the logistic distribution $\textrm{Logistic}(0,\sqrt{3}/\uppi)$. The densities for all three shown for comparison in Figure \ref{fig:unit-variance-densities} and Table \ref{tab:standardisation-reparametrisations} shows reparameterisations for some common distributions in terms of variables distributed according to these standard densities. Normalising the marginal variances of variables in $\rho$ typically makes it easier to choose an appropriate scale parameters for the \ac{MCMC} transitions. %The posterior distribution of the input variables $\rvct{u}$ after conditioning on the output of the generator will typically differ significantly from $\prob{\rvct{u}}$ and so normalising the scale of variables in $\prob{\rvct{u}}$ is no guarantee of similar scaling in the posterior, however empirically we have found it to be a useful guideline.%particularly when the inputs are only weakly affected by the outputs conditioned on.

Also note that although we motivated our definition of the random inputs $\rvct{u}$ in Chapter \ref{ch:differentiable-generative-models} by saying it could be constructed by tracking all the draws from a random number generator, in general we will not want to parameterise $\rvct{u}$ in terms of low-level uniform draws, but instead use the output of higher-level functions for producing samples from standard densities using the transform and rejection sampling methods discussed in Chapter \ref{ch:approximate-inference}. This is important as if for example we defined as inputs the uniform draws used in the rejection sampling routines used to generate Gamma random variables, we would both require dealing with the complications involved with generators using variable numbers of random inputs as described in Chapter \ref{ch:differentiable-generative-models} and also have that $\genfunc_{\rvct{x}}$ would be non-differentiable with respect to the rejection sampling inputs even if the all of the operations performed with the Gamma variable to produce the generated outputs are themselves differentiable. If we instead use the generated Gamma variable itself as the input by including an appropriate Gamma density factor in $\rho$ we side step these issues. 

In some cases using the outputs of higher-level random number generator routines as the input variables will introduce dependencies between the variables in the input density $\rho$. In particular if $\rvar{u}_i$ is drawn from a distribution with parameters depending on one or more previous random inputs $\lbrace \rvar{u}_j\rbrace_{j\in\set{J}}$, then an appropriate conditional density factor on $\rvar{u}_i$ given $\lbrace \rvar{u}_j\rbrace_{j\in\set{J}}$ will need to be included in $\rho$. By using alternative parameterisations it may be possible to avoid introducing such dependencies; for example a random input $\rvar{v}_i$ generated from a normal distribution with mean $\mu$ and standard deviation $\sigma$ which depend on previous random inputs $\lbrace \rvar{u}_j\rbrace_{j\in\set{J}}$ can instead be parameterised in terms of an independent random variable $\rvar{u}_i$ distributed with a standard normal density $\nrm{0,1}$ and $\rvar{v}_i$ computed as $\sigma \rvar{u}_i + \mu$ in the generator. 

Such \emph{non-centred parameterisations} \citep{price1958useful,bonnet1964transformations,papaspiliopoulos2007general} are available for example for all location-scale family distributions. The reparametrisation of the Gaussian \ac{VAE} decoder discussed in Chapter \ref{ch:differentiable-generative-models} also uses this same identity, and the term `reparameterisation trick' is often used in the machine learning literature to describe this idea \citep{kingma2013auto}. Whether it is necessarily helpful to remove dependencies in $\rho$ like this for the methods discussed in Chapter \ref{ch:differentiable-generative-models} is an open question and will likely be model specific; it has previously been found that non-centred parameterisations can be beneficial when performing \ac{MCMC} inference in hierarchical models when the unobserved variables are only weakly identified by observations \citep{papaspiliopoulos2003non,papaspiliopoulos2007general,betancourt2015hamiltonian}.  %Whether it is desirable to remove dependencies between variables in $\rho$ will likely be model specific; in our experiments

%Even when completely removing dependencies is not possible, decreases in the strength of dependencies can often be achieved using `partial reparameterisations' such as those proposed by \cite{naesseth2017reparameterization} and \citep{ruiz2016generalized} in the context of variational inference.


%Sometimes by using alternative parameterisations such as the non-centred parameterisation \citep{price1958useful,bonnet1964transformations} for location-scale family distributions  we can avoid introducing dependencies between components of $\rvct{u}$, with instead the dependencies introduced in the generator. In some cases this can improve Even when completely removing dependencies is not possible, decreases in the strength of dependencies can often be achieved using `partial reparameterisations' such as those proposed by \cite{naesseth2017reparameterization} and \citep{ruiz2016generalized} in the context of variational inference.