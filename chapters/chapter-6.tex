\chapter{Summary}\label{ch:conclusions}

This thesis has been concerned with the development of methods for performing inference in complex probabilistic models and in particular the associated key computational challenge of approximating high-dimensional integrals. In particular we have contributed several novel Markov chain Monte Carlo approaches based on the introduction of auxiliary variables in to the chain state.

In Chapter \ref{ch:probabilistic-modelling} we introduced the probabilistic modelling theory and tools which we used to describe the models and methods discussed in the rest of the thesis, and concluded with a discussion of some classes of probabilistic models with key challenging features for existing inference methods: hierarchical latent variable models with both global latent variables affecting all observations and local per data point variables; simulator models which are defined by a generative process for producing samples from the model rather than by an explicit density function on model variables; and undirected models such as Boltzmann machines which tend to produce highly multimodal distributions.

Chapter \ref{ch:approximate-inference} reviewed some the key existing approximate inference approaches for estimating high-dimensional integrals. After introducing the basic Monte Carlo integration method, we discussed some simple approaches for generating and using samples to approximate integrals and identified that the complexities of the geometry of distributions on high-dimensional spaces typically mean such methods are typically limited to only very small models or as building blocks within more efficient algorithms. We then introduced the main class of methods we focused on in the thesis - Markov chain Monte Carlo methods, and motivated the ability of this approach to  provide a scalable, general purpose framework for approximating integrals with respect to probability distributions on high-dimensional spaces. The chapter concluded with a discussion of auxiliary variable approaches to constructing efficient Markov chains, and we described three instantiations of this idea - slice sampling, Hamiltonian Monte Carlo, with these methods underlying much of work discussed in the remainder of the thesis.

Chapter \ref{ch:pseudo-marginal-methods} considered \emph{pseudo-marginal} methods for constructing Markov chains using noisy unbiased estimators of the target density of interest. We concentrated in particular on the application of these approaches to inferring the posterior distribution on global variables in hierarchical latent variable models. Markov chains produced using standard pseudo-marginal methods are often susceptible to getting stuck at points in the state space of many iterations and are difficult to tune using standard heuristics. By considering a reparameterisation of the density estimator as a deterministic function of the auxiliary random inputs used to compute the estimate, we proposed a \emph{auxiliary pseudo-marginal} framework which applies separate updates to the auxiliary random inputs used in the estimator and the original target variables. 

The simplest instantiation of this framework just corresponds to splitting the standard pseudo-marginal update in to two separate Metropolis--Hastings accept steps, but even this minor change is able to produce chains which are significantly easier to tune and in some cases give improved sampling efficiency. The auxiliary pseudo-marginal framework proposed also allows the use of alternative transition operators such as slice sampling algorithms, which as we demonstrated empirically are significantly less sensitive to the choices made for their free parameters than random-walk Metropolis methods. Although we found the slice sampling approaches proposed were generally less efficient than optimally tune random-walk Metropolis updates, we argue that extra robustness provided by the adaptivity of the slice sampling algorithm and minimal need for user tuning may often be more important benefits. Our emphasis was on empirical evaluation of the methods we proposed and a possible useful direction for future work would be to try to put the methods on a more firm theoretical footing, for example characterising when the proposed `split' auxiliary pseudo-marginal update would be expected to give improved performance over the standard pseudo-marginal Metropolis--Hastings update and when it would be expected to be less beneficial.

In Chapter \ref{ch:differentiable-generative-models} we considered methods for performing inference in generative models where we can generate samples from the model but the density on the model variables is only implicitly defined. We demonstrated that a general framework for describing generative models is as a deterministic mapping from a vector of random inputs drawn from a known distribution, and defined a restricted class of \emph{differentiable generative models} where the generator function which transforms inputs to simulated outputs is differentiable. Using this formulation of a generative model we showed how the \acf{ABC} approach for inference in the parameter space of implicit generative models could be reposed in the input space to the generator. Similarly to the methods proposed in Chapter \ref{ch:pseudo-marginal-methods}, this reparameterisation enables the application of more efficient \ac{MCMC} transition operators such as slice sampling and Hamiltonian Monte Carlo to \ac{ABC} inference problems. As we demonstrated empirically, in some cases the resulting methods are able to support inference when conditioning on the full set of observed data when standard \ac{ABC} approaches are forced to used reduced dimensionality summary statistics.

For differentiable generative models we showed that computing expectations conditioned on observed values of the output of the generator corresponded to integrating against a distribution defined by an implicitly defined manifold in the generator input space. Based on this insight, we proposed a novel constrained Hamiltonian Monte Carlo method for performing inference in differentiable generative models while conditioning the model outputs to be arbitrarily close to observed values. This offers an asymptotically exact alternative to \ac{ABC} in some models and as we demonstrated empirically the method can both provide improved sampling efficiency over competing approaches by better exploiting the geometric structure of the problem, while also preserving more information about the variables being inferred.

We found that application of the standard \ac{HMC} algorithm to inference in the input space of a generator based on conditioning with via a Gaussian \ac{ABC} kernel did not perform particularly well, with the resulting simulated dynamics showing highly oscillatory behaviour which limited coherent exploration of the target distribution. A potential resolution to this issue would be to instead use a Riemannian-manifold Hamiltonian dynamic, using a metric exploiting the tangent space structure defined by the generator Jacobian. Although the requirement to use an implicit solver in the integrator in this case would increase the computational cost per update, it would interesting to see if as with the more costly constrained \ac{HMC} dynamic considered here, whether any resulting improved exploration of the space was able to outweigh the higher computational costs.

An assumption of differentiability of the generative model is restrictive, with many of the models \ac{ABC} approaches are commonly applied to involving discrete latent variables or having generators with branching control flow logic that introduce discontinuities. It would be interesting to consider if the general approach of parameterising a generative model as a deterministic map can also be utilised in these setting to suggest more efficient inference methods. From a software engineering perspective, it would also be useful to develop approaches for automating the tracking of calls to a pseudo-random number generator in simulator code, to allow for models to be more easily parameterised in a form suitable for the inference approaches we suggest. %We focused on direct application of \ac{ABC} \ac{MCMC} methods to perform inference. Sequential Monte Carlo based \ac{ABC} approaches which define a series of approximate posteriors by varying the kernel tolerance $\epsilon$ typically make use of Markov transitions as a component of the algorithm it would be useful to consider extensions of the the approaches we propose to an \ac{SMC} context.

In Chapter \ref{ch:continuous-tempering} we introduced a novel tempering method which introduces an auxiliary \emph{inverse temperature} variable in to the state of a Markov chain to improve the exploration of challenging multimodal distributions and allow for normalising constants to be estimated. Unlike existing simulated tempering methods, the approach we proposed uses a continuous temperature. This reduces the tuning burden on users by eliminating the need to choose a set of of inverse temperature values. Further using a continuous formulation allows the inverse temperature variable to be jointly updated with the original variables in a chain, making it simple to apply efficient gradient based Hamiltonian Monte Carlo transition operators to the augmented space. We also proposed a novel importance sampling inspired approach for using all of the sampled states of a tempered chain to estimate expectations with respect to the target distribution, this making more effective usage of the computation performed than the standard estimator used in simulated tempering which computes averages over only a small subset of sampled states. We also demonstrated that variational inference is a natural approach for fitting the base distribution bridged to by the tempered dynamic, with the use of more informative base distributions key to scaling tempering approaches to large-scale problems by helping to flatten the marginal density on the inverse temperature.

In practice in complex target distributions even when using a base distribution fit using a variational approach, the tempered dynamic will still typically struggle to move up and down the inverse temperature range. We briefly outlined the basics of an iterative approach for trying to exploit information from initial pilot chains to improve mixing in subsequent chains by iteratively improving the fit of the base distribution and making use of improved estimates of the normalising constant. Ideally this would be done in an online manner to prevent the need to do multiple distinct runs, however maintaining reversibility in such a scheme would challenging. It may be fruitful to consider however if adaptive \ac{MCMC} approaches could be employed in this setting. 

We employed a standard \ac{HMC} transition operator when performing joint updates on the extended space. The distribution on the extended space has a rich geometric structure with for example the smoothness of the distribution typically increasing in regions corresponding to low inverse temperatures. It may be possible to exploit this structure to construct more efficient \ac{HMC} updates in the extended space, for example using a mass metric which depends on the current inverse temperature value within a Riemannian-manifold \ac{HMC} framework.

%introduced the pseudo-marginal framework for constructing Markov chains using noisy unbiased estimators of the target density of interest. We described some of the computational issues faced by the existing methods within this framework, in particular the often fragile nature of the chains produced, with a tendency to get stuck in parts of the state space over many iterations, and the difficult in tuning the methods using standard heuristics.